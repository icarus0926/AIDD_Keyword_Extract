<!DOCTYPE html>
<html data-a11y-animated-images="system" data-a11y-link-underlines="true" data-color-mode="auto" data-dark-theme="dark" data-light-theme="light" lang="en">
 <head>
  <meta charset="utf-8"/>
  <link href="https://github.githubassets.com" rel="dns-prefetch"/>
  <link href="https://avatars.githubusercontent.com" rel="dns-prefetch"/>
  <link href="https://github-cloud.s3.amazonaws.com" rel="dns-prefetch"/>
  <link href="https://user-images.githubusercontent.com/" rel="dns-prefetch"/>
  <link crossorigin="" href="https://github.githubassets.com" rel="preconnect"/>
  <link href="https://avatars.githubusercontent.com" rel="preconnect"/>
  <link crossorigin="anonymous" href="https://github.githubassets.com/assets/light-f13f84a2af0d.css" media="all" rel="stylesheet">
   <link crossorigin="anonymous" href="https://github.githubassets.com/assets/dark-1ee85695b584.css" media="all" rel="stylesheet">
    <link crossorigin="anonymous" data-color-theme="dark_dimmed" data-href="https://github.githubassets.com/assets/dark_dimmed-8c42799cfb52.css" media="all" rel="stylesheet">
     <link crossorigin="anonymous" data-color-theme="dark_high_contrast" data-href="https://github.githubassets.com/assets/dark_high_contrast-dc99d916bf90.css" media="all" rel="stylesheet">
      <link crossorigin="anonymous" data-color-theme="dark_colorblind" data-href="https://github.githubassets.com/assets/dark_colorblind-0a83868d0e43.css" media="all" rel="stylesheet">
       <link crossorigin="anonymous" data-color-theme="light_colorblind" data-href="https://github.githubassets.com/assets/light_colorblind-3c798f5a8bef.css" media="all" rel="stylesheet">
        <link crossorigin="anonymous" data-color-theme="light_high_contrast" data-href="https://github.githubassets.com/assets/light_high_contrast-4c72a7f3b765.css" media="all" rel="stylesheet"/>
        <link crossorigin="anonymous" data-color-theme="light_tritanopia" data-href="https://github.githubassets.com/assets/light_tritanopia-222bf22536c7.css" media="all" rel="stylesheet"/>
        <link crossorigin="anonymous" data-color-theme="dark_tritanopia" data-href="https://github.githubassets.com/assets/dark_tritanopia-c1d9496197fa.css" media="all" rel="stylesheet"/>
        <link crossorigin="anonymous" href="https://github.githubassets.com/assets/primer-primitives-0b5bee5c70e9.css" media="all" rel="stylesheet"/>
        <link crossorigin="anonymous" href="https://github.githubassets.com/assets/primer-44fa1513ddd0.css" media="all" rel="stylesheet"/>
        <link crossorigin="anonymous" href="https://github.githubassets.com/assets/global-1c8bb26336c1.css" media="all" rel="stylesheet"/>
        <link crossorigin="anonymous" href="https://github.githubassets.com/assets/github-07f750db5d7c.css" media="all" rel="stylesheet"/>
        <link crossorigin="anonymous" href="https://github.githubassets.com/assets/repository-fa69f138fe8d.css" media="all" rel="stylesheet"/>
        <link crossorigin="anonymous" href="https://github.githubassets.com/assets/code-111be5e4092d.css" media="all" rel="stylesheet"/>
        <script id="client-env" type="application/json">
         {"locale":"en","featureFlags":["code_vulnerability_scanning","copilot_conversational_ux_history_refs","copilot_smell_icebreaker_ux","copilot_implicit_context","failbot_handle_non_errors","geojson_azure_maps","image_metric_tracking","marketing_forms_api_integration_contact_request","marketing_pages_search_explore_provider","turbo_experiment_risky","sample_network_conn_type","no_character_key_shortcuts_in_inputs","react_start_transition_for_navigations","report_hydro_web_vitals","custom_inp","remove_child_patch"]}
        </script>
        <script crossorigin="anonymous" defer="defer" src="https://github.githubassets.com/assets/wp-runtime-ca56c0b8833a.js" type="application/javascript">
        </script>
        <script crossorigin="anonymous" defer="defer" src="https://github.githubassets.com/assets/vendors-node_modules_dompurify_dist_purify_js-6890e890956f.js" type="application/javascript">
        </script>
        <script crossorigin="anonymous" defer="defer" src="https://github.githubassets.com/assets/vendors-node_modules_oddbird_popover-polyfill_dist_popover_js-7bd350d761f4.js" type="application/javascript">
        </script>
        <script crossorigin="anonymous" defer="defer" src="https://github.githubassets.com/assets/vendors-node_modules_smoothscroll-polyfill_dist_smoothscroll_js-node_modules_stacktrace-parse-a448e4-bb5415637fe0.js" type="application/javascript">
        </script>
        <script crossorigin="anonymous" defer="defer" src="https://github.githubassets.com/assets/environment-775215f6b8df.js" type="application/javascript">
        </script>
        <script crossorigin="anonymous" defer="defer" src="https://github.githubassets.com/assets/vendors-node_modules_github_selector-observer_dist_index_esm_js-9f960d9b217c.js" type="application/javascript">
        </script>
        <script crossorigin="anonymous" defer="defer" src="https://github.githubassets.com/assets/vendors-node_modules_primer_behaviors_dist_esm_focus-zone_js-086f7a27bac0.js" type="application/javascript">
        </script>
        <script crossorigin="anonymous" defer="defer" src="https://github.githubassets.com/assets/vendors-node_modules_github_relative-time-element_dist_index_js-c76945c5961a.js" type="application/javascript">
        </script>
        <script crossorigin="anonymous" defer="defer" src="https://github.githubassets.com/assets/vendors-node_modules_github_combobox-nav_dist_index_js-node_modules_github_markdown-toolbar-e-820fc0-bc8f02b96749.js" type="application/javascript">
        </script>
        <script crossorigin="anonymous" defer="defer" src="https://github.githubassets.com/assets/vendors-node_modules_github_auto-complete-element_dist_index_js-03fc21f4e80c.js" type="application/javascript">
        </script>
        <script crossorigin="anonymous" defer="defer" src="https://github.githubassets.com/assets/vendors-node_modules_github_text-expander-element_dist_index_js-8a621df59e80.js" type="application/javascript">
        </script>
        <script crossorigin="anonymous" defer="defer" src="https://github.githubassets.com/assets/vendors-node_modules_delegated-events_dist_index_js-node_modules_stacktrace-parser_dist_stack-443cd5-1ba4dbac454f.js" type="application/javascript">
        </script>
        <script crossorigin="anonymous" defer="defer" src="https://github.githubassets.com/assets/vendors-node_modules_github_filter-input-element_dist_index_js-node_modules_github_remote-inp-b7d8f4-7dc906febe69.js" type="application/javascript">
        </script>
        <script crossorigin="anonymous" defer="defer" src="https://github.githubassets.com/assets/vendors-node_modules_github_file-attachment-element_dist_index_js-node_modules_primer_view-co-27181b-3509ed8075c4.js" type="application/javascript">
        </script>
        <script crossorigin="anonymous" defer="defer" src="https://github.githubassets.com/assets/app_assets_modules_github_onfocus_ts-ui_packages_trusted-types-policies_policy_ts-ui_packages-6fe316-745e8b6794ab.js" type="application/javascript">
        </script>
        <script crossorigin="anonymous" defer="defer" src="https://github.githubassets.com/assets/github-elements-34cbf079a4f4.js" type="application/javascript">
        </script>
        <script crossorigin="anonymous" defer="defer" src="https://github.githubassets.com/assets/element-registry-b38ce746f71a.js" type="application/javascript">
        </script>
        <script crossorigin="anonymous" defer="defer" src="https://github.githubassets.com/assets/vendors-node_modules_github_catalyst_lib_index_js-node_modules_github_hydro-analytics-client_-4da1df-9de8d527f925.js" type="application/javascript">
        </script>
        <script crossorigin="anonymous" defer="defer" src="https://github.githubassets.com/assets/vendors-node_modules_braintree_browser-detection_dist_browser-detection_js-node_modules_githu-fd5530-6fc33e963fc0.js" type="application/javascript">
        </script>
        <script crossorigin="anonymous" defer="defer" src="https://github.githubassets.com/assets/vendors-node_modules_lit-html_lit-html_js-5b376145beff.js" type="application/javascript">
        </script>
        <script crossorigin="anonymous" defer="defer" src="https://github.githubassets.com/assets/vendors-node_modules_morphdom_dist_morphdom-esm_js-node_modules_github_memoize_dist_esm_index_js-05801f7ca718.js" type="application/javascript">
        </script>
        <script crossorigin="anonymous" defer="defer" src="https://github.githubassets.com/assets/vendors-node_modules_github_turbo_dist_turbo_es2017-esm_js-c91f4ad18b62.js" type="application/javascript">
        </script>
        <script crossorigin="anonymous" defer="defer" src="https://github.githubassets.com/assets/vendors-node_modules_github_remote-form_dist_index_js-node_modules_delegated-events_dist_inde-893f9f-a8ec7ed862cf.js" type="application/javascript">
        </script>
        <script crossorigin="anonymous" defer="defer" src="https://github.githubassets.com/assets/vendors-node_modules_scroll-anchoring_dist_scroll-anchoring_esm_js-node_modules_github_detail-c9d0ba-387cde917623.js" type="application/javascript">
        </script>
        <script crossorigin="anonymous" defer="defer" src="https://github.githubassets.com/assets/vendors-node_modules_color-convert_index_js-72c9fbde5ad4.js" type="application/javascript">
        </script>
        <script crossorigin="anonymous" defer="defer" src="https://github.githubassets.com/assets/vendors-node_modules_primer_behaviors_dist_esm_dimensions_js-node_modules_github_jtml_lib_index_js-95b84ee6bc34.js" type="application/javascript">
        </script>
        <script crossorigin="anonymous" defer="defer" src="https://github.githubassets.com/assets/vendors-node_modules_github_quote-selection_dist_index_js-node_modules_github_session-resume_-84957b-7b4e472db160.js" type="application/javascript">
        </script>
        <script crossorigin="anonymous" defer="defer" src="https://github.githubassets.com/assets/ui_packages_sudo_sudo_ts-235370c302ce.js" type="application/javascript">
        </script>
        <script crossorigin="anonymous" defer="defer" src="https://github.githubassets.com/assets/app_assets_modules_github_updatable-content_ts-ui_packages_hydro-analytics_hydro-analytics_ts-82813f-05346aa543fe.js" type="application/javascript">
        </script>
        <script crossorigin="anonymous" defer="defer" src="https://github.githubassets.com/assets/app_assets_modules_github_behaviors_task-list_ts-app_assets_modules_github_onfocus_ts-app_ass-421cec-355eb4940fad.js" type="application/javascript">
        </script>
        <script crossorigin="anonymous" defer="defer" src="https://github.githubassets.com/assets/app_assets_modules_github_sticky-scroll-into-view_ts-94209c43e6af.js" type="application/javascript">
        </script>
        <script crossorigin="anonymous" defer="defer" src="https://github.githubassets.com/assets/app_assets_modules_github_behaviors_ajax-error_ts-app_assets_modules_github_behaviors_include-467754-782c9388f902.js" type="application/javascript">
        </script>
        <script crossorigin="anonymous" defer="defer" src="https://github.githubassets.com/assets/app_assets_modules_github_behaviors_commenting_edit_ts-app_assets_modules_github_behaviors_ht-83c235-9285faa0e011.js" type="application/javascript">
        </script>
        <script crossorigin="anonymous" defer="defer" src="https://github.githubassets.com/assets/behaviors-7f67a24be639.js" type="application/javascript">
        </script>
        <script crossorigin="anonymous" defer="defer" src="https://github.githubassets.com/assets/vendors-node_modules_delegated-events_dist_index_js-node_modules_github_catalyst_lib_index_js-06ff531-2ea61fcc9a71.js" type="application/javascript">
        </script>
        <script crossorigin="anonymous" defer="defer" src="https://github.githubassets.com/assets/notifications-global-6d6db5144cc3.js" type="application/javascript">
        </script>
        <script crossorigin="anonymous" defer="defer" src="https://github.githubassets.com/assets/vendors-node_modules_virtualized-list_es_index_js-node_modules_github_template-parts_lib_index_js-878844713bc9.js" type="application/javascript">
        </script>
        <script crossorigin="anonymous" defer="defer" src="https://github.githubassets.com/assets/vendors-node_modules_github_remote-form_dist_index_js-node_modules_delegated-events_dist_inde-e53a3f-f924cc31bbb1.js" type="application/javascript">
        </script>
        <script crossorigin="anonymous" defer="defer" src="https://github.githubassets.com/assets/app_assets_modules_github_ref-selector_ts-2b432e185ab2.js" type="application/javascript">
        </script>
        <script crossorigin="anonymous" defer="defer" src="https://github.githubassets.com/assets/codespaces-b7f6071c8422.js" type="application/javascript">
        </script>
        <script crossorigin="anonymous" defer="defer" src="https://github.githubassets.com/assets/vendors-node_modules_github_filter-input-element_dist_index_js-node_modules_github_mini-throt-1f9a80-369ba092db4f.js" type="application/javascript">
        </script>
        <script crossorigin="anonymous" defer="defer" src="https://github.githubassets.com/assets/vendors-node_modules_github_mini-throttle_dist_decorators_js-node_modules_github_remote-form_-737e8d-13881b8e7358.js" type="application/javascript">
        </script>
        <script crossorigin="anonymous" defer="defer" src="https://github.githubassets.com/assets/vendors-node_modules_github_file-attachment-element_dist_index_js-node_modules_primer_behavio-2144fe-a56c35798651.js" type="application/javascript">
        </script>
        <script crossorigin="anonymous" defer="defer" src="https://github.githubassets.com/assets/app_assets_modules_github_repositories_get-repo-element_ts-f6b365a47eda.js" type="application/javascript">
        </script>
        <script crossorigin="anonymous" defer="defer" src="https://github.githubassets.com/assets/repositories-57e1f829b48d.js" type="application/javascript">
        </script>
        <script crossorigin="anonymous" defer="defer" src="https://github.githubassets.com/assets/code-menu-67595c3a6d0c.js" type="application/javascript">
        </script>
        <title>
         GitHub - AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset: A collection of some datasets for segmentation / saliency detection. Welcome to PR...:smile:
        </title>
        <meta content="/:user_id/:repository" data-turbo-transient="" name="route-pattern"/>
        <meta content="files" data-turbo-transient="" name="route-controller"/>
        <meta content="disambiguate" data-turbo-transient="" name="route-action"/>
        <meta content="82c569b93da5c18ed649ebd4c2c79437db4611a6a1373e805a3cb001c64130b7" name="current-catalog-service-hash"/>
        <meta content="046A:183950:90C618:9B36EF:6627766E" data-pjax-transient="true" name="request-id">
         <meta content="c5e7f4b781b371fadd4413cfc5b706b0dd4f5ee09f0671f9bb8a46cf7e4b8382" data-pjax-transient="true" name="html-safe-nonce">
          <meta content="eyJyZWZlcnJlciI6IiIsInJlcXVlc3RfaWQiOiIwNDZBOjE4Mzk1MDo5MEM2MTg6OUIzNkVGOjY2Mjc3NjZFIiwidmlzaXRvcl9pZCI6IjI3NDk1MTMxMzI2MDQzNTUxMzYiLCJyZWdpb25fZWRnZSI6InNvdXRoZWFzdGFzaWEiLCJyZWdpb25fcmVuZGVyIjoic291dGhlYXN0YXNpYSJ9" data-pjax-transient="true" name="visitor-payload">
           <meta content="bf5bfbed5b14d689f91ab365438747af50b13cded12f0a4d6a398d3a26db1c02" data-pjax-transient="true" name="visitor-hmac">
            <meta content="repository:306616897" data-turbo-transient="" name="hovercard-subject-tag"/>
            <meta content="repository,copilot" data-turbo-transient="true" name="github-keyboard-shortcuts">
             <meta data-turbo-transient="" name="selected-link" value="repo_source"/>
             <link href="https://github.githubassets.com/" rel="assets"/>
             <meta content="c1kuD-K2HIVF635lypcsWPoD4kilo5-jA_wBFyT4uMY" name="google-site-verification"/>
             <meta content="KT5gs8h0wvaagLKAVWq8bbeNwnZZK1r1XQysX3xurLU" name="google-site-verification"/>
             <meta content="ZzhVyEFwb7w3e0-uOTltm8Jsck2F5StVihD0exw2fsA" name="google-site-verification"/>
             <meta content="GXs5KoUUkNCoaAZn7wPN-t01Pywp9M3sEjnt_3_ZWPc" name="google-site-verification"/>
             <meta content="Apib7-x98H0j5cPqHWwSMm6dNU4GmODRoqxLiDzdx9I" name="google-site-verification"/>
             <meta content="https://collector.github.com/github/collect" name="octolytics-url">
              <meta content="/&lt;user-name&gt;/&lt;repo-name&gt;" data-turbo-transient="true" name="analytics-location">
               <meta content="" name="user-login"/>
               <meta content="width=device-width" name="viewport"/>
               <meta content="A collection of some datasets for segmentation / saliency detection. Welcome to PR...:smile: - AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset" name="description"/>
               <link href="/opensearch.xml" rel="search" title="GitHub" type="application/opensearchdescription+xml"/>
               <link href="https://github.com/fluidicon.png" rel="fluid-icon" title="GitHub"/>
               <meta content="1401488693436528" property="fb:app_id"/>
               <meta content="app-id=1477376905, app-argument=https://github.com/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset" name="apple-itunes-app">
                <meta content="https://opengraph.githubassets.com/26afc79ff5776b501e5ccf5e567efe512ab35dc8c536428fb7576474d10b3e5a/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset" name="twitter:image:src">
                 <meta content="@github" name="twitter:site">
                  <meta content="summary_large_image" name="twitter:card">
                   <meta content="GitHub - AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset: A collection of some datasets for segmentation / saliency detection. Welcome to PR...:smile:" name="twitter:title">
                    <meta content="A collection of some datasets for segmentation / saliency detection. Welcome to PR...:smile: - AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset" name="twitter:description">
                     <meta content="https://opengraph.githubassets.com/26afc79ff5776b501e5ccf5e567efe512ab35dc8c536428fb7576474d10b3e5a/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset" property="og:image">
                      <meta content="A collection of some datasets for segmentation / saliency detection. Welcome to PR...:smile: - AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset" property="og:image:alt">
                       <meta content="1200" property="og:image:width">
                        <meta content="600" property="og:image:height"/>
                        <meta content="GitHub" property="og:site_name"/>
                        <meta content="object" property="og:type"/>
                        <meta content="GitHub - AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset: A collection of some datasets for segmentation / saliency detection. Welcome to PR...:smile:" property="og:title"/>
                        <meta content="https://github.com/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset" property="og:url"/>
                        <meta content="A collection of some datasets for segmentation / saliency detection. Welcome to PR...:smile: - AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset" property="og:description"/>
                        <meta content="github.com" name="hostname"/>
                        <meta content="github.com" name="expected-hostname"/>
                        <meta content="6624cbd8ff7e6738c23a70bc94a6aa2fadf4328b6dc40d7b714384a5aed85622" data-turbo-track="reload" http-equiv="x-pjax-version"/>
                        <meta content="f226bf37af9c33162063db3eb018fed7f088f86d0a20ca54c013fda96c7f2e05" data-turbo-track="reload" http-equiv="x-pjax-csp-version"/>
                        <meta content="c7c53f4a8c1805ddf3ad2b644dd42a4962efe1cdc844e0f7d13ea6efe106ef15" data-turbo-track="reload" http-equiv="x-pjax-css-version"/>
                        <meta content="fb90063b7e4db7d5f7a032053a8929ddd17b0296bd7673f33b338216018839ce" data-turbo-track="reload" http-equiv="x-pjax-js-version"/>
                        <meta content="no-preview" data-turbo-transient="" name="turbo-cache-control"/>
                        <meta data-hydrostats="publish"/>
                        <meta content="github.com/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset git https://github.com/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset.git" name="go-import"/>
                        <meta content="72787137" name="octolytics-dimension-user_id">
                         <meta content="AIMonk-Labs-Private-Limited" name="octolytics-dimension-user_login">
                          <meta content="306616897" name="octolytics-dimension-repository_id">
                           <meta content="AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset" name="octolytics-dimension-repository_nwo">
                            <meta content="true" name="octolytics-dimension-repository_public">
                             <meta content="true" name="octolytics-dimension-repository_is_fork">
                              <meta content="306616805" name="octolytics-dimension-repository_parent_id">
                               <meta content="FnSK4R17s/awesome-segmentation-saliency-dataset" name="octolytics-dimension-repository_parent_nwo">
                                <meta content="163572209" name="octolytics-dimension-repository_network_root_id">
                                 <meta content="lartpang/awesome-segmentation-saliency-dataset" name="octolytics-dimension-repository_network_root_nwo"/>
                                 <link data-turbo-transient="" href="https://github.com/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset" rel="canonical"/>
                                 <meta content="logged-out env-production page-responsive" name="turbo-body-classes"/>
                                 <meta content="https://api.github.com/_private/browser/stats" name="browser-stats-url"/>
                                 <meta content="https://api.github.com/_private/browser/errors" name="browser-errors-url"/>
                                 <link color="#000000" href="https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg" rel="mask-icon"/>
                                 <link class="js-site-favicon" href="https://github.githubassets.com/favicons/favicon.png" rel="alternate icon" type="image/png"/>
                                 <link class="js-site-favicon" href="https://github.githubassets.com/favicons/favicon.svg" rel="icon" type="image/svg+xml"/>
                                 <meta content="#1e2327" name="theme-color"/>
                                 <meta content="light dark" name="color-scheme">
                                  <link crossorigin="use-credentials" href="/manifest.json" rel="manifest"/>
                                 </meta>
                                </meta>
                               </meta>
                              </meta>
                             </meta>
                            </meta>
                           </meta>
                          </meta>
                         </meta>
                        </meta>
                       </meta>
                      </meta>
                     </meta>
                    </meta>
                   </meta>
                  </meta>
                 </meta>
                </meta>
               </meta>
              </meta>
             </meta>
            </meta>
           </meta>
          </meta>
         </meta>
        </meta>
       </link>
      </link>
     </link>
    </link>
   </link>
  </link>
 </head>
 <body class="logged-out env-production page-responsive" style="word-wrap: break-word;">
  <div class="logged-out env-production page-responsive" data-turbo-body="" style="word-wrap: break-word;">
   <div class="position-relative js-header-wrapper">
    <h2 class="sr-only">
     Navigation Menu
    </h2>
    <a class="px-2 py-4 color-bg-accent-emphasis color-fg-on-emphasis show-on-focus js-skip-to-content" href="#start-of-content">
     Skip to content
    </a>
    <span class="progress-pjax-loader Progress position-fixed width-full" data-view-component="true">
     <span class="Progress-item progress-pjax-loader-bar left-0 top-0 color-bg-accent-emphasis" data-view-component="true" style="width: 0%;">
     </span>
    </span>
    <script crossorigin="anonymous" defer="defer" src="https://github.githubassets.com/assets/vendors-node_modules_primer_react_lib-esm_Button_IconButton_js-node_modules_primer_react_lib--b964b4-f8441db8b94b.js" type="application/javascript">
    </script>
    <script crossorigin="anonymous" defer="defer" src="https://github.githubassets.com/assets/keyboard-shortcuts-dialog-b4f13290b41c.js" type="application/javascript">
    </script>
    <react-partial data-ssr="false" partial-name="keyboard-shortcuts-dialog">
     <script data-target="react-partial.embeddedData" type="application/json">
      {"props":{"docsUrl":"https://docs.github.com/get-started/accessibility/keyboard-shortcuts"}}
     </script>
     <div data-target="react-partial.reactRoot">
     </div>
    </react-partial>
    <script crossorigin="anonymous" defer="defer" src="https://github.githubassets.com/assets/vendors-node_modules_github_remote-form_dist_index_js-node_modules_delegated-events_dist_inde-94fd67-9f3636b94e7e.js" type="application/javascript">
    </script>
    <script crossorigin="anonymous" defer="defer" src="https://github.githubassets.com/assets/sessions-ff11af600d3e.js" type="application/javascript">
    </script>
    <header class="Header-old header-logged-out js-details-container Details position-relative f4 py-3" data-color-mode="light" data-dark-theme="dark" data-light-theme="light" role="banner">
     <button aria-label="Toggle navigation" class="Header-backdrop d-lg-none border-0 position-fixed top-0 left-0 width-full height-full js-details-target" type="button">
      <span class="d-none">
       Toggle navigation
      </span>
     </button>
     <div class="d-flex flex-column flex-lg-row flex-items-center p-responsive height-full position-relative z-1">
      <div class="d-flex flex-justify-between flex-items-center width-full width-lg-auto">
       <a aria-label="Homepage" class="mr-lg-3 color-fg-inherit flex-order-2" data-ga-click="(Logged out) Header, go to homepage, icon:logo-wordmark" href="https://github.com/">
        <svg aria-hidden="true" class="octicon octicon-mark-github" data-view-component="true" height="32" version="1.1" viewbox="0 0 16 16" width="32">
         <path d="M8 0c4.42 0 8 3.58 8 8a8.013 8.013 0 0 1-5.45 7.59c-.4.08-.55-.17-.55-.38 0-.27.01-1.13.01-2.2 0-.75-.25-1.23-.54-1.48 1.78-.2 3.65-.88 3.65-3.95 0-.88-.31-1.59-.82-2.15.08-.2.36-1.02-.08-2.12 0 0-.67-.22-2.2.82-.64-.18-1.32-.27-2-.27-.68 0-1.36.09-2 .27-1.53-1.03-2.2-.82-2.2-.82-.44 1.1-.16 1.92-.08 2.12-.51.56-.82 1.28-.82 2.15 0 3.06 1.86 3.75 3.64 3.95-.23.2-.44.55-.51 1.07-.46.21-1.61.55-2.33-.66-.15-.24-.6-.83-1.23-.82-.67.01-.27.38.01.53.34.19.73.9.82 1.13.16.45.68 1.31 2.69.94 0 .67.01 1.3.01 1.49 0 .21-.15.45-.55.38A7.995 7.995 0 0 1 0 8c0-4.42 3.58-8 8-8Z">
         </path>
        </svg>
       </a>
       <div class="flex-1">
        <a class="d-inline-block d-lg-none flex-order-1 f5 no-underline border color-border-default rounded-2 px-2 py-1 color-fg-inherit" data-ga-click="(Logged out) Header, clicked Sign in, text:sign-in" data-hydro-click='{"event_type":"authentication.click","payload":{"location_in_page":"site header menu","repository_id":null,"auth_type":"SIGN_UP","originating_url":"https://github.com/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset","user_id":null}}' data-hydro-click-hmac="211adfa7ff6e6f8d70662dfdacb459dd515e9e8a6c8de9f97d4615c38cd51ff3" href="/login?return_to=https%3A%2F%2Fgithub.com%2FAIMonk-Labs-Private-Limited%2Fawesome-segmentation-saliency-dataset">
         Sign in
        </a>
       </div>
       <div class="flex-1 flex-order-2 text-right">
        <button aria-expanded="false" aria-label="Toggle navigation" class="js-details-target Button--link Button--medium Button d-lg-none color-fg-inherit p-1" data-view-component="true" type="button">
         <span class="Button-content">
          <span class="Button-label">
           <div class="HeaderMenu-toggle-bar rounded my-1">
           </div>
           <div class="HeaderMenu-toggle-bar rounded my-1">
           </div>
           <div class="HeaderMenu-toggle-bar rounded my-1">
           </div>
          </span>
         </span>
        </button>
       </div>
      </div>
      <div class="HeaderMenu--logged-out p-responsive height-fit position-lg-relative d-lg-flex flex-column flex-auto pt-7 pb-4 top-0">
       <div class="header-menu-wrapper d-flex flex-column flex-self-end flex-lg-row flex-justify-between flex-auto p-3 p-lg-0 rounded rounded-lg-0 mt-3 mt-lg-0">
        <nav aria-label="Global" class="mt-0 px-3 px-lg-0 mb-3 mb-lg-0">
         <ul class="d-lg-flex list-style-none">
          <li class="HeaderMenu-item position-relative flex-wrap flex-justify-between flex-items-center d-block d-lg-flex flex-lg-nowrap flex-lg-items-center js-details-container js-header-menu-item">
           <button aria-expanded="false" class="HeaderMenu-link border-0 width-full width-lg-auto px-0 px-lg-2 py-3 py-lg-2 no-wrap d-flex flex-items-center flex-justify-between js-details-target" type="button">
            Product
            <svg aria-hidden="true" class="octicon octicon-chevron-down HeaderMenu-icon ml-1" data-view-component="true" height="16" opacity="0.5" version="1.1" viewbox="0 0 16 16" width="16">
             <path d="M12.78 5.22a.749.749 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.06 0L3.22 6.28a.749.749 0 1 1 1.06-1.06L8 8.939l3.72-3.719a.749.749 0 0 1 1.06 0Z">
             </path>
            </svg>
           </button>
           <div class="HeaderMenu-dropdown dropdown-menu rounded m-0 p-0 py-2 py-lg-4 position-relative position-lg-absolute left-0 left-lg-n3 d-lg-flex dropdown-menu-wide">
            <div class="px-lg-4 border-lg-right mb-4 mb-lg-0 pr-lg-7">
             <ul class="list-style-none f5">
              <li>
               <a class="HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center pb-lg-3" data-analytics-event='{"category":"Header dropdown (logged out), Product","action":"click to go to Actions","label":"ref_cta:Actions;"}' href="/features/actions">
                <svg aria-hidden="true" class="octicon octicon-workflow color-fg-subtle mr-3" data-view-component="true" height="24" version="1.1" viewbox="0 0 24 24" width="24">
                 <path d="M1 3a2 2 0 0 1 2-2h6.5a2 2 0 0 1 2 2v6.5a2 2 0 0 1-2 2H7v4.063C7 16.355 7.644 17 8.438 17H12.5v-2.5a2 2 0 0 1 2-2H21a2 2 0 0 1 2 2V21a2 2 0 0 1-2 2h-6.5a2 2 0 0 1-2-2v-2.5H8.437A2.939 2.939 0 0 1 5.5 15.562V11.5H3a2 2 0 0 1-2-2Zm2-.5a.5.5 0 0 0-.5.5v6.5a.5.5 0 0 0 .5.5h6.5a.5.5 0 0 0 .5-.5V3a.5.5 0 0 0-.5-.5ZM14.5 14a.5.5 0 0 0-.5.5V21a.5.5 0 0 0 .5.5H21a.5.5 0 0 0 .5-.5v-6.5a.5.5 0 0 0-.5-.5Z">
                 </path>
                </svg>
                <div>
                 <div class="color-fg-default h4">
                  Actions
                 </div>
                 Automate any workflow
                </div>
               </a>
              </li>
              <li>
               <a class="HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center pb-lg-3" data-analytics-event='{"category":"Header dropdown (logged out), Product","action":"click to go to Packages","label":"ref_cta:Packages;"}' href="/features/packages">
                <svg aria-hidden="true" class="octicon octicon-package color-fg-subtle mr-3" data-view-component="true" height="24" version="1.1" viewbox="0 0 24 24" width="24">
                 <path d="M12.876.64V.639l8.25 4.763c.541.313.875.89.875 1.515v9.525a1.75 1.75 0 0 1-.875 1.516l-8.25 4.762a1.748 1.748 0 0 1-1.75 0l-8.25-4.763a1.75 1.75 0 0 1-.875-1.515V6.917c0-.625.334-1.202.875-1.515L11.126.64a1.748 1.748 0 0 1 1.75 0Zm-1 1.298L4.251 6.34l7.75 4.474 7.75-4.474-7.625-4.402a.248.248 0 0 0-.25 0Zm.875 19.123 7.625-4.402a.25.25 0 0 0 .125-.216V7.639l-7.75 4.474ZM3.501 7.64v8.803c0 .09.048.172.125.216l7.625 4.402v-8.947Z">
                 </path>
                </svg>
                <div>
                 <div class="color-fg-default h4">
                  Packages
                 </div>
                 Host and manage packages
                </div>
               </a>
              </li>
              <li>
               <a class="HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center pb-lg-3" data-analytics-event='{"category":"Header dropdown (logged out), Product","action":"click to go to Security","label":"ref_cta:Security;"}' href="/features/security">
                <svg aria-hidden="true" class="octicon octicon-shield-check color-fg-subtle mr-3" data-view-component="true" height="24" version="1.1" viewbox="0 0 24 24" width="24">
                 <path d="M16.53 9.78a.75.75 0 0 0-1.06-1.06L11 13.19l-1.97-1.97a.75.75 0 0 0-1.06 1.06l2.5 2.5a.75.75 0 0 0 1.06 0l5-5Z">
                 </path>
                 <path d="m12.54.637 8.25 2.675A1.75 1.75 0 0 1 22 4.976V10c0 6.19-3.771 10.704-9.401 12.83a1.704 1.704 0 0 1-1.198 0C5.77 20.705 2 16.19 2 10V4.976c0-.758.489-1.43 1.21-1.664L11.46.637a1.748 1.748 0 0 1 1.08 0Zm-.617 1.426-8.25 2.676a.249.249 0 0 0-.173.237V10c0 5.46 3.28 9.483 8.43 11.426a.199.199 0 0 0 .14 0C17.22 19.483 20.5 15.461 20.5 10V4.976a.25.25 0 0 0-.173-.237l-8.25-2.676a.253.253 0 0 0-.154 0Z">
                 </path>
                </svg>
                <div>
                 <div class="color-fg-default h4">
                  Security
                 </div>
                 Find and fix vulnerabilities
                </div>
               </a>
              </li>
              <li>
               <a class="HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center pb-lg-3" data-analytics-event='{"category":"Header dropdown (logged out), Product","action":"click to go to Codespaces","label":"ref_cta:Codespaces;"}' href="/features/codespaces">
                <svg aria-hidden="true" class="octicon octicon-codespaces color-fg-subtle mr-3" data-view-component="true" height="24" version="1.1" viewbox="0 0 24 24" width="24">
                 <path d="M3.5 3.75C3.5 2.784 4.284 2 5.25 2h13.5c.966 0 1.75.784 1.75 1.75v7.5A1.75 1.75 0 0 1 18.75 13H5.25a1.75 1.75 0 0 1-1.75-1.75Zm-2 12c0-.966.784-1.75 1.75-1.75h17.5c.966 0 1.75.784 1.75 1.75v4a1.75 1.75 0 0 1-1.75 1.75H3.25a1.75 1.75 0 0 1-1.75-1.75ZM5.25 3.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h13.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Zm-2 12a.25.25 0 0 0-.25.25v4c0 .138.112.25.25.25h17.5a.25.25 0 0 0 .25-.25v-4a.25.25 0 0 0-.25-.25Z">
                 </path>
                 <path d="M10 17.75a.75.75 0 0 1 .75-.75h6.5a.75.75 0 0 1 0 1.5h-6.5a.75.75 0 0 1-.75-.75Zm-4 0a.75.75 0 0 1 .75-.75h.5a.75.75 0 0 1 0 1.5h-.5a.75.75 0 0 1-.75-.75Z">
                 </path>
                </svg>
                <div>
                 <div class="color-fg-default h4">
                  Codespaces
                 </div>
                 Instant dev environments
                </div>
               </a>
              </li>
              <li>
               <a class="HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center pb-lg-3" data-analytics-event='{"category":"Header dropdown (logged out), Product","action":"click to go to Copilot","label":"ref_cta:Copilot;"}' href="/features/copilot">
                <svg aria-hidden="true" class="octicon octicon-copilot color-fg-subtle mr-3" data-view-component="true" height="24" version="1.1" viewbox="0 0 24 24" width="24">
                 <path d="M23.922 16.992c-.861 1.495-5.859 5.023-11.922 5.023-6.063 0-11.061-3.528-11.922-5.023A.641.641 0 0 1 0 16.736v-2.869a.841.841 0 0 1 .053-.22c.372-.935 1.347-2.292 2.605-2.656.167-.429.414-1.055.644-1.517a10.195 10.195 0 0 1-.052-1.086c0-1.331.282-2.499 1.132-3.368.397-.406.89-.717 1.474-.952 1.399-1.136 3.392-2.093 6.122-2.093 2.731 0 4.767.957 6.166 2.093.584.235 1.077.546 1.474.952.85.869 1.132 2.037 1.132 3.368 0 .368-.014.733-.052 1.086.23.462.477 1.088.644 1.517 1.258.364 2.233 1.721 2.605 2.656a.832.832 0 0 1 .053.22v2.869a.641.641 0 0 1-.078.256ZM12.172 11h-.344a4.323 4.323 0 0 1-.355.508C10.703 12.455 9.555 13 7.965 13c-1.725 0-2.989-.359-3.782-1.259a2.005 2.005 0 0 1-.085-.104L4 11.741v6.585c1.435.779 4.514 2.179 8 2.179 3.486 0 6.565-1.4 8-2.179v-6.585l-.098-.104s-.033.045-.085.104c-.793.9-2.057 1.259-3.782 1.259-1.59 0-2.738-.545-3.508-1.492a4.323 4.323 0 0 1-.355-.508h-.016.016Zm.641-2.935c.136 1.057.403 1.913.878 2.497.442.544 1.134.938 2.344.938 1.573 0 2.292-.337 2.657-.751.384-.435.558-1.15.558-2.361 0-1.14-.243-1.847-.705-2.319-.477-.488-1.319-.862-2.824-1.025-1.487-.161-2.192.138-2.533.529-.269.307-.437.808-.438 1.578v.021c0 .265.021.562.063.893Zm-1.626 0c.042-.331.063-.628.063-.894v-.02c-.001-.77-.169-1.271-.438-1.578-.341-.391-1.046-.69-2.533-.529-1.505.163-2.347.537-2.824 1.025-.462.472-.705 1.179-.705 2.319 0 1.211.175 1.926.558 2.361.365.414 1.084.751 2.657.751 1.21 0 1.902-.394 2.344-.938.475-.584.742-1.44.878-2.497Z">
                 </path>
                 <path d="M14.5 14.25a1 1 0 0 1 1 1v2a1 1 0 0 1-2 0v-2a1 1 0 0 1 1-1Zm-5 0a1 1 0 0 1 1 1v2a1 1 0 0 1-2 0v-2a1 1 0 0 1 1-1Z">
                 </path>
                </svg>
                <div>
                 <div class="color-fg-default h4">
                  Copilot
                 </div>
                 Write better code with AI
                </div>
               </a>
              </li>
              <li>
               <a class="HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center pb-lg-3" data-analytics-event='{"category":"Header dropdown (logged out), Product","action":"click to go to Code review","label":"ref_cta:Code review;"}' href="/features/code-review">
                <svg aria-hidden="true" class="octicon octicon-code-review color-fg-subtle mr-3" data-view-component="true" height="24" version="1.1" viewbox="0 0 24 24" width="24">
                 <path d="M10.3 6.74a.75.75 0 0 1-.04 1.06l-2.908 2.7 2.908 2.7a.75.75 0 1 1-1.02 1.1l-3.5-3.25a.75.75 0 0 1 0-1.1l3.5-3.25a.75.75 0 0 1 1.06.04Zm3.44 1.06a.75.75 0 1 1 1.02-1.1l3.5 3.25a.75.75 0 0 1 0 1.1l-3.5 3.25a.75.75 0 1 1-1.02-1.1l2.908-2.7-2.908-2.7Z">
                 </path>
                 <path d="M1.5 4.25c0-.966.784-1.75 1.75-1.75h17.5c.966 0 1.75.784 1.75 1.75v12.5a1.75 1.75 0 0 1-1.75 1.75h-9.69l-3.573 3.573A1.458 1.458 0 0 1 5 21.043V18.5H3.25a1.75 1.75 0 0 1-1.75-1.75ZM3.25 4a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h2.5a.75.75 0 0 1 .75.75v3.19l3.72-3.72a.749.749 0 0 1 .53-.22h10a.25.25 0 0 0 .25-.25V4.25a.25.25 0 0 0-.25-.25Z">
                 </path>
                </svg>
                <div>
                 <div class="color-fg-default h4">
                  Code review
                 </div>
                 Manage code changes
                </div>
               </a>
              </li>
              <li>
               <a class="HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center pb-lg-3" data-analytics-event='{"category":"Header dropdown (logged out), Product","action":"click to go to Issues","label":"ref_cta:Issues;"}' href="/features/issues">
                <svg aria-hidden="true" class="octicon octicon-issue-opened color-fg-subtle mr-3" data-view-component="true" height="24" version="1.1" viewbox="0 0 24 24" width="24">
                 <path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1ZM2.5 12a9.5 9.5 0 0 0 9.5 9.5 9.5 9.5 0 0 0 9.5-9.5A9.5 9.5 0 0 0 12 2.5 9.5 9.5 0 0 0 2.5 12Zm9.5 2a2 2 0 1 1-.001-3.999A2 2 0 0 1 12 14Z">
                 </path>
                </svg>
                <div>
                 <div class="color-fg-default h4">
                  Issues
                 </div>
                 Plan and track work
                </div>
               </a>
              </li>
              <li>
               <a class="HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center" data-analytics-event='{"category":"Header dropdown (logged out), Product","action":"click to go to Discussions","label":"ref_cta:Discussions;"}' href="/features/discussions">
                <svg aria-hidden="true" class="octicon octicon-comment-discussion color-fg-subtle mr-3" data-view-component="true" height="24" version="1.1" viewbox="0 0 24 24" width="24">
                 <path d="M1.75 1h12.5c.966 0 1.75.784 1.75 1.75v9.5A1.75 1.75 0 0 1 14.25 14H8.061l-2.574 2.573A1.458 1.458 0 0 1 3 15.543V14H1.75A1.75 1.75 0 0 1 0 12.25v-9.5C0 1.784.784 1 1.75 1ZM1.5 2.75v9.5c0 .138.112.25.25.25h2a.75.75 0 0 1 .75.75v2.19l2.72-2.72a.749.749 0 0 1 .53-.22h6.5a.25.25 0 0 0 .25-.25v-9.5a.25.25 0 0 0-.25-.25H1.75a.25.25 0 0 0-.25.25Z">
                 </path>
                 <path d="M22.5 8.75a.25.25 0 0 0-.25-.25h-3.5a.75.75 0 0 1 0-1.5h3.5c.966 0 1.75.784 1.75 1.75v9.5A1.75 1.75 0 0 1 22.25 20H21v1.543a1.457 1.457 0 0 1-2.487 1.03L15.939 20H10.75A1.75 1.75 0 0 1 9 18.25v-1.465a.75.75 0 0 1 1.5 0v1.465c0 .138.112.25.25.25h5.5a.75.75 0 0 1 .53.22l2.72 2.72v-2.19a.75.75 0 0 1 .75-.75h2a.25.25 0 0 0 .25-.25v-9.5Z">
                 </path>
                </svg>
                <div>
                 <div class="color-fg-default h4">
                  Discussions
                 </div>
                 Collaborate outside of code
                </div>
               </a>
              </li>
             </ul>
            </div>
            <div class="px-lg-4">
             <span class="d-block h4 color-fg-default my-1" id="product-explore-heading">
              Explore
             </span>
             <ul aria-labelledby="product-explore-heading" class="list-style-none f5">
              <li>
               <a class="HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary" data-analytics-event='{"category":"Header dropdown (logged out), Product","action":"click to go to All features","label":"ref_cta:All features;"}' href="/features">
                All features
               </a>
              </li>
              <li>
               <a class="HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary" data-analytics-event='{"category":"Header dropdown (logged out), Product","action":"click to go to Documentation","label":"ref_cta:Documentation;"}' href="https://docs.github.com" target="_blank">
                Documentation
                <svg aria-hidden="true" class="octicon octicon-link-external HeaderMenu-external-icon color-fg-subtle" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                 <path d="M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z">
                 </path>
                </svg>
               </a>
              </li>
              <li>
               <a class="HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary" data-analytics-event='{"category":"Header dropdown (logged out), Product","action":"click to go to GitHub Skills","label":"ref_cta:GitHub Skills;"}' href="https://skills.github.com" target="_blank">
                GitHub Skills
                <svg aria-hidden="true" class="octicon octicon-link-external HeaderMenu-external-icon color-fg-subtle" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                 <path d="M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z">
                 </path>
                </svg>
               </a>
              </li>
              <li>
               <a class="HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary" data-analytics-event='{"category":"Header dropdown (logged out), Product","action":"click to go to Blog","label":"ref_cta:Blog;"}' href="https://github.blog" target="_blank">
                Blog
                <svg aria-hidden="true" class="octicon octicon-link-external HeaderMenu-external-icon color-fg-subtle" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                 <path d="M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z">
                 </path>
                </svg>
               </a>
              </li>
             </ul>
            </div>
           </div>
          </li>
          <li class="HeaderMenu-item position-relative flex-wrap flex-justify-between flex-items-center d-block d-lg-flex flex-lg-nowrap flex-lg-items-center js-details-container js-header-menu-item">
           <button aria-expanded="false" class="HeaderMenu-link border-0 width-full width-lg-auto px-0 px-lg-2 py-3 py-lg-2 no-wrap d-flex flex-items-center flex-justify-between js-details-target" type="button">
            Solutions
            <svg aria-hidden="true" class="octicon octicon-chevron-down HeaderMenu-icon ml-1" data-view-component="true" height="16" opacity="0.5" version="1.1" viewbox="0 0 16 16" width="16">
             <path d="M12.78 5.22a.749.749 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.06 0L3.22 6.28a.749.749 0 1 1 1.06-1.06L8 8.939l3.72-3.719a.749.749 0 0 1 1.06 0Z">
             </path>
            </svg>
           </button>
           <div class="HeaderMenu-dropdown dropdown-menu rounded m-0 p-0 py-2 py-lg-4 position-relative position-lg-absolute left-0 left-lg-n3 px-lg-4">
            <div class="border-bottom pb-3 mb-3">
             <span class="d-block h4 color-fg-default my-1" id="solutions-for-heading">
              For
             </span>
             <ul aria-labelledby="solutions-for-heading" class="list-style-none f5">
              <li>
               <a class="HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary" data-analytics-event='{"category":"Header dropdown (logged out), Solutions","action":"click to go to Enterprise","label":"ref_cta:Enterprise;"}' href="/enterprise">
                Enterprise
               </a>
              </li>
              <li>
               <a class="HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary" data-analytics-event='{"category":"Header dropdown (logged out), Solutions","action":"click to go to Teams","label":"ref_cta:Teams;"}' href="/team">
                Teams
               </a>
              </li>
              <li>
               <a class="HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary" data-analytics-event='{"category":"Header dropdown (logged out), Solutions","action":"click to go to Startups","label":"ref_cta:Startups;"}' href="/enterprise/startups">
                Startups
               </a>
              </li>
              <li>
               <a class="HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary" data-analytics-event='{"category":"Header dropdown (logged out), Solutions","action":"click to go to Education","label":"ref_cta:Education;"}' href="https://education.github.com" target="_blank">
                Education
                <svg aria-hidden="true" class="octicon octicon-link-external HeaderMenu-external-icon color-fg-subtle" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                 <path d="M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z">
                 </path>
                </svg>
               </a>
              </li>
             </ul>
            </div>
            <div class="border-bottom pb-3 mb-3">
             <span class="d-block h4 color-fg-default my-1" id="solutions-by-solution-heading">
              By Solution
             </span>
             <ul aria-labelledby="solutions-by-solution-heading" class="list-style-none f5">
              <li>
               <a class="HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary" data-analytics-event='{"category":"Header dropdown (logged out), Solutions","action":"click to go to CI/CD &amp;amp; Automation","label":"ref_cta:CI/CD &amp;amp; Automation;"}' href="/solutions/ci-cd">
                CI/CD &amp; Automation
               </a>
              </li>
              <li>
               <a class="HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary" data-analytics-event='{"category":"Header dropdown (logged out), Solutions","action":"click to go to DevOps","label":"ref_cta:DevOps;"}' href="/solutions/devops">
                DevOps
               </a>
              </li>
              <li>
               <a class="HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary" data-analytics-event='{"category":"Header dropdown (logged out), Solutions","action":"click to go to DevSecOps","label":"ref_cta:DevSecOps;"}' href="https://resources.github.com/devops/fundamentals/devsecops" target="_blank">
                DevSecOps
                <svg aria-hidden="true" class="octicon octicon-link-external HeaderMenu-external-icon color-fg-subtle" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                 <path d="M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z">
                 </path>
                </svg>
               </a>
              </li>
             </ul>
            </div>
            <div class="">
             <span class="d-block h4 color-fg-default my-1" id="solutions-resources-heading">
              Resources
             </span>
             <ul aria-labelledby="solutions-resources-heading" class="list-style-none f5">
              <li>
               <a class="HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary" data-analytics-event='{"category":"Header dropdown (logged out), Solutions","action":"click to go to Learning Pathways","label":"ref_cta:Learning Pathways;"}' href="https://resources.github.com/learn/pathways" target="_blank">
                Learning Pathways
                <svg aria-hidden="true" class="octicon octicon-link-external HeaderMenu-external-icon color-fg-subtle" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                 <path d="M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z">
                 </path>
                </svg>
               </a>
              </li>
              <li>
               <a class="HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary" data-analytics-event='{"category":"Header dropdown (logged out), Solutions","action":"click to go to White papers, Ebooks, Webinars","label":"ref_cta:White papers, Ebooks, Webinars;"}' href="https://resources.github.com" target="_blank">
                White papers, Ebooks, Webinars
                <svg aria-hidden="true" class="octicon octicon-link-external HeaderMenu-external-icon color-fg-subtle" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                 <path d="M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z">
                 </path>
                </svg>
               </a>
              </li>
              <li>
               <a class="HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary" data-analytics-event='{"category":"Header dropdown (logged out), Solutions","action":"click to go to Customer Stories","label":"ref_cta:Customer Stories;"}' href="/customer-stories">
                Customer Stories
               </a>
              </li>
              <li>
               <a class="HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary" data-analytics-event='{"category":"Header dropdown (logged out), Solutions","action":"click to go to Partners","label":"ref_cta:Partners;"}' href="https://partner.github.com" target="_blank">
                Partners
                <svg aria-hidden="true" class="octicon octicon-link-external HeaderMenu-external-icon color-fg-subtle" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                 <path d="M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z">
                 </path>
                </svg>
               </a>
              </li>
             </ul>
            </div>
           </div>
          </li>
          <li class="HeaderMenu-item position-relative flex-wrap flex-justify-between flex-items-center d-block d-lg-flex flex-lg-nowrap flex-lg-items-center js-details-container js-header-menu-item">
           <button aria-expanded="false" class="HeaderMenu-link border-0 width-full width-lg-auto px-0 px-lg-2 py-3 py-lg-2 no-wrap d-flex flex-items-center flex-justify-between js-details-target" type="button">
            Open Source
            <svg aria-hidden="true" class="octicon octicon-chevron-down HeaderMenu-icon ml-1" data-view-component="true" height="16" opacity="0.5" version="1.1" viewbox="0 0 16 16" width="16">
             <path d="M12.78 5.22a.749.749 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.06 0L3.22 6.28a.749.749 0 1 1 1.06-1.06L8 8.939l3.72-3.719a.749.749 0 0 1 1.06 0Z">
             </path>
            </svg>
           </button>
           <div class="HeaderMenu-dropdown dropdown-menu rounded m-0 p-0 py-2 py-lg-4 position-relative position-lg-absolute left-0 left-lg-n3 px-lg-4">
            <div class="border-bottom pb-3 mb-3">
             <ul class="list-style-none f5">
              <li>
               <a class="HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center" data-analytics-event='{"category":"Header dropdown (logged out), Open Source","action":"click to go to GitHub Sponsors","label":"ref_cta:GitHub Sponsors;"}' href="/sponsors">
                <div>
                 <div class="color-fg-default h4">
                  GitHub Sponsors
                 </div>
                 Fund open source developers
                </div>
               </a>
              </li>
             </ul>
            </div>
            <div class="border-bottom pb-3 mb-3">
             <ul class="list-style-none f5">
              <li>
               <a class="HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center" data-analytics-event='{"category":"Header dropdown (logged out), Open Source","action":"click to go to The ReadME Project","label":"ref_cta:The ReadME Project;"}' href="/readme">
                <div>
                 <div class="color-fg-default h4">
                  The ReadME Project
                 </div>
                 GitHub community articles
                </div>
               </a>
              </li>
             </ul>
            </div>
            <div class="">
             <span class="d-block h4 color-fg-default my-1" id="open-source-repositories-heading">
              Repositories
             </span>
             <ul aria-labelledby="open-source-repositories-heading" class="list-style-none f5">
              <li>
               <a class="HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary" data-analytics-event='{"category":"Header dropdown (logged out), Open Source","action":"click to go to Topics","label":"ref_cta:Topics;"}' href="/topics">
                Topics
               </a>
              </li>
              <li>
               <a class="HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary" data-analytics-event='{"category":"Header dropdown (logged out), Open Source","action":"click to go to Trending","label":"ref_cta:Trending;"}' href="/trending">
                Trending
               </a>
              </li>
              <li>
               <a class="HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary" data-analytics-event='{"category":"Header dropdown (logged out), Open Source","action":"click to go to Collections","label":"ref_cta:Collections;"}' href="/collections">
                Collections
               </a>
              </li>
             </ul>
            </div>
           </div>
          </li>
          <li class="HeaderMenu-item position-relative flex-wrap flex-justify-between flex-items-center d-block d-lg-flex flex-lg-nowrap flex-lg-items-center js-details-container js-header-menu-item">
           <a class="HeaderMenu-link no-underline px-0 px-lg-2 py-3 py-lg-2 d-block d-lg-inline-block" data-analytics-event='{"category":"Header menu top item (logged out)","action":"click to go to Pricing","label":"ref_cta:Pricing;"}' href="/pricing">
            Pricing
           </a>
          </li>
         </ul>
        </nav>
        <div class="d-lg-flex flex-items-center mb-3 mb-lg-0 text-center text-lg-left ml-3" style="">
         <qbsearch-input class="search-input" data-blackbird-indexed-repo-csrf='&lt;esi:include src="/_esi/rails_csrf_token_form_hidden?r=FKBJvkKIXucgusA2lPsv79StGJBDiWrKqja4XqnIk33Gi7syfia6rErBHrz9Yj2K7M5bmtFgruuZb%2Fk2YDLm932gkm0V1KKbFgmljJv1kFJvGGrNlFVub4Mp7gupDWCLmd9w019aIeeyiRhyFTcdO2ZZTjQbb0wyY5PTq9sRs0qsos8vkT%2B7l1dLJUtdkn15psmS0JWhCzo%2BN2dlkFFcdaAgdO0KVlBqRVK3rVu4DGRYjjGe51qc2929ogEU%2F2Sbl36QCM41ixa5tfAWEmO91KpS6uEzL2lH2cGUngz4lbZJpuDAXMLlLrNFVElTGHop91y8t87FyariB%2FfzFtpUxiH6%2FUT3JmuF8kjJ9ED4ov%2FWaR6%2FlgqVkJB296YzCBaXARpv76f%2BD3q%2BmysLrdoeLf%2BgjtGr6E%2Fym8NuHN6y%2FXac0PrNtiTjJNtNgUMuv91mBhHgrijMOgZO9i6OS5y3scsRN8%2FliNtc0qhUqec4atZ%2BjxFwSY0y8yGaYque5t9y%2F0dFkFiw3mv9lvClqGiAOgm%2FA%2FL8fER2EmgFQyxUvLl3YK9DvJ9Fr35tlsbYAOlim1Ct5ZphTLd9n7nJ8f6IKRMEk7YMXhP8galg%2BEEu--E9vUaMkdogmHODAZ--eRot6oDEirNO%2Fubjg9Yolg%3D%3D" /&gt;' data-blackbird-suggestions-path="/search/suggestions" data-copilot-chat-enabled="false" data-current-org="AIMonk-Labs-Private-Limited" data-current-owner="" data-current-repository="AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="Cuu3Tzx_RsOoK5MbUfo0qiS3m8y6XAWxx8rEE6LEnGNRyEJGc8vvYnPvh39NmHE1F_Q4Rp3KxOiJxi6PH9keHg" data-header-redesign-enabled="false" data-initial-value="" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-logged-in="false" data-max-custom-scopes="10" data-scope="repo:AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset">
          <div class="search-input-container search-with-dialog position-relative d-flex flex-row flex-items-center mr-4 rounded" data-action="click:qbsearch-input#searchInputContainerClicked">
           <button autocapitalize="off" class="header-search-button placeholder input-button form-control d-flex flex-1 flex-self-stretch flex-items-center no-wrap width-full py-0 pl-2 pr-0 text-left border-0 box-shadow-none" data-action="click:qbsearch-input#handleExpand" data-hotkey="s,/" data-target="qbsearch-input.inputButton" placeholder="Search or jump to..." type="button">
            <div class="mr-2 color-fg-muted">
             <svg aria-hidden="true" class="octicon octicon-search" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
              <path d="M10.68 11.74a6 6 0 0 1-7.922-8.982 6 6 0 0 1 8.982 7.922l3.04 3.04a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215ZM11.5 7a4.499 4.499 0 1 0-8.997 0A4.499 4.499 0 0 0 11.5 7Z">
              </path>
             </svg>
            </div>
            <span class="flex-1" data-target="qbsearch-input.inputButtonText">
             Search or jump to...
            </span>
            <div class="d-flex" data-target="qbsearch-input.hotkeyIndicator">
             <svg aria-hidden="true" class="mr-1" height="20" width="22" xmlns="http://www.w3.org/2000/svg">
              <path d="M3.5.5h12c1.7 0 3 1.3 3 3v13c0 1.7-1.3 3-3 3h-12c-1.7 0-3-1.3-3-3v-13c0-1.7 1.3-3 3-3z" fill="none" opacity=".4" stroke="#979A9C">
              </path>
              <path d="M11.8 6L8 15.1h-.9L10.8 6h1z" fill="#979A9C">
              </path>
             </svg>
            </div>
           </button>
           <input class="js-site-search-type-field" name="type" type="hidden"/>
           <div class="Overlay--hidden" data-modal-dialog-overlay="">
            <modal-dialog aria-labelledby="search-suggestions-dialog-header" aria-modal="true" class="Overlay Overlay--width-large Overlay--height-auto" data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" data-view-component="true" id="search-suggestions-dialog" role="dialog">
             <h1 class="sr-only" id="search-suggestions-dialog-header">
              Search code, repositories, users, issues, pull requests...
             </h1>
             <div class="Overlay-body Overlay-body--paddingNone">
              <div data-view-component="true">
               <div class="search-suggestions position-fixed width-full color-shadow-large border color-fg-default color-bg-default overflow-hidden d-flex flex-column query-builder-container" data-target="qbsearch-input.queryBuilderContainer" hidden="" style="border-radius: 12px;">
                <!-- '"` -->
                <!-- </textarea></xmp> -->
                <form accept-charset="UTF-8" action="" id="query-builder-test-form" method="get">
                 <query-builder class="QueryBuilder search-query-builder" data-filter-key=":" data-target="qbsearch-input.queryBuilder" data-view-component="true" id="query-builder-query-builder-test">
                  <div class="FormControl FormControl--fullWidth">
                   <label class="FormControl-label sr-only" for="query-builder-test" id="query-builder-test-label">
                    Search
                   </label>
                   <div class="QueryBuilder-StyledInput width-fit" data-target="query-builder.styledInput">
                    <span class="FormControl-input-leadingVisualWrap QueryBuilder-leadingVisualWrap" id="query-builder-test-leadingvisual-wrap">
                     <svg aria-hidden="true" class="octicon octicon-search FormControl-input-leadingVisual" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                      <path d="M10.68 11.74a6 6 0 0 1-7.922-8.982 6 6 0 0 1 8.982 7.922l3.04 3.04a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215ZM11.5 7a4.499 4.499 0 1 0-8.997 0A4.499 4.499 0 0 0 11.5 7Z">
                      </path>
                     </svg>
                    </span>
                    <div class="QueryBuilder-StyledInputContainer" data-target="query-builder.styledInputContainer">
                     <div aria-hidden="true" class="QueryBuilder-StyledInputContent" data-target="query-builder.styledInputContent">
                     </div>
                     <div class="QueryBuilder-InputWrapper">
                      <div aria-hidden="true" class="QueryBuilder-Sizer" data-target="query-builder.sizer">
                      </div>
                      <input aria-describedby="validation-4d0e00a9-7685-4bd0-9019-3f399c5a9b84" aria-expanded="false" autocomplete="off" class="FormControl-input QueryBuilder-Input FormControl-medium" data-action="
          input:query-builder#inputChange
          blur:query-builder#inputBlur
          keydown:query-builder#inputKeydown
          focus:query-builder#inputFocus
        " data-target="query-builder.input" data-view-component="true" id="query-builder-test" name="query-builder-test" role="combobox" spellcheck="false" type="text" value="">
                      </input>
                     </div>
                    </div>
                    <span class="sr-only" id="query-builder-test-clear">
                     Clear
                    </span>
                    <button aria-labelledby="query-builder-test-clear query-builder-test-label" class="Button Button--iconOnly Button--invisible Button--medium mr-1 px-2 py-0 d-flex flex-items-center rounded-1 color-fg-muted" data-action="
                click:query-builder#clear
                focus:query-builder#clearButtonFocus
                blur:query-builder#clearButtonBlur
              " data-target="query-builder.clearButton" data-view-component="true" hidden="hidden" id="query-builder-test-clear-button" role="button" type="button" variant="small">
                     <svg aria-hidden="true" class="octicon octicon-x-circle-fill Button-visual" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                      <path d="M2.343 13.657A8 8 0 1 1 13.658 2.343 8 8 0 0 1 2.343 13.657ZM6.03 4.97a.751.751 0 0 0-1.042.018.751.751 0 0 0-.018 1.042L6.94 8 4.97 9.97a.749.749 0 0 0 .326 1.275.749.749 0 0 0 .734-.215L8 9.06l1.97 1.97a.749.749 0 0 0 1.275-.326.749.749 0 0 0-.215-.734L9.06 8l1.97-1.97a.749.749 0 0 0-.326-1.275.749.749 0 0 0-.734.215L8 6.94Z">
                      </path>
                     </svg>
                    </button>
                   </div>
                   <template id="search-icon">
                    <svg aria-hidden="true" class="octicon octicon-search" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                     <path d="M10.68 11.74a6 6 0 0 1-7.922-8.982 6 6 0 0 1 8.982 7.922l3.04 3.04a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215ZM11.5 7a4.499 4.499 0 1 0-8.997 0A4.499 4.499 0 0 0 11.5 7Z">
                     </path>
                    </svg>
                   </template>
                   <template id="code-icon">
                    <svg aria-hidden="true" class="octicon octicon-code" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                     <path d="m11.28 3.22 4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.275-.326.749.749 0 0 1 .215-.734L13.94 8l-3.72-3.72a.749.749 0 0 1 .326-1.275.749.749 0 0 1 .734.215Zm-6.56 0a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042L2.06 8l3.72 3.72a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L.47 8.53a.75.75 0 0 1 0-1.06Z">
                     </path>
                    </svg>
                   </template>
                   <template id="file-code-icon">
                    <svg aria-hidden="true" class="octicon octicon-file-code" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                     <path d="M4 1.75C4 .784 4.784 0 5.75 0h5.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v8.586A1.75 1.75 0 0 1 14.25 15h-9a.75.75 0 0 1 0-1.5h9a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 10 4.25V1.5H5.75a.25.25 0 0 0-.25.25v2.5a.75.75 0 0 1-1.5 0Zm1.72 4.97a.75.75 0 0 1 1.06 0l2 2a.75.75 0 0 1 0 1.06l-2 2a.749.749 0 0 1-1.275-.326.749.749 0 0 1 .215-.734l1.47-1.47-1.47-1.47a.75.75 0 0 1 0-1.06ZM3.28 7.78 1.81 9.25l1.47 1.47a.751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018l-2-2a.75.75 0 0 1 0-1.06l2-2a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042Zm8.22-6.218V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z">
                     </path>
                    </svg>
                   </template>
                   <template id="history-icon">
                    <svg aria-hidden="true" class="octicon octicon-history" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                     <path d="m.427 1.927 1.215 1.215a8.002 8.002 0 1 1-1.6 5.685.75.75 0 1 1 1.493-.154 6.5 6.5 0 1 0 1.18-4.458l1.358 1.358A.25.25 0 0 1 3.896 6H.25A.25.25 0 0 1 0 5.75V2.104a.25.25 0 0 1 .427-.177ZM7.75 4a.75.75 0 0 1 .75.75v2.992l2.028.812a.75.75 0 0 1-.557 1.392l-2.5-1A.751.751 0 0 1 7 8.25v-3.5A.75.75 0 0 1 7.75 4Z">
                     </path>
                    </svg>
                   </template>
                   <template id="repo-icon">
                    <svg aria-hidden="true" class="octicon octicon-repo" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                     <path d="M2 2.5A2.5 2.5 0 0 1 4.5 0h8.75a.75.75 0 0 1 .75.75v12.5a.75.75 0 0 1-.75.75h-2.5a.75.75 0 0 1 0-1.5h1.75v-2h-8a1 1 0 0 0-.714 1.7.75.75 0 1 1-1.072 1.05A2.495 2.495 0 0 1 2 11.5Zm10.5-1h-8a1 1 0 0 0-1 1v6.708A2.486 2.486 0 0 1 4.5 9h8ZM5 12.25a.25.25 0 0 1 .25-.25h3.5a.25.25 0 0 1 .25.25v3.25a.25.25 0 0 1-.4.2l-1.45-1.087a.249.249 0 0 0-.3 0L5.4 15.7a.25.25 0 0 1-.4-.2Z">
                     </path>
                    </svg>
                   </template>
                   <template id="bookmark-icon">
                    <svg aria-hidden="true" class="octicon octicon-bookmark" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                     <path d="M3 2.75C3 1.784 3.784 1 4.75 1h6.5c.966 0 1.75.784 1.75 1.75v11.5a.75.75 0 0 1-1.227.579L8 11.722l-3.773 3.107A.751.751 0 0 1 3 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v9.91l3.023-2.489a.75.75 0 0 1 .954 0l3.023 2.49V2.75a.25.25 0 0 0-.25-.25Z">
                     </path>
                    </svg>
                   </template>
                   <template id="plus-circle-icon">
                    <svg aria-hidden="true" class="octicon octicon-plus-circle" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                     <path d="M8 0a8 8 0 1 1 0 16A8 8 0 0 1 8 0ZM1.5 8a6.5 6.5 0 1 0 13 0 6.5 6.5 0 0 0-13 0Zm7.25-3.25v2.5h2.5a.75.75 0 0 1 0 1.5h-2.5v2.5a.75.75 0 0 1-1.5 0v-2.5h-2.5a.75.75 0 0 1 0-1.5h2.5v-2.5a.75.75 0 0 1 1.5 0Z">
                     </path>
                    </svg>
                   </template>
                   <template id="circle-icon">
                    <svg aria-hidden="true" class="octicon octicon-dot-fill" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                     <path d="M8 4a4 4 0 1 1 0 8 4 4 0 0 1 0-8Z">
                     </path>
                    </svg>
                   </template>
                   <template id="trash-icon">
                    <svg aria-hidden="true" class="octicon octicon-trash" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                     <path d="M11 1.75V3h2.25a.75.75 0 0 1 0 1.5H2.75a.75.75 0 0 1 0-1.5H5V1.75C5 .784 5.784 0 6.75 0h2.5C10.216 0 11 .784 11 1.75ZM4.496 6.675l.66 6.6a.25.25 0 0 0 .249.225h5.19a.25.25 0 0 0 .249-.225l.66-6.6a.75.75 0 0 1 1.492.149l-.66 6.6A1.748 1.748 0 0 1 10.595 15h-5.19a1.75 1.75 0 0 1-1.741-1.575l-.66-6.6a.75.75 0 1 1 1.492-.15ZM6.5 1.75V3h3V1.75a.25.25 0 0 0-.25-.25h-2.5a.25.25 0 0 0-.25.25Z">
                     </path>
                    </svg>
                   </template>
                   <template id="team-icon">
                    <svg aria-hidden="true" class="octicon octicon-people" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                     <path d="M2 5.5a3.5 3.5 0 1 1 5.898 2.549 5.508 5.508 0 0 1 3.034 4.084.75.75 0 1 1-1.482.235 4 4 0 0 0-7.9 0 .75.75 0 0 1-1.482-.236A5.507 5.507 0 0 1 3.102 8.05 3.493 3.493 0 0 1 2 5.5ZM11 4a3.001 3.001 0 0 1 2.22 5.018 5.01 5.01 0 0 1 2.56 3.012.749.749 0 0 1-.885.954.752.752 0 0 1-.549-.514 3.507 3.507 0 0 0-2.522-2.372.75.75 0 0 1-.574-.73v-.352a.75.75 0 0 1 .416-.672A1.5 1.5 0 0 0 11 5.5.75.75 0 0 1 11 4Zm-5.5-.5a2 2 0 1 0-.001 3.999A2 2 0 0 0 5.5 3.5Z">
                     </path>
                    </svg>
                   </template>
                   <template id="project-icon">
                    <svg aria-hidden="true" class="octicon octicon-project" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                     <path d="M1.75 0h12.5C15.216 0 16 .784 16 1.75v12.5A1.75 1.75 0 0 1 14.25 16H1.75A1.75 1.75 0 0 1 0 14.25V1.75C0 .784.784 0 1.75 0ZM1.5 1.75v12.5c0 .138.112.25.25.25h12.5a.25.25 0 0 0 .25-.25V1.75a.25.25 0 0 0-.25-.25H1.75a.25.25 0 0 0-.25.25ZM11.75 3a.75.75 0 0 1 .75.75v7.5a.75.75 0 0 1-1.5 0v-7.5a.75.75 0 0 1 .75-.75Zm-8.25.75a.75.75 0 0 1 1.5 0v5.5a.75.75 0 0 1-1.5 0ZM8 3a.75.75 0 0 1 .75.75v3.5a.75.75 0 0 1-1.5 0v-3.5A.75.75 0 0 1 8 3Z">
                     </path>
                    </svg>
                   </template>
                   <template id="pencil-icon">
                    <svg aria-hidden="true" class="octicon octicon-pencil" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                     <path d="M11.013 1.427a1.75 1.75 0 0 1 2.474 0l1.086 1.086a1.75 1.75 0 0 1 0 2.474l-8.61 8.61c-.21.21-.47.364-.756.445l-3.251.93a.75.75 0 0 1-.927-.928l.929-3.25c.081-.286.235-.547.445-.758l8.61-8.61Zm.176 4.823L9.75 4.81l-6.286 6.287a.253.253 0 0 0-.064.108l-.558 1.953 1.953-.558a.253.253 0 0 0 .108-.064Zm1.238-3.763a.25.25 0 0 0-.354 0L10.811 3.75l1.439 1.44 1.263-1.263a.25.25 0 0 0 0-.354Z">
                     </path>
                    </svg>
                   </template>
                   <template id="copilot-icon">
                    <svg aria-hidden="true" class="octicon octicon-copilot" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                     <path d="M7.998 15.035c-4.562 0-7.873-2.914-7.998-3.749V9.338c.085-.628.677-1.686 1.588-2.065.013-.07.024-.143.036-.218.029-.183.06-.384.126-.612-.201-.508-.254-1.084-.254-1.656 0-.87.128-1.769.693-2.484.579-.733 1.494-1.124 2.724-1.261 1.206-.134 2.262.034 2.944.765.05.053.096.108.139.165.044-.057.094-.112.143-.165.682-.731 1.738-.899 2.944-.765 1.23.137 2.145.528 2.724 1.261.566.715.693 1.614.693 2.484 0 .572-.053 1.148-.254 1.656.066.228.098.429.126.612.012.076.024.148.037.218.924.385 1.522 1.471 1.591 2.095v1.872c0 .766-3.351 3.795-8.002 3.795Zm0-1.485c2.28 0 4.584-1.11 5.002-1.433V7.862l-.023-.116c-.49.21-1.075.291-1.727.291-1.146 0-2.059-.327-2.71-.991A3.222 3.222 0 0 1 8 6.303a3.24 3.24 0 0 1-.544.743c-.65.664-1.563.991-2.71.991-.652 0-1.236-.081-1.727-.291l-.023.116v4.255c.419.323 2.722 1.433 5.002 1.433ZM6.762 2.83c-.193-.206-.637-.413-1.682-.297-1.019.113-1.479.404-1.713.7-.247.312-.369.789-.369 1.554 0 .793.129 1.171.308 1.371.162.181.519.379 1.442.379.853 0 1.339-.235 1.638-.54.315-.322.527-.827.617-1.553.117-.935-.037-1.395-.241-1.614Zm4.155-.297c-1.044-.116-1.488.091-1.681.297-.204.219-.359.679-.242 1.614.091.726.303 1.231.618 1.553.299.305.784.54 1.638.54.922 0 1.28-.198 1.442-.379.179-.2.308-.578.308-1.371 0-.765-.123-1.242-.37-1.554-.233-.296-.693-.587-1.713-.7Z">
                     </path>
                     <path d="M6.25 9.037a.75.75 0 0 1 .75.75v1.501a.75.75 0 0 1-1.5 0V9.787a.75.75 0 0 1 .75-.75Zm4.25.75v1.501a.75.75 0 0 1-1.5 0V9.787a.75.75 0 0 1 1.5 0Z">
                     </path>
                    </svg>
                   </template>
                   <template id="workflow-icon">
                    <svg aria-hidden="true" class="octicon octicon-workflow" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                     <path d="M0 1.75C0 .784.784 0 1.75 0h3.5C6.216 0 7 .784 7 1.75v3.5A1.75 1.75 0 0 1 5.25 7H4v4a1 1 0 0 0 1 1h4v-1.25C9 9.784 9.784 9 10.75 9h3.5c.966 0 1.75.784 1.75 1.75v3.5A1.75 1.75 0 0 1 14.25 16h-3.5A1.75 1.75 0 0 1 9 14.25v-.75H5A2.5 2.5 0 0 1 2.5 11V7h-.75A1.75 1.75 0 0 1 0 5.25Zm1.75-.25a.25.25 0 0 0-.25.25v3.5c0 .138.112.25.25.25h3.5a.25.25 0 0 0 .25-.25v-3.5a.25.25 0 0 0-.25-.25Zm9 9a.25.25 0 0 0-.25.25v3.5c0 .138.112.25.25.25h3.5a.25.25 0 0 0 .25-.25v-3.5a.25.25 0 0 0-.25-.25Z">
                     </path>
                    </svg>
                   </template>
                   <template id="book-icon">
                    <svg aria-hidden="true" class="octicon octicon-book" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                     <path d="M0 1.75A.75.75 0 0 1 .75 1h4.253c1.227 0 2.317.59 3 1.501A3.743 3.743 0 0 1 11.006 1h4.245a.75.75 0 0 1 .75.75v10.5a.75.75 0 0 1-.75.75h-4.507a2.25 2.25 0 0 0-1.591.659l-.622.621a.75.75 0 0 1-1.06 0l-.622-.621A2.25 2.25 0 0 0 5.258 13H.75a.75.75 0 0 1-.75-.75Zm7.251 10.324.004-5.073-.002-2.253A2.25 2.25 0 0 0 5.003 2.5H1.5v9h3.757a3.75 3.75 0 0 1 1.994.574ZM8.755 4.75l-.004 7.322a3.752 3.752 0 0 1 1.992-.572H14.5v-9h-3.495a2.25 2.25 0 0 0-2.25 2.25Z">
                     </path>
                    </svg>
                   </template>
                   <template id="code-review-icon">
                    <svg aria-hidden="true" class="octicon octicon-code-review" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                     <path d="M1.75 1h12.5c.966 0 1.75.784 1.75 1.75v8.5A1.75 1.75 0 0 1 14.25 13H8.061l-2.574 2.573A1.458 1.458 0 0 1 3 14.543V13H1.75A1.75 1.75 0 0 1 0 11.25v-8.5C0 1.784.784 1 1.75 1ZM1.5 2.75v8.5c0 .138.112.25.25.25h2a.75.75 0 0 1 .75.75v2.19l2.72-2.72a.749.749 0 0 1 .53-.22h6.5a.25.25 0 0 0 .25-.25v-8.5a.25.25 0 0 0-.25-.25H1.75a.25.25 0 0 0-.25.25Zm5.28 1.72a.75.75 0 0 1 0 1.06L5.31 7l1.47 1.47a.751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018l-2-2a.75.75 0 0 1 0-1.06l2-2a.75.75 0 0 1 1.06 0Zm2.44 0a.75.75 0 0 1 1.06 0l2 2a.75.75 0 0 1 0 1.06l-2 2a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L10.69 7 9.22 5.53a.75.75 0 0 1 0-1.06Z">
                     </path>
                    </svg>
                   </template>
                   <template id="codespaces-icon">
                    <svg aria-hidden="true" class="octicon octicon-codespaces" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                     <path d="M0 11.25c0-.966.784-1.75 1.75-1.75h12.5c.966 0 1.75.784 1.75 1.75v3A1.75 1.75 0 0 1 14.25 16H1.75A1.75 1.75 0 0 1 0 14.25Zm2-9.5C2 .784 2.784 0 3.75 0h8.5C13.216 0 14 .784 14 1.75v5a1.75 1.75 0 0 1-1.75 1.75h-8.5A1.75 1.75 0 0 1 2 6.75Zm1.75-.25a.25.25 0 0 0-.25.25v5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-5a.25.25 0 0 0-.25-.25Zm-2 9.5a.25.25 0 0 0-.25.25v3c0 .138.112.25.25.25h12.5a.25.25 0 0 0 .25-.25v-3a.25.25 0 0 0-.25-.25Z">
                     </path>
                     <path d="M7 12.75a.75.75 0 0 1 .75-.75h4.5a.75.75 0 0 1 0 1.5h-4.5a.75.75 0 0 1-.75-.75Zm-4 0a.75.75 0 0 1 .75-.75h.5a.75.75 0 0 1 0 1.5h-.5a.75.75 0 0 1-.75-.75Z">
                     </path>
                    </svg>
                   </template>
                   <template id="comment-icon">
                    <svg aria-hidden="true" class="octicon octicon-comment" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                     <path d="M1 2.75C1 1.784 1.784 1 2.75 1h10.5c.966 0 1.75.784 1.75 1.75v7.5A1.75 1.75 0 0 1 13.25 12H9.06l-2.573 2.573A1.458 1.458 0 0 1 4 13.543V12H2.75A1.75 1.75 0 0 1 1 10.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h2a.75.75 0 0 1 .75.75v2.19l2.72-2.72a.749.749 0 0 1 .53-.22h4.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z">
                     </path>
                    </svg>
                   </template>
                   <template id="comment-discussion-icon">
                    <svg aria-hidden="true" class="octicon octicon-comment-discussion" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                     <path d="M1.75 1h8.5c.966 0 1.75.784 1.75 1.75v5.5A1.75 1.75 0 0 1 10.25 10H7.061l-2.574 2.573A1.458 1.458 0 0 1 2 11.543V10h-.25A1.75 1.75 0 0 1 0 8.25v-5.5C0 1.784.784 1 1.75 1ZM1.5 2.75v5.5c0 .138.112.25.25.25h1a.75.75 0 0 1 .75.75v2.19l2.72-2.72a.749.749 0 0 1 .53-.22h3.5a.25.25 0 0 0 .25-.25v-5.5a.25.25 0 0 0-.25-.25h-8.5a.25.25 0 0 0-.25.25Zm13 2a.25.25 0 0 0-.25-.25h-.5a.75.75 0 0 1 0-1.5h.5c.966 0 1.75.784 1.75 1.75v5.5A1.75 1.75 0 0 1 14.25 12H14v1.543a1.458 1.458 0 0 1-2.487 1.03L9.22 12.28a.749.749 0 0 1 .326-1.275.749.749 0 0 1 .734.215l2.22 2.22v-2.19a.75.75 0 0 1 .75-.75h1a.25.25 0 0 0 .25-.25Z">
                     </path>
                    </svg>
                   </template>
                   <template id="organization-icon">
                    <svg aria-hidden="true" class="octicon octicon-organization" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                     <path d="M1.75 16A1.75 1.75 0 0 1 0 14.25V1.75C0 .784.784 0 1.75 0h8.5C11.216 0 12 .784 12 1.75v12.5c0 .085-.006.168-.018.25h2.268a.25.25 0 0 0 .25-.25V8.285a.25.25 0 0 0-.111-.208l-1.055-.703a.749.749 0 1 1 .832-1.248l1.055.703c.487.325.779.871.779 1.456v5.965A1.75 1.75 0 0 1 14.25 16h-3.5a.766.766 0 0 1-.197-.026c-.099.017-.2.026-.303.026h-3a.75.75 0 0 1-.75-.75V14h-1v1.25a.75.75 0 0 1-.75.75Zm-.25-1.75c0 .138.112.25.25.25H4v-1.25a.75.75 0 0 1 .75-.75h2.5a.75.75 0 0 1 .75.75v1.25h2.25a.25.25 0 0 0 .25-.25V1.75a.25.25 0 0 0-.25-.25h-8.5a.25.25 0 0 0-.25.25ZM3.75 6h.5a.75.75 0 0 1 0 1.5h-.5a.75.75 0 0 1 0-1.5ZM3 3.75A.75.75 0 0 1 3.75 3h.5a.75.75 0 0 1 0 1.5h-.5A.75.75 0 0 1 3 3.75Zm4 3A.75.75 0 0 1 7.75 6h.5a.75.75 0 0 1 0 1.5h-.5A.75.75 0 0 1 7 6.75ZM7.75 3h.5a.75.75 0 0 1 0 1.5h-.5a.75.75 0 0 1 0-1.5ZM3 9.75A.75.75 0 0 1 3.75 9h.5a.75.75 0 0 1 0 1.5h-.5A.75.75 0 0 1 3 9.75ZM7.75 9h.5a.75.75 0 0 1 0 1.5h-.5a.75.75 0 0 1 0-1.5Z">
                     </path>
                    </svg>
                   </template>
                   <template id="rocket-icon">
                    <svg aria-hidden="true" class="octicon octicon-rocket" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                     <path d="M14.064 0h.186C15.216 0 16 .784 16 1.75v.186a8.752 8.752 0 0 1-2.564 6.186l-.458.459c-.314.314-.641.616-.979.904v3.207c0 .608-.315 1.172-.833 1.49l-2.774 1.707a.749.749 0 0 1-1.11-.418l-.954-3.102a1.214 1.214 0 0 1-.145-.125L3.754 9.816a1.218 1.218 0 0 1-.124-.145L.528 8.717a.749.749 0 0 1-.418-1.11l1.71-2.774A1.748 1.748 0 0 1 3.31 4h3.204c.288-.338.59-.665.904-.979l.459-.458A8.749 8.749 0 0 1 14.064 0ZM8.938 3.623h-.002l-.458.458c-.76.76-1.437 1.598-2.02 2.5l-1.5 2.317 2.143 2.143 2.317-1.5c.902-.583 1.74-1.26 2.499-2.02l.459-.458a7.25 7.25 0 0 0 2.123-5.127V1.75a.25.25 0 0 0-.25-.25h-.186a7.249 7.249 0 0 0-5.125 2.123ZM3.56 14.56c-.732.732-2.334 1.045-3.005 1.148a.234.234 0 0 1-.201-.064.234.234 0 0 1-.064-.201c.103-.671.416-2.273 1.15-3.003a1.502 1.502 0 1 1 2.12 2.12Zm6.94-3.935c-.088.06-.177.118-.266.175l-2.35 1.521.548 1.783 1.949-1.2a.25.25 0 0 0 .119-.213ZM3.678 8.116 5.2 5.766c.058-.09.117-.178.176-.266H3.309a.25.25 0 0 0-.213.119l-1.2 1.95ZM12 5a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z">
                     </path>
                    </svg>
                   </template>
                   <template id="shield-check-icon">
                    <svg aria-hidden="true" class="octicon octicon-shield-check" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                     <path d="m8.533.133 5.25 1.68A1.75 1.75 0 0 1 15 3.48V7c0 1.566-.32 3.182-1.303 4.682-.983 1.498-2.585 2.813-5.032 3.855a1.697 1.697 0 0 1-1.33 0c-2.447-1.042-4.049-2.357-5.032-3.855C1.32 10.182 1 8.566 1 7V3.48a1.75 1.75 0 0 1 1.217-1.667l5.25-1.68a1.748 1.748 0 0 1 1.066 0Zm-.61 1.429.001.001-5.25 1.68a.251.251 0 0 0-.174.237V7c0 1.36.275 2.666 1.057 3.859.784 1.194 2.121 2.342 4.366 3.298a.196.196 0 0 0 .154 0c2.245-.957 3.582-2.103 4.366-3.297C13.225 9.666 13.5 8.358 13.5 7V3.48a.25.25 0 0 0-.174-.238l-5.25-1.68a.25.25 0 0 0-.153 0ZM11.28 6.28l-3.5 3.5a.75.75 0 0 1-1.06 0l-1.5-1.5a.749.749 0 0 1 .326-1.275.749.749 0 0 1 .734.215l.97.97 2.97-2.97a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042Z">
                     </path>
                    </svg>
                   </template>
                   <template id="heart-icon">
                    <svg aria-hidden="true" class="octicon octicon-heart" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                     <path d="m8 14.25.345.666a.75.75 0 0 1-.69 0l-.008-.004-.018-.01a7.152 7.152 0 0 1-.31-.17 22.055 22.055 0 0 1-3.434-2.414C2.045 10.731 0 8.35 0 5.5 0 2.836 2.086 1 4.25 1 5.797 1 7.153 1.802 8 3.02 8.847 1.802 10.203 1 11.75 1 13.914 1 16 2.836 16 5.5c0 2.85-2.045 5.231-3.885 6.818a22.066 22.066 0 0 1-3.744 2.584l-.018.01-.006.003h-.002ZM4.25 2.5c-1.336 0-2.75 1.164-2.75 3 0 2.15 1.58 4.144 3.365 5.682A20.58 20.58 0 0 0 8 13.393a20.58 20.58 0 0 0 3.135-2.211C12.92 9.644 14.5 7.65 14.5 5.5c0-1.836-1.414-3-2.75-3-1.373 0-2.609.986-3.029 2.456a.749.749 0 0 1-1.442 0C6.859 3.486 5.623 2.5 4.25 2.5Z">
                     </path>
                    </svg>
                   </template>
                   <template id="server-icon">
                    <svg aria-hidden="true" class="octicon octicon-server" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                     <path d="M1.75 1h12.5c.966 0 1.75.784 1.75 1.75v4c0 .372-.116.717-.314 1 .198.283.314.628.314 1v4a1.75 1.75 0 0 1-1.75 1.75H1.75A1.75 1.75 0 0 1 0 12.75v-4c0-.358.109-.707.314-1a1.739 1.739 0 0 1-.314-1v-4C0 1.784.784 1 1.75 1ZM1.5 2.75v4c0 .138.112.25.25.25h12.5a.25.25 0 0 0 .25-.25v-4a.25.25 0 0 0-.25-.25H1.75a.25.25 0 0 0-.25.25Zm.25 5.75a.25.25 0 0 0-.25.25v4c0 .138.112.25.25.25h12.5a.25.25 0 0 0 .25-.25v-4a.25.25 0 0 0-.25-.25ZM7 4.75A.75.75 0 0 1 7.75 4h4.5a.75.75 0 0 1 0 1.5h-4.5A.75.75 0 0 1 7 4.75ZM7.75 10h4.5a.75.75 0 0 1 0 1.5h-4.5a.75.75 0 0 1 0-1.5ZM3 4.75A.75.75 0 0 1 3.75 4h.5a.75.75 0 0 1 0 1.5h-.5A.75.75 0 0 1 3 4.75ZM3.75 10h.5a.75.75 0 0 1 0 1.5h-.5a.75.75 0 0 1 0-1.5Z">
                     </path>
                    </svg>
                   </template>
                   <template id="globe-icon">
                    <svg aria-hidden="true" class="octicon octicon-globe" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                     <path d="M8 0a8 8 0 1 1 0 16A8 8 0 0 1 8 0ZM5.78 8.75a9.64 9.64 0 0 0 1.363 4.177c.255.426.542.832.857 1.215.245-.296.551-.705.857-1.215A9.64 9.64 0 0 0 10.22 8.75Zm4.44-1.5a9.64 9.64 0 0 0-1.363-4.177c-.307-.51-.612-.919-.857-1.215a9.927 9.927 0 0 0-.857 1.215A9.64 9.64 0 0 0 5.78 7.25Zm-5.944 1.5H1.543a6.507 6.507 0 0 0 4.666 5.5c-.123-.181-.24-.365-.352-.552-.715-1.192-1.437-2.874-1.581-4.948Zm-2.733-1.5h2.733c.144-2.074.866-3.756 1.58-4.948.12-.197.237-.381.353-.552a6.507 6.507 0 0 0-4.666 5.5Zm10.181 1.5c-.144 2.074-.866 3.756-1.58 4.948-.12.197-.237.381-.353.552a6.507 6.507 0 0 0 4.666-5.5Zm2.733-1.5a6.507 6.507 0 0 0-4.666-5.5c.123.181.24.365.353.552.714 1.192 1.436 2.874 1.58 4.948Z">
                     </path>
                    </svg>
                   </template>
                   <template id="issue-opened-icon">
                    <svg aria-hidden="true" class="octicon octicon-issue-opened" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                     <path d="M8 9.5a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Z">
                     </path>
                     <path d="M8 0a8 8 0 1 1 0 16A8 8 0 0 1 8 0ZM1.5 8a6.5 6.5 0 1 0 13 0 6.5 6.5 0 0 0-13 0Z">
                     </path>
                    </svg>
                   </template>
                   <template id="device-mobile-icon">
                    <svg aria-hidden="true" class="octicon octicon-device-mobile" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                     <path d="M3.75 0h8.5C13.216 0 14 .784 14 1.75v12.5A1.75 1.75 0 0 1 12.25 16h-8.5A1.75 1.75 0 0 1 2 14.25V1.75C2 .784 2.784 0 3.75 0ZM3.5 1.75v12.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25V1.75a.25.25 0 0 0-.25-.25h-8.5a.25.25 0 0 0-.25.25ZM8 13a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z">
                     </path>
                    </svg>
                   </template>
                   <template id="package-icon">
                    <svg aria-hidden="true" class="octicon octicon-package" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                     <path d="m8.878.392 5.25 3.045c.54.314.872.89.872 1.514v6.098a1.75 1.75 0 0 1-.872 1.514l-5.25 3.045a1.75 1.75 0 0 1-1.756 0l-5.25-3.045A1.75 1.75 0 0 1 1 11.049V4.951c0-.624.332-1.201.872-1.514L7.122.392a1.75 1.75 0 0 1 1.756 0ZM7.875 1.69l-4.63 2.685L8 7.133l4.755-2.758-4.63-2.685a.248.248 0 0 0-.25 0ZM2.5 5.677v5.372c0 .09.047.171.125.216l4.625 2.683V8.432Zm6.25 8.271 4.625-2.683a.25.25 0 0 0 .125-.216V5.677L8.75 8.432Z">
                     </path>
                    </svg>
                   </template>
                   <template id="credit-card-icon">
                    <svg aria-hidden="true" class="octicon octicon-credit-card" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                     <path d="M10.75 9a.75.75 0 0 0 0 1.5h1.5a.75.75 0 0 0 0-1.5h-1.5Z">
                     </path>
                     <path d="M0 3.75C0 2.784.784 2 1.75 2h12.5c.966 0 1.75.784 1.75 1.75v8.5A1.75 1.75 0 0 1 14.25 14H1.75A1.75 1.75 0 0 1 0 12.25ZM14.5 6.5h-13v5.75c0 .138.112.25.25.25h12.5a.25.25 0 0 0 .25-.25Zm0-2.75a.25.25 0 0 0-.25-.25H1.75a.25.25 0 0 0-.25.25V5h13Z">
                     </path>
                    </svg>
                   </template>
                   <template id="play-icon">
                    <svg aria-hidden="true" class="octicon octicon-play" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                     <path d="M8 0a8 8 0 1 1 0 16A8 8 0 0 1 8 0ZM1.5 8a6.5 6.5 0 1 0 13 0 6.5 6.5 0 0 0-13 0Zm4.879-2.773 4.264 2.559a.25.25 0 0 1 0 .428l-4.264 2.559A.25.25 0 0 1 6 10.559V5.442a.25.25 0 0 1 .379-.215Z">
                     </path>
                    </svg>
                   </template>
                   <template id="gift-icon">
                    <svg aria-hidden="true" class="octicon octicon-gift" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                     <path d="M2 2.75A2.75 2.75 0 0 1 4.75 0c.983 0 1.873.42 2.57 1.232.268.318.497.668.68 1.042.183-.375.411-.725.68-1.044C9.376.42 10.266 0 11.25 0a2.75 2.75 0 0 1 2.45 4h.55c.966 0 1.75.784 1.75 1.75v2c0 .698-.409 1.301-1 1.582v4.918A1.75 1.75 0 0 1 13.25 16H2.75A1.75 1.75 0 0 1 1 14.25V9.332C.409 9.05 0 8.448 0 7.75v-2C0 4.784.784 4 1.75 4h.55c-.192-.375-.3-.8-.3-1.25ZM7.25 9.5H2.5v4.75c0 .138.112.25.25.25h4.5Zm1.5 0v5h4.5a.25.25 0 0 0 .25-.25V9.5Zm0-4V8h5.5a.25.25 0 0 0 .25-.25v-2a.25.25 0 0 0-.25-.25Zm-7 0a.25.25 0 0 0-.25.25v2c0 .138.112.25.25.25h5.5V5.5h-5.5Zm3-4a1.25 1.25 0 0 0 0 2.5h2.309c-.233-.818-.542-1.401-.878-1.793-.43-.502-.915-.707-1.431-.707ZM8.941 4h2.309a1.25 1.25 0 0 0 0-2.5c-.516 0-1 .205-1.43.707-.337.392-.646.975-.879 1.793Z">
                     </path>
                    </svg>
                   </template>
                   <template id="code-square-icon">
                    <svg aria-hidden="true" class="octicon octicon-code-square" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                     <path d="M0 1.75C0 .784.784 0 1.75 0h12.5C15.216 0 16 .784 16 1.75v12.5A1.75 1.75 0 0 1 14.25 16H1.75A1.75 1.75 0 0 1 0 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h12.5a.25.25 0 0 0 .25-.25V1.75a.25.25 0 0 0-.25-.25Zm7.47 3.97a.75.75 0 0 1 1.06 0l2 2a.75.75 0 0 1 0 1.06l-2 2a.749.749 0 0 1-1.275-.326.749.749 0 0 1 .215-.734L10.69 8 9.22 6.53a.75.75 0 0 1 0-1.06ZM6.78 6.53 5.31 8l1.47 1.47a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215l-2-2a.75.75 0 0 1 0-1.06l2-2a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042Z">
                     </path>
                    </svg>
                   </template>
                   <template id="device-desktop-icon">
                    <svg aria-hidden="true" class="octicon octicon-device-desktop" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                     <path d="M14.25 1c.966 0 1.75.784 1.75 1.75v7.5A1.75 1.75 0 0 1 14.25 12h-3.727c.099 1.041.52 1.872 1.292 2.757A.752.752 0 0 1 11.25 16h-6.5a.75.75 0 0 1-.565-1.243c.772-.885 1.192-1.716 1.292-2.757H1.75A1.75 1.75 0 0 1 0 10.25v-7.5C0 1.784.784 1 1.75 1ZM1.75 2.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h12.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25ZM9.018 12H6.982a5.72 5.72 0 0 1-.765 2.5h3.566a5.72 5.72 0 0 1-.765-2.5Z">
                     </path>
                    </svg>
                   </template>
                   <div class="position-relative">
                    <ul aria-label="Suggestions" class="ActionListWrap QueryBuilder-ListWrap" data-action="
                    combobox-commit:query-builder#comboboxCommit
                    mousedown:query-builder#resultsMousedown
                  " data-persist-list="false" data-target="query-builder.resultsList" id="query-builder-test-results" role="listbox">
                    </ul>
                   </div>
                   <div class="FormControl-inlineValidation" hidden="hidden" id="validation-4d0e00a9-7685-4bd0-9019-3f399c5a9b84">
                    <span class="FormControl-inlineValidation--visual">
                     <svg aria-hidden="true" class="octicon octicon-alert-fill" data-view-component="true" height="12" version="1.1" viewbox="0 0 12 12" width="12">
                      <path d="M4.855.708c.5-.896 1.79-.896 2.29 0l4.675 8.351a1.312 1.312 0 0 1-1.146 1.954H1.33A1.313 1.313 0 0 1 .183 9.058ZM7 7V3H5v4Zm-1 3a1 1 0 1 0 0-2 1 1 0 0 0 0 2Z">
                      </path>
                     </svg>
                    </span>
                    <span>
                    </span>
                   </div>
                  </div>
                  <div aria-atomic="true" aria-live="polite" class="sr-only" data-target="query-builder.screenReaderFeedback">
                  </div>
                 </query-builder>
                </form>
                <div class="d-flex flex-row color-fg-muted px-3 text-small color-bg-default search-feedback-prompt">
                 <a class="Link color-fg-accent text-normal ml-2" data-view-component="true" href="https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax" target="_blank">
                  Search syntax tips
                 </a>
                 <div class="d-flex flex-1">
                 </div>
                </div>
               </div>
              </div>
             </div>
            </modal-dialog>
           </div>
          </div>
          <div class="dark-backdrop position-fixed" data-action="click:qbsearch-input#retract" data-target="qbsearch-input.darkBackdrop" hidden="">
          </div>
          <div class="color-fg-default">
           <dialog-helper>
            <dialog aria-describedby="feedback-dialog-description" aria-labelledby="feedback-dialog-title" aria-modal="true" class="Overlay Overlay-whenNarrow Overlay--size-medium Overlay--motion-scaleFade" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" data-target="qbsearch-input.feedbackDialog" data-view-component="true" id="feedback-dialog">
             <div class="Overlay-header" data-view-component="true">
              <div class="Overlay-headerContentWrap">
               <div class="Overlay-titleWrap">
                <h1 class="Overlay-title" id="feedback-dialog-title">
                 Provide feedback
                </h1>
               </div>
               <div class="Overlay-actionWrap">
                <button aria-label="Close" class="close-button Overlay-closeButton" data-close-dialog-id="feedback-dialog" data-view-component="true" type="button">
                 <svg aria-hidden="true" class="octicon octicon-x" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                  <path d="M3.72 3.72a.75.75 0 0 1 1.06 0L8 6.94l3.22-3.22a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l3.22 3.22a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-3.22 3.22a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L6.94 8 3.72 4.78a.75.75 0 0 1 0-1.06Z">
                  </path>
                 </svg>
                </button>
               </div>
              </div>
             </div>
             <scrollable-region data-labelled-by="feedback-dialog-title">
              <div class="Overlay-body" data-view-component="true">
               <!-- '"` -->
               <!-- </textarea></xmp> -->
               <form accept-charset="UTF-8" action="/search/feedback" data-turbo="false" id="code-search-feedback-form" method="post">
                <input data-csrf="true" name="authenticity_token" type="hidden" value="+eKDyw/10KzK30FQV2Rtjj72t860RZnkgSHx+MshDqRK7Vg03akpkXg+87aSfvR0Ls+/r50mwT8ELcrkPS6cwA=="/>
                <p>
                 We read every piece of feedback, and take your input very seriously.
                </p>
                <textarea class="form-control width-full mb-2" id="feedback" name="feedback" style="height: 120px"></textarea>
                <input aria-label="Include my email address so I can be contacted" class="form-control mr-2" id="include_email" name="include_email" type="checkbox"/>
                <label for="include_email" style="font-weight: normal">
                 Include my email address so I can be contacted
                </label>
               </form>
              </div>
             </scrollable-region>
             <div class="Overlay-footer Overlay-footer--alignEnd" data-view-component="true">
              <button class="btn" data-close-dialog-id="feedback-dialog" data-view-component="true" type="button">
               Cancel
              </button>
              <button class="btn-primary btn" data-action="click:qbsearch-input#submitFeedback" data-view-component="true" form="code-search-feedback-form" type="submit">
               Submit feedback
              </button>
             </div>
            </dialog>
           </dialog-helper>
           <custom-scopes data-target="qbsearch-input.customScopesManager">
            <dialog-helper>
             <dialog aria-describedby="custom-scopes-dialog-description" aria-labelledby="custom-scopes-dialog-title" aria-modal="true" class="Overlay Overlay-whenNarrow Overlay--size-medium Overlay--motion-scaleFade" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" data-target="custom-scopes.customScopesModalDialog" data-view-component="true" id="custom-scopes-dialog">
              <div class="Overlay-header Overlay-header--divided" data-view-component="true">
               <div class="Overlay-headerContentWrap">
                <div class="Overlay-titleWrap">
                 <h1 class="Overlay-title" id="custom-scopes-dialog-title">
                  Saved searches
                 </h1>
                 <h2 class="Overlay-description" id="custom-scopes-dialog-description">
                  Use saved searches to filter your results more quickly
                 </h2>
                </div>
                <div class="Overlay-actionWrap">
                 <button aria-label="Close" class="close-button Overlay-closeButton" data-close-dialog-id="custom-scopes-dialog" data-view-component="true" type="button">
                  <svg aria-hidden="true" class="octicon octicon-x" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                   <path d="M3.72 3.72a.75.75 0 0 1 1.06 0L8 6.94l3.22-3.22a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l3.22 3.22a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-3.22 3.22a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L6.94 8 3.72 4.78a.75.75 0 0 1 0-1.06Z">
                   </path>
                  </svg>
                 </button>
                </div>
               </div>
              </div>
              <scrollable-region data-labelled-by="custom-scopes-dialog-title">
               <div class="Overlay-body" data-view-component="true">
                <div data-target="custom-scopes.customScopesModalDialogFlash">
                </div>
                <div class="create-custom-scope-form" data-target="custom-scopes.createCustomScopeForm" hidden="">
                 <!-- '"` -->
                 <!-- </textarea></xmp> -->
                 <form accept-charset="UTF-8" action="/search/custom_scopes" data-turbo="false" id="custom-scopes-dialog-form" method="post">
                  <input data-csrf="true" name="authenticity_token" type="hidden" value="y81/qjpuIxTWibgus3im6/CwycVSkrw4DNgsy1dSh8+cZbY6ZLIFQH+LD+hq431NF5MLEBrGPLwqD2mVqPpRtQ==">
                   <div data-target="custom-scopes.customScopesModalDialogFlash">
                   </div>
                   <input data-target="custom-scopes.customScopesIdField" id="custom_scope_id" name="custom_scope_id" type="hidden"/>
                   <div class="form-group">
                    <label for="custom_scope_name">
                     Name
                    </label>
                    <auto-check required="" src="/search/custom_scopes/check_name">
                     <input autocomplete="off" class="form-control" data-target="custom-scopes.customScopesNameField" id="custom_scope_name" maxlength="50" name="custom_scope_name" placeholder="github-ruby" required="" type="text"/>
                     <input data-csrf="true" type="hidden" value="JToYaC9kQ6llYTwxFTtIlIv6T2VoAk0f7nv/lVcRyQ53MKYpPOut8yYw9Yk5Oc1oFPrpLOzDEyD/nLrwUIsuPw==">
                     </input>
                    </auto-check>
                   </div>
                   <div class="form-group">
                    <label for="custom_scope_query">
                     Query
                    </label>
                    <input autocomplete="off" class="form-control" data-target="custom-scopes.customScopesQueryField" id="custom_scope_query" maxlength="500" name="custom_scope_query" placeholder="(repo:mona/a OR repo:mona/b) AND lang:python" required="" type="text"/>
                   </div>
                   <p class="text-small color-fg-muted">
                    To see all available qualifiers, see our
                    <a class="Link--inTextBlock" href="https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax">
                     documentation
                    </a>
                    .
                   </p>
                  </input>
                 </form>
                </div>
                <div data-target="custom-scopes.manageCustomScopesForm">
                 <div data-target="custom-scopes.list">
                 </div>
                </div>
               </div>
              </scrollable-region>
              <div class="Overlay-footer Overlay-footer--alignEnd Overlay-footer--divided" data-view-component="true">
               <button class="btn" data-action="click:custom-scopes#customScopesCancel" data-view-component="true" type="button">
                Cancel
               </button>
               <button class="btn-primary btn" data-action="click:custom-scopes#customScopesSubmit" data-target="custom-scopes.customScopesSubmitButton" data-view-component="true" form="custom-scopes-dialog-form" type="submit">
                Create saved search
               </button>
              </div>
             </dialog>
            </dialog-helper>
           </custom-scopes>
          </div>
         </qbsearch-input>
         <input class="js-data-jump-to-suggestions-path-csrf" data-csrf="true" type="hidden" value="E0ING73jJtY980+jQMWJsuBh0FZhWG2N7RoDWd/5ADZYUgM+DWscCn7VNQiE/yfIueKMF1YkiXILaxS3Ks4PkA==">
          <div class="position-relative mr-lg-3 d-lg-inline-block">
           <a class="HeaderMenu-link HeaderMenu-link--sign-in flex-shrink-0 no-underline d-block d-lg-inline-block border border-lg-0 rounded rounded-lg-0 p-2 p-lg-0" data-ga-click="(Logged out) Header, clicked Sign in, text:sign-in" data-hydro-click='{"event_type":"authentication.click","payload":{"location_in_page":"site header menu","repository_id":null,"auth_type":"SIGN_UP","originating_url":"https://github.com/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset","user_id":null}}' data-hydro-click-hmac="211adfa7ff6e6f8d70662dfdacb459dd515e9e8a6c8de9f97d4615c38cd51ff3" href="/login?return_to=https%3A%2F%2Fgithub.com%2FAIMonk-Labs-Private-Limited%2Fawesome-segmentation-saliency-dataset">
            Sign in
           </a>
          </div>
          <a class="HeaderMenu-link HeaderMenu-link--sign-up flex-shrink-0 d-none d-lg-inline-block no-underline border color-border-default rounded px-2 py-1" data-analytics-event='{"category":"Sign up","action":"click to sign up for account","label":"ref_page:/&lt;user-name&gt;/&lt;repo-name&gt;;ref_cta:Sign up;ref_loc:header logged out"}' data-hydro-click='{"event_type":"authentication.click","payload":{"location_in_page":"site header menu","repository_id":null,"auth_type":"SIGN_UP","originating_url":"https://github.com/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset","user_id":null}}' data-hydro-click-hmac="211adfa7ff6e6f8d70662dfdacb459dd515e9e8a6c8de9f97d4615c38cd51ff3" href="/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=AIMonk-Labs-Private-Limited%2Fawesome-segmentation-saliency-dataset">
           Sign up
          </a>
         </input>
        </div>
       </div>
      </div>
     </div>
    </header>
    <div class="js-stale-session-flash stale-session-flash flash flash-warn flash-full mb-3" data-view-component="true" hidden="hidden">
     <svg aria-hidden="true" class="octicon octicon-alert" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
      <path d="M6.457 1.047c.659-1.234 2.427-1.234 3.086 0l6.082 11.378A1.75 1.75 0 0 1 14.082 15H1.918a1.75 1.75 0 0 1-1.543-2.575Zm1.763.707a.25.25 0 0 0-.44 0L1.698 13.132a.25.25 0 0 0 .22.368h12.164a.25.25 0 0 0 .22-.368Zm.53 3.996v2.5a.75.75 0 0 1-1.5 0v-2.5a.75.75 0 0 1 1.5 0ZM9 11a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z">
      </path>
     </svg>
     <span class="js-stale-session-flash-signed-in" hidden="">
      You signed in with another tab or window.
      <a class="Link--inTextBlock" href="">
       Reload
      </a>
      to refresh your session.
     </span>
     <span class="js-stale-session-flash-signed-out" hidden="">
      You signed out in another tab or window.
      <a class="Link--inTextBlock" href="">
       Reload
      </a>
      to refresh your session.
     </span>
     <span class="js-stale-session-flash-switched" hidden="">
      You switched accounts on another tab or window.
      <a class="Link--inTextBlock" href="">
       Reload
      </a>
      to refresh your session.
     </span>
     <button aria-labelledby="tooltip-8c75d315-4d9a-4bbe-a757-345dc356d5dc" class="Button Button--iconOnly Button--invisible Button--medium flash-close js-flash-close" data-view-component="true" id="icon-button-82f25f05-b91d-4372-8e5c-eb9eb2e77bdd" type="button">
      <svg aria-hidden="true" class="octicon octicon-x Button-visual" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
       <path d="M3.72 3.72a.75.75 0 0 1 1.06 0L8 6.94l3.22-3.22a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l3.22 3.22a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-3.22 3.22a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L6.94 8 3.72 4.78a.75.75 0 0 1 0-1.06Z">
       </path>
      </svg>
     </button>
     <tool-tip class="sr-only position-absolute" data-direction="s" data-type="label" data-view-component="true" for="icon-button-82f25f05-b91d-4372-8e5c-eb9eb2e77bdd" id="tooltip-8c75d315-4d9a-4bbe-a757-345dc356d5dc" popover="manual">
      Dismiss alert
     </tool-tip>
    </div>
   </div>
   <div class="show-on-focus" id="start-of-content">
   </div>
   <div class="flash-container" data-turbo-replace="" id="js-flash-container">
    <template class="js-flash-template">
     <div class="flash flash-full {{ className }}">
      <div>
       <button aria-label="Dismiss this message" autofocus="" class="flash-close js-flash-close" type="button">
        <svg aria-hidden="true" class="octicon octicon-x" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
         <path d="M3.72 3.72a.75.75 0 0 1 1.06 0L8 6.94l3.22-3.22a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l3.22 3.22a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-3.22 3.22a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L6.94 8 3.72 4.78a.75.75 0 0 1 0-1.06Z">
         </path>
        </svg>
       </button>
       <div aria-atomic="true" class="js-flash-alert" role="alert">
        <div>
         {{ message }}
        </div>
       </div>
      </div>
     </div>
    </template>
   </div>
   <include-fragment class="js-notification-shelf-include-fragment" data-base-src="https://github.com/notifications/beta/shelf">
   </include-fragment>
   <div class="application-main" data-commit-hovercards-enabled="" data-discussion-hovercards-enabled="" data-issue-and-pr-hovercards-enabled="">
    <div class="" itemscope="" itemtype="http://schema.org/SoftwareSourceCode">
     <main id="js-repo-pjax-container">
      <div class="pt-3 hide-full-screen" data-turbo-replace="" id="repository-container-header" style="background-color: var(--page-header-bgColor, var(--color-page-header-bg));">
       <div class="d-flex flex-wrap flex-justify-end mb-3 px-3 px-md-4 px-lg-5" style="gap: 1rem;">
        <div class="flex-auto min-width-0 width-fit mr-3">
         <div class="d-flex flex-wrap flex-items-center wb-break-word f3 text-normal">
          <svg aria-hidden="true" class="octicon octicon-repo-forked color-fg-muted mr-2" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
           <path d="M5 5.372v.878c0 .414.336.75.75.75h4.5a.75.75 0 0 0 .75-.75v-.878a2.25 2.25 0 1 1 1.5 0v.878a2.25 2.25 0 0 1-2.25 2.25h-1.5v2.128a2.251 2.251 0 1 1-1.5 0V8.5h-1.5A2.25 2.25 0 0 1 3.5 6.25v-.878a2.25 2.25 0 1 1 1.5 0ZM5 3.25a.75.75 0 1 0-1.5 0 .75.75 0 0 0 1.5 0Zm6.75.75a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5Zm-3 8.75a.75.75 0 1 0-1.5 0 .75.75 0 0 0 1.5 0Z">
           </path>
          </svg>
          <span class="author flex-self-stretch" itemprop="author">
           <a class="url fn" data-hovercard-type="organization" data-hovercard-url="/orgs/AIMonk-Labs-Private-Limited/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="/AIMonk-Labs-Private-Limited" rel="author">
            AIMonk-Labs-Private-Limited
           </a>
          </span>
          <span class="mx-1 flex-self-stretch color-fg-muted">
           /
          </span>
          <strong class="mr-2 flex-self-stretch" itemprop="name">
           <a data-pjax="#repo-content-pjax-container" data-turbo-frame="repo-content-turbo-frame" href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset">
            awesome-segmentation-saliency-dataset
           </a>
          </strong>
          <span>
          </span>
          <span class="Label Label--secondary v-align-middle mr-1">
           Public
          </span>
         </div>
         <span class="text-small lh-condensed-ultra no-wrap mt-1" data-repository-hovercards-enabled="">
          forked from
          <a class="Link--inTextBlock" data-hovercard-type="repository" data-hovercard-url="/FnSK4R17s/awesome-segmentation-saliency-dataset/hovercard" href="/FnSK4R17s/awesome-segmentation-saliency-dataset">
           FnSK4R17s/awesome-segmentation-saliency-dataset
          </a>
         </span>
        </div>
        <div data-turbo-replace="" id="repository-details-container">
         <ul class="pagehead-actions flex-shrink-0 d-none d-md-inline" style="padding: 2px 0;">
          <li>
           <a aria-label="You must be signed in to change notification settings" class="tooltipped tooltipped-s btn-sm btn" data-hydro-click='{"event_type":"authentication.click","payload":{"location_in_page":"notification subscription menu watch","repository_id":null,"auth_type":"LOG_IN","originating_url":"https://github.com/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset","user_id":null}}' data-hydro-click-hmac="865abacf6aa438d71ee80cb4fa4b4575124e3d7468e385265dff8994f1c120c6" data-view-component="true" href="/login?return_to=%2FAIMonk-Labs-Private-Limited%2Fawesome-segmentation-saliency-dataset" rel="nofollow">
            <svg aria-hidden="true" class="octicon octicon-bell mr-2" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
             <path d="M8 16a2 2 0 0 0 1.985-1.75c.017-.137-.097-.25-.235-.25h-3.5c-.138 0-.252.113-.235.25A2 2 0 0 0 8 16ZM3 5a5 5 0 0 1 10 0v2.947c0 .05.015.098.042.139l1.703 2.555A1.519 1.519 0 0 1 13.482 13H2.518a1.516 1.516 0 0 1-1.263-2.36l1.703-2.554A.255.255 0 0 0 3 7.947Zm5-3.5A3.5 3.5 0 0 0 4.5 5v2.947c0 .346-.102.683-.294.97l-1.703 2.556a.017.017 0 0 0-.003.01l.001.006c0 .002.002.004.004.006l.006.004.007.001h10.964l.007-.001.006-.004.004-.006.001-.007a.017.017 0 0 0-.003-.01l-1.703-2.554a1.745 1.745 0 0 1-.294-.97V5A3.5 3.5 0 0 0 8 1.5Z">
             </path>
            </svg>
            Notifications
           </a>
          </li>
          <li>
           <a class="btn-sm btn" data-hydro-click='{"event_type":"authentication.click","payload":{"location_in_page":"repo details fork button","repository_id":306616897,"auth_type":"LOG_IN","originating_url":"https://github.com/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset","user_id":null}}' data-hydro-click-hmac="782b6abc13f413ae4e1869af8ff7f922793e598448ed61fa4c8288ba97a501b4" data-view-component="true" href="/login?return_to=%2FAIMonk-Labs-Private-Limited%2Fawesome-segmentation-saliency-dataset" icon="repo-forked" id="fork-button" rel="nofollow">
            <svg aria-hidden="true" class="octicon octicon-repo-forked mr-2" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
             <path d="M5 5.372v.878c0 .414.336.75.75.75h4.5a.75.75 0 0 0 .75-.75v-.878a2.25 2.25 0 1 1 1.5 0v.878a2.25 2.25 0 0 1-2.25 2.25h-1.5v2.128a2.251 2.251 0 1 1-1.5 0V8.5h-1.5A2.25 2.25 0 0 1 3.5 6.25v-.878a2.25 2.25 0 1 1 1.5 0ZM5 3.25a.75.75 0 1 0-1.5 0 .75.75 0 0 0 1.5 0Zm6.75.75a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5Zm-3 8.75a.75.75 0 1 0-1.5 0 .75.75 0 0 0 1.5 0Z">
             </path>
            </svg>
            Fork
            <span class="Counter" data-pjax-replace="true" data-turbo-replace="true" data-view-component="true" id="repo-network-counter" title="0">
             0
            </span>
           </a>
          </li>
          <li>
           <div class="BtnGroup d-flex" data-view-component="true">
            <a aria-label="You must be signed in to star a repository" class="tooltipped tooltipped-s btn-sm btn BtnGroup-item" data-hydro-click='{"event_type":"authentication.click","payload":{"location_in_page":"star button","repository_id":306616897,"auth_type":"LOG_IN","originating_url":"https://github.com/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset","user_id":null}}' data-hydro-click-hmac="89d2396c1cb9091371dbf3b4e5ab6a0b0d5dafa5321bd5cff62ea7ed0c113f1a" data-view-component="true" href="/login?return_to=%2FAIMonk-Labs-Private-Limited%2Fawesome-segmentation-saliency-dataset" rel="nofollow">
             <svg aria-hidden="true" class="octicon octicon-star v-align-text-bottom d-inline-block mr-2" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
              <path d="M8 .25a.75.75 0 0 1 .673.418l1.882 3.815 4.21.612a.75.75 0 0 1 .416 1.279l-3.046 2.97.719 4.192a.751.751 0 0 1-1.088.791L8 12.347l-3.766 1.98a.75.75 0 0 1-1.088-.79l.72-4.194L.818 6.374a.75.75 0 0 1 .416-1.28l4.21-.611L7.327.668A.75.75 0 0 1 8 .25Zm0 2.445L6.615 5.5a.75.75 0 0 1-.564.41l-3.097.45 2.24 2.184a.75.75 0 0 1 .216.664l-.528 3.084 2.769-1.456a.75.75 0 0 1 .698 0l2.77 1.456-.53-3.084a.75.75 0 0 1 .216-.664l2.24-2.183-3.096-.45a.75.75 0 0 1-.564-.41L8 2.694Z">
              </path>
             </svg>
             <span class="d-inline" data-view-component="true">
              Star
             </span>
             <span aria-label="0 users starred this repository" class="Counter js-social-count" data-plural-suffix="users starred this repository" data-singular-suffix="user starred this repository" data-turbo-replace="true" data-view-component="true" id="repo-stars-counter-star" title="0">
              0
             </span>
            </a>
            <button aria-label="You must be signed in to add this repository to a list" class="btn-sm btn BtnGroup-item px-2" data-view-component="true" disabled="disabled" type="button">
             <svg aria-hidden="true" class="octicon octicon-triangle-down" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
              <path d="m4.427 7.427 3.396 3.396a.25.25 0 0 0 .354 0l3.396-3.396A.25.25 0 0 0 11.396 7H4.604a.25.25 0 0 0-.177.427Z">
              </path>
             </svg>
            </button>
           </div>
          </li>
          <li>
          </li>
         </ul>
        </div>
       </div>
       <div data-turbo-replace="" id="responsive-meta-container">
        <div class="d-block d-md-none mb-2 px-3 px-md-4 px-lg-5">
         <p class="f4 mb-3">
          A collection of some datasets for segmentation / saliency detection. Welcome to PR...:smile:
         </p>
         <div class="mb-2 d-flex flex-items-center Link--secondary">
          <svg aria-hidden="true" class="octicon octicon-link flex-shrink-0 mr-2" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
           <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
           </path>
          </svg>
          <span class="flex-auto min-width-0 css-truncate css-truncate-target width-fit">
           <a class="text-bold" href="https://lartpang.github.io/awesome-segmentation-saliency-dataset/README.html" rel="noopener noreferrer" role="link" target="_blank" title="https://lartpang.github.io/awesome-segmentation-saliency-dataset/README.html">
            lartpang.github.io/awesome-segmentation-saliency-dataset/readme.html
           </a>
          </span>
         </div>
         <div class="mb-3">
          <a class="Link--secondary no-underline mr-3" href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/stargazers">
           <svg aria-hidden="true" class="octicon octicon-star mr-1" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
            <path d="M8 .25a.75.75 0 0 1 .673.418l1.882 3.815 4.21.612a.75.75 0 0 1 .416 1.279l-3.046 2.97.719 4.192a.751.751 0 0 1-1.088.791L8 12.347l-3.766 1.98a.75.75 0 0 1-1.088-.79l.72-4.194L.818 6.374a.75.75 0 0 1 .416-1.28l4.21-.611L7.327.668A.75.75 0 0 1 8 .25Zm0 2.445L6.615 5.5a.75.75 0 0 1-.564.41l-3.097.45 2.24 2.184a.75.75 0 0 1 .216.664l-.528 3.084 2.769-1.456a.75.75 0 0 1 .698 0l2.77 1.456-.53-3.084a.75.75 0 0 1 .216-.664l2.24-2.183-3.096-.45a.75.75 0 0 1-.564-.41L8 2.694Z">
            </path>
           </svg>
           <span class="text-bold">
            0
           </span>
           stars
          </a>
          <a class="Link--secondary no-underline mr-3" href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/forks">
           <svg aria-hidden="true" class="octicon octicon-repo-forked mr-1" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
            <path d="M5 5.372v.878c0 .414.336.75.75.75h4.5a.75.75 0 0 0 .75-.75v-.878a2.25 2.25 0 1 1 1.5 0v.878a2.25 2.25 0 0 1-2.25 2.25h-1.5v2.128a2.251 2.251 0 1 1-1.5 0V8.5h-1.5A2.25 2.25 0 0 1 3.5 6.25v-.878a2.25 2.25 0 1 1 1.5 0ZM5 3.25a.75.75 0 1 0-1.5 0 .75.75 0 0 0 1.5 0Zm6.75.75a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5Zm-3 8.75a.75.75 0 1 0-1.5 0 .75.75 0 0 0 1.5 0Z">
            </path>
           </svg>
           <span class="text-bold">
            95
           </span>
           forks
          </a>
          <a class="Link--secondary no-underline mr-3 d-inline-block" href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/branches">
           <svg aria-hidden="true" class="octicon octicon-git-branch mr-1" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
            <path d="M9.5 3.25a2.25 2.25 0 1 1 3 2.122V6A2.5 2.5 0 0 1 10 8.5H6a1 1 0 0 0-1 1v1.128a2.251 2.251 0 1 1-1.5 0V5.372a2.25 2.25 0 1 1 1.5 0v1.836A2.493 2.493 0 0 1 6 7h4a1 1 0 0 0 1-1v-.628A2.25 2.25 0 0 1 9.5 3.25Zm-6 0a.75.75 0 1 0 1.5 0 .75.75 0 0 0-1.5 0Zm8.25-.75a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5ZM4.25 12a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Z">
            </path>
           </svg>
           <span>
            Branches
           </span>
          </a>
          <a class="Link--secondary no-underline d-inline-block" href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/tags">
           <svg aria-hidden="true" class="octicon octicon-tag mr-1" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
            <path d="M1 7.775V2.75C1 1.784 1.784 1 2.75 1h5.025c.464 0 .91.184 1.238.513l6.25 6.25a1.75 1.75 0 0 1 0 2.474l-5.026 5.026a1.75 1.75 0 0 1-2.474 0l-6.25-6.25A1.752 1.752 0 0 1 1 7.775Zm1.5 0c0 .066.026.13.073.177l6.25 6.25a.25.25 0 0 0 .354 0l5.025-5.025a.25.25 0 0 0 0-.354l-6.25-6.25a.25.25 0 0 0-.177-.073H2.75a.25.25 0 0 0-.25.25ZM6 5a1 1 0 1 1 0 2 1 1 0 0 1 0-2Z">
            </path>
           </svg>
           <span>
            Tags
           </span>
          </a>
          <a class="Link--secondary no-underline d-inline-block" href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/activity">
           <svg aria-hidden="true" class="octicon octicon-pulse mr-1" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
            <path d="M6 2c.306 0 .582.187.696.471L10 10.731l1.304-3.26A.751.751 0 0 1 12 7h3.25a.75.75 0 0 1 0 1.5h-2.742l-1.812 4.528a.751.751 0 0 1-1.392 0L6 4.77 4.696 8.03A.75.75 0 0 1 4 8.5H.75a.75.75 0 0 1 0-1.5h2.742l1.812-4.529A.751.751 0 0 1 6 2Z">
            </path>
           </svg>
           <span>
            Activity
           </span>
          </a>
         </div>
         <div class="d-flex flex-wrap gap-2">
          <div class="flex-1">
           <div class="BtnGroup d-flex" data-view-component="true">
            <a aria-label="You must be signed in to star a repository" class="tooltipped tooltipped-s btn-sm btn btn-block BtnGroup-item" data-hydro-click='{"event_type":"authentication.click","payload":{"location_in_page":"star button","repository_id":306616897,"auth_type":"LOG_IN","originating_url":"https://github.com/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset","user_id":null}}' data-hydro-click-hmac="89d2396c1cb9091371dbf3b4e5ab6a0b0d5dafa5321bd5cff62ea7ed0c113f1a" data-view-component="true" href="/login?return_to=%2FAIMonk-Labs-Private-Limited%2Fawesome-segmentation-saliency-dataset" rel="nofollow">
             <svg aria-hidden="true" class="octicon octicon-star v-align-text-bottom d-inline-block mr-2" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
              <path d="M8 .25a.75.75 0 0 1 .673.418l1.882 3.815 4.21.612a.75.75 0 0 1 .416 1.279l-3.046 2.97.719 4.192a.751.751 0 0 1-1.088.791L8 12.347l-3.766 1.98a.75.75 0 0 1-1.088-.79l.72-4.194L.818 6.374a.75.75 0 0 1 .416-1.28l4.21-.611L7.327.668A.75.75 0 0 1 8 .25Zm0 2.445L6.615 5.5a.75.75 0 0 1-.564.41l-3.097.45 2.24 2.184a.75.75 0 0 1 .216.664l-.528 3.084 2.769-1.456a.75.75 0 0 1 .698 0l2.77 1.456-.53-3.084a.75.75 0 0 1 .216-.664l2.24-2.183-3.096-.45a.75.75 0 0 1-.564-.41L8 2.694Z">
              </path>
             </svg>
             <span class="d-inline" data-view-component="true">
              Star
             </span>
            </a>
            <button aria-label="You must be signed in to add this repository to a list" class="btn-sm btn BtnGroup-item px-2" data-view-component="true" disabled="disabled" type="button">
             <svg aria-hidden="true" class="octicon octicon-triangle-down" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
              <path d="m4.427 7.427 3.396 3.396a.25.25 0 0 0 .354 0l3.396-3.396A.25.25 0 0 0 11.396 7H4.604a.25.25 0 0 0-.177.427Z">
              </path>
             </svg>
            </button>
           </div>
          </div>
          <div class="flex-1">
           <a aria-label="You must be signed in to change notification settings" class="tooltipped tooltipped-s btn-sm btn btn-block" data-hydro-click='{"event_type":"authentication.click","payload":{"location_in_page":"notification subscription menu watch","repository_id":null,"auth_type":"LOG_IN","originating_url":"https://github.com/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset","user_id":null}}' data-hydro-click-hmac="865abacf6aa438d71ee80cb4fa4b4575124e3d7468e385265dff8994f1c120c6" data-view-component="true" href="/login?return_to=%2FAIMonk-Labs-Private-Limited%2Fawesome-segmentation-saliency-dataset" rel="nofollow">
            <svg aria-hidden="true" class="octicon octicon-bell mr-2" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
             <path d="M8 16a2 2 0 0 0 1.985-1.75c.017-.137-.097-.25-.235-.25h-3.5c-.138 0-.252.113-.235.25A2 2 0 0 0 8 16ZM3 5a5 5 0 0 1 10 0v2.947c0 .05.015.098.042.139l1.703 2.555A1.519 1.519 0 0 1 13.482 13H2.518a1.516 1.516 0 0 1-1.263-2.36l1.703-2.554A.255.255 0 0 0 3 7.947Zm5-3.5A3.5 3.5 0 0 0 4.5 5v2.947c0 .346-.102.683-.294.97l-1.703 2.556a.017.017 0 0 0-.003.01l.001.006c0 .002.002.004.004.006l.006.004.007.001h10.964l.007-.001.006-.004.004-.006.001-.007a.017.017 0 0 0-.003-.01l-1.703-2.554a1.745 1.745 0 0 1-.294-.97V5A3.5 3.5 0 0 0 8 1.5Z">
             </path>
            </svg>
            Notifications
           </a>
          </div>
          <span>
          </span>
         </div>
        </div>
       </div>
       <nav aria-label="Repository" class="js-repo-nav js-sidenav-container-pjax js-responsive-underlinenav overflow-hidden UnderlineNav px-3 px-md-4 px-lg-5" data-pjax="#js-repo-pjax-container" data-view-component="true">
        <ul class="UnderlineNav-body list-style-none" data-view-component="true">
         <li class="d-inline-flex" data-view-component="true">
          <a aria-current="page" class="UnderlineNav-item no-wrap js-responsive-underlinenav-item js-selected-navigation-item selected" data-analytics-event='{"category":"Underline navbar","action":"Click tab","label":"Code","target":"UNDERLINE_NAV.TAB"}' data-hotkey="g c" data-pjax="#repo-content-pjax-container" data-selected-links="repo_source repo_downloads repo_commits repo_releases repo_tags repo_branches repo_packages repo_deployments repo_attestations /AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset" data-tab-item="i0code-tab" data-turbo-frame="repo-content-turbo-frame" data-view-component="true" href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset" id="code-tab">
           <svg aria-hidden="true" class="octicon octicon-code UnderlineNav-octicon d-none d-sm-inline" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
            <path d="m11.28 3.22 4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.275-.326.749.749 0 0 1 .215-.734L13.94 8l-3.72-3.72a.749.749 0 0 1 .326-1.275.749.749 0 0 1 .734.215Zm-6.56 0a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042L2.06 8l3.72 3.72a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L.47 8.53a.75.75 0 0 1 0-1.06Z">
            </path>
           </svg>
           <span data-content="Code">
            Code
           </span>
           <span class="Counter" data-pjax-replace="" data-turbo-replace="" data-view-component="true" id="code-repo-tab-count" title="Not available">
           </span>
          </a>
         </li>
         <li class="d-inline-flex" data-view-component="true">
          <a class="UnderlineNav-item no-wrap js-responsive-underlinenav-item js-selected-navigation-item" data-analytics-event='{"category":"Underline navbar","action":"Click tab","label":"Pull requests","target":"UNDERLINE_NAV.TAB"}' data-hotkey="g p" data-pjax="#repo-content-pjax-container" data-selected-links="repo_pulls checks /AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/pulls" data-tab-item="i1pull-requests-tab" data-turbo-frame="repo-content-turbo-frame" data-view-component="true" href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/pulls" id="pull-requests-tab">
           <svg aria-hidden="true" class="octicon octicon-git-pull-request UnderlineNav-octicon d-none d-sm-inline" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
            <path d="M1.5 3.25a2.25 2.25 0 1 1 3 2.122v5.256a2.251 2.251 0 1 1-1.5 0V5.372A2.25 2.25 0 0 1 1.5 3.25Zm5.677-.177L9.573.677A.25.25 0 0 1 10 .854V2.5h1A2.5 2.5 0 0 1 13.5 5v5.628a2.251 2.251 0 1 1-1.5 0V5a1 1 0 0 0-1-1h-1v1.646a.25.25 0 0 1-.427.177L7.177 3.427a.25.25 0 0 1 0-.354ZM3.75 2.5a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Zm0 9.5a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Zm8.25.75a.75.75 0 1 0 1.5 0 .75.75 0 0 0-1.5 0Z">
            </path>
           </svg>
           <span data-content="Pull requests">
            Pull requests
           </span>
           <span class="Counter" data-pjax-replace="" data-turbo-replace="" data-view-component="true" hidden="hidden" id="pull-requests-repo-tab-count" title="0">
            0
           </span>
          </a>
         </li>
         <li class="d-inline-flex" data-view-component="true">
          <a class="UnderlineNav-item no-wrap js-responsive-underlinenav-item js-selected-navigation-item" data-analytics-event='{"category":"Underline navbar","action":"Click tab","label":"Actions","target":"UNDERLINE_NAV.TAB"}' data-hotkey="g a" data-pjax="#repo-content-pjax-container" data-selected-links="repo_actions /AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/actions" data-tab-item="i2actions-tab" data-turbo-frame="repo-content-turbo-frame" data-view-component="true" href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/actions" id="actions-tab">
           <svg aria-hidden="true" class="octicon octicon-play UnderlineNav-octicon d-none d-sm-inline" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
            <path d="M8 0a8 8 0 1 1 0 16A8 8 0 0 1 8 0ZM1.5 8a6.5 6.5 0 1 0 13 0 6.5 6.5 0 0 0-13 0Zm4.879-2.773 4.264 2.559a.25.25 0 0 1 0 .428l-4.264 2.559A.25.25 0 0 1 6 10.559V5.442a.25.25 0 0 1 .379-.215Z">
            </path>
           </svg>
           <span data-content="Actions">
            Actions
           </span>
           <span class="Counter" data-pjax-replace="" data-turbo-replace="" data-view-component="true" id="actions-repo-tab-count" title="Not available">
           </span>
          </a>
         </li>
         <li class="d-inline-flex" data-view-component="true">
          <a class="UnderlineNav-item no-wrap js-responsive-underlinenav-item js-selected-navigation-item" data-analytics-event='{"category":"Underline navbar","action":"Click tab","label":"Projects","target":"UNDERLINE_NAV.TAB"}' data-hotkey="g b" data-pjax="#repo-content-pjax-container" data-selected-links="repo_projects new_repo_project repo_project /AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/projects" data-tab-item="i3projects-tab" data-turbo-frame="repo-content-turbo-frame" data-view-component="true" href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/projects" id="projects-tab">
           <svg aria-hidden="true" class="octicon octicon-table UnderlineNav-octicon d-none d-sm-inline" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
            <path d="M0 1.75C0 .784.784 0 1.75 0h12.5C15.216 0 16 .784 16 1.75v12.5A1.75 1.75 0 0 1 14.25 16H1.75A1.75 1.75 0 0 1 0 14.25ZM6.5 6.5v8h7.75a.25.25 0 0 0 .25-.25V6.5Zm8-1.5V1.75a.25.25 0 0 0-.25-.25H6.5V5Zm-13 1.5v7.75c0 .138.112.25.25.25H5v-8ZM5 5V1.5H1.75a.25.25 0 0 0-.25.25V5Z">
            </path>
           </svg>
           <span data-content="Projects">
            Projects
           </span>
           <span class="Counter" data-pjax-replace="" data-turbo-replace="" data-view-component="true" hidden="hidden" id="projects-repo-tab-count" title="0">
            0
           </span>
          </a>
         </li>
         <li class="d-inline-flex" data-view-component="true">
          <a class="UnderlineNav-item no-wrap js-responsive-underlinenav-item js-selected-navigation-item" data-analytics-event='{"category":"Underline navbar","action":"Click tab","label":"Security","target":"UNDERLINE_NAV.TAB"}' data-hotkey="g s" data-pjax="#repo-content-pjax-container" data-selected-links="security overview alerts policy token_scanning code_scanning /AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/security" data-tab-item="i4security-tab" data-turbo-frame="repo-content-turbo-frame" data-view-component="true" href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/security" id="security-tab">
           <svg aria-hidden="true" class="octicon octicon-shield UnderlineNav-octicon d-none d-sm-inline" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
            <path d="M7.467.133a1.748 1.748 0 0 1 1.066 0l5.25 1.68A1.75 1.75 0 0 1 15 3.48V7c0 1.566-.32 3.182-1.303 4.682-.983 1.498-2.585 2.813-5.032 3.855a1.697 1.697 0 0 1-1.33 0c-2.447-1.042-4.049-2.357-5.032-3.855C1.32 10.182 1 8.566 1 7V3.48a1.75 1.75 0 0 1 1.217-1.667Zm.61 1.429a.25.25 0 0 0-.153 0l-5.25 1.68a.25.25 0 0 0-.174.238V7c0 1.358.275 2.666 1.057 3.86.784 1.194 2.121 2.34 4.366 3.297a.196.196 0 0 0 .154 0c2.245-.956 3.582-2.104 4.366-3.298C13.225 9.666 13.5 8.36 13.5 7V3.48a.251.251 0 0 0-.174-.237l-5.25-1.68ZM8.75 4.75v3a.75.75 0 0 1-1.5 0v-3a.75.75 0 0 1 1.5 0ZM9 10.5a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z">
            </path>
           </svg>
           <span data-content="Security">
            Security
           </span>
           <include-fragment accept="text/fragment+html" src="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/security/overall-count">
           </include-fragment>
          </a>
         </li>
         <li class="d-inline-flex" data-view-component="true">
          <a class="UnderlineNav-item no-wrap js-responsive-underlinenav-item js-selected-navigation-item" data-analytics-event='{"category":"Underline navbar","action":"Click tab","label":"Insights","target":"UNDERLINE_NAV.TAB"}' data-pjax="#repo-content-pjax-container" data-selected-links="repo_graphs repo_contributors dependency_graph dependabot_updates pulse people community /AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/pulse" data-tab-item="i5insights-tab" data-turbo-frame="repo-content-turbo-frame" data-view-component="true" href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/pulse" id="insights-tab">
           <svg aria-hidden="true" class="octicon octicon-graph UnderlineNav-octicon d-none d-sm-inline" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
            <path d="M1.5 1.75V13.5h13.75a.75.75 0 0 1 0 1.5H.75a.75.75 0 0 1-.75-.75V1.75a.75.75 0 0 1 1.5 0Zm14.28 2.53-5.25 5.25a.75.75 0 0 1-1.06 0L7 7.06 4.28 9.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.25-3.25a.75.75 0 0 1 1.06 0L10 7.94l4.72-4.72a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042Z">
            </path>
           </svg>
           <span data-content="Insights">
            Insights
           </span>
           <span class="Counter" data-pjax-replace="" data-turbo-replace="" data-view-component="true" id="insights-repo-tab-count" title="Not available">
           </span>
          </a>
         </li>
        </ul>
        <div class="UnderlineNav-actions js-responsive-underlinenav-overflow position-absolute pr-3 pr-md-4 pr-lg-5 right-0" data-view-component="true" style="visibility:hidden;">
         <action-menu data-select-variant="none" data-view-component="true">
          <focus-group direction="vertical" mnemonics="" retain="">
           <button aria-controls="action-menu-7ab7e437-f220-4906-8a30-0e481687ab02-list" aria-haspopup="true" aria-labelledby="tooltip-dc871e95-84be-492f-b2bd-bb2ef8e27827" class="Button Button--iconOnly Button--secondary Button--medium UnderlineNav-item" data-view-component="true" id="action-menu-7ab7e437-f220-4906-8a30-0e481687ab02-button" popovertarget="action-menu-7ab7e437-f220-4906-8a30-0e481687ab02-overlay" type="button">
            <svg aria-hidden="true" class="octicon octicon-kebab-horizontal Button-visual" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
             <path d="M8 9a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3ZM1.5 9a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Zm13 0a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Z">
             </path>
            </svg>
           </button>
           <tool-tip class="sr-only position-absolute" data-direction="s" data-type="label" data-view-component="true" for="action-menu-7ab7e437-f220-4906-8a30-0e481687ab02-button" id="tooltip-dc871e95-84be-492f-b2bd-bb2ef8e27827" popover="manual">
            Additional navigation options
           </tool-tip>
           <anchored-position align="start" anchor="action-menu-7ab7e437-f220-4906-8a30-0e481687ab02-button" anchor-offset="normal" data-view-component="true" id="action-menu-7ab7e437-f220-4906-8a30-0e481687ab02-overlay" popover="auto" side="outside-bottom">
            <div class="Overlay Overlay--size-auto" data-view-component="true">
             <div class="Overlay-body Overlay-body--paddingNone" data-view-component="true">
              <action-list>
               <div data-view-component="true">
                <ul aria-labelledby="action-menu-7ab7e437-f220-4906-8a30-0e481687ab02-button" class="ActionListWrap--inset ActionListWrap" data-view-component="true" id="action-menu-7ab7e437-f220-4906-8a30-0e481687ab02-list" role="menu">
                 <li class="ActionListItem" data-menu-item="i0code-tab" data-targets="action-list.items" data-view-component="true" hidden="hidden" role="none">
                  <a class="ActionListContent ActionListContent--visual16" data-view-component="true" href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset" id="item-be78988f-e75b-46cd-a95f-8d956b3e228d" role="menuitem" tabindex="-1">
                   <span class="ActionListItem-visual ActionListItem-visual--leading">
                    <svg aria-hidden="true" class="octicon octicon-code" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                     <path d="m11.28 3.22 4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.275-.326.749.749 0 0 1 .215-.734L13.94 8l-3.72-3.72a.749.749 0 0 1 .326-1.275.749.749 0 0 1 .734.215Zm-6.56 0a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042L2.06 8l3.72 3.72a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L.47 8.53a.75.75 0 0 1 0-1.06Z">
                     </path>
                    </svg>
                   </span>
                   <span class="ActionListItem-label" data-view-component="true">
                    Code
                   </span>
                  </a>
                 </li>
                 <li class="ActionListItem" data-menu-item="i1pull-requests-tab" data-targets="action-list.items" data-view-component="true" hidden="hidden" role="none">
                  <a class="ActionListContent ActionListContent--visual16" data-view-component="true" href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/pulls" id="item-d3ca6aee-d70a-4ff3-b456-35e07020e3fe" role="menuitem" tabindex="-1">
                   <span class="ActionListItem-visual ActionListItem-visual--leading">
                    <svg aria-hidden="true" class="octicon octicon-git-pull-request" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                     <path d="M1.5 3.25a2.25 2.25 0 1 1 3 2.122v5.256a2.251 2.251 0 1 1-1.5 0V5.372A2.25 2.25 0 0 1 1.5 3.25Zm5.677-.177L9.573.677A.25.25 0 0 1 10 .854V2.5h1A2.5 2.5 0 0 1 13.5 5v5.628a2.251 2.251 0 1 1-1.5 0V5a1 1 0 0 0-1-1h-1v1.646a.25.25 0 0 1-.427.177L7.177 3.427a.25.25 0 0 1 0-.354ZM3.75 2.5a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Zm0 9.5a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Zm8.25.75a.75.75 0 1 0 1.5 0 .75.75 0 0 0-1.5 0Z">
                     </path>
                    </svg>
                   </span>
                   <span class="ActionListItem-label" data-view-component="true">
                    Pull requests
                   </span>
                  </a>
                 </li>
                 <li class="ActionListItem" data-menu-item="i2actions-tab" data-targets="action-list.items" data-view-component="true" hidden="hidden" role="none">
                  <a class="ActionListContent ActionListContent--visual16" data-view-component="true" href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/actions" id="item-39b578af-3bef-4a72-a27b-873467f000a1" role="menuitem" tabindex="-1">
                   <span class="ActionListItem-visual ActionListItem-visual--leading">
                    <svg aria-hidden="true" class="octicon octicon-play" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                     <path d="M8 0a8 8 0 1 1 0 16A8 8 0 0 1 8 0ZM1.5 8a6.5 6.5 0 1 0 13 0 6.5 6.5 0 0 0-13 0Zm4.879-2.773 4.264 2.559a.25.25 0 0 1 0 .428l-4.264 2.559A.25.25 0 0 1 6 10.559V5.442a.25.25 0 0 1 .379-.215Z">
                     </path>
                    </svg>
                   </span>
                   <span class="ActionListItem-label" data-view-component="true">
                    Actions
                   </span>
                  </a>
                 </li>
                 <li class="ActionListItem" data-menu-item="i3projects-tab" data-targets="action-list.items" data-view-component="true" hidden="hidden" role="none">
                  <a class="ActionListContent ActionListContent--visual16" data-view-component="true" href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/projects" id="item-e7b3fb26-ebb6-4c2e-bed3-52b525044d8c" role="menuitem" tabindex="-1">
                   <span class="ActionListItem-visual ActionListItem-visual--leading">
                    <svg aria-hidden="true" class="octicon octicon-table" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                     <path d="M0 1.75C0 .784.784 0 1.75 0h12.5C15.216 0 16 .784 16 1.75v12.5A1.75 1.75 0 0 1 14.25 16H1.75A1.75 1.75 0 0 1 0 14.25ZM6.5 6.5v8h7.75a.25.25 0 0 0 .25-.25V6.5Zm8-1.5V1.75a.25.25 0 0 0-.25-.25H6.5V5Zm-13 1.5v7.75c0 .138.112.25.25.25H5v-8ZM5 5V1.5H1.75a.25.25 0 0 0-.25.25V5Z">
                     </path>
                    </svg>
                   </span>
                   <span class="ActionListItem-label" data-view-component="true">
                    Projects
                   </span>
                  </a>
                 </li>
                 <li class="ActionListItem" data-menu-item="i4security-tab" data-targets="action-list.items" data-view-component="true" hidden="hidden" role="none">
                  <a class="ActionListContent ActionListContent--visual16" data-view-component="true" href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/security" id="item-ca5f809f-fbe6-469a-aee6-0e22005c0e63" role="menuitem" tabindex="-1">
                   <span class="ActionListItem-visual ActionListItem-visual--leading">
                    <svg aria-hidden="true" class="octicon octicon-shield" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                     <path d="M7.467.133a1.748 1.748 0 0 1 1.066 0l5.25 1.68A1.75 1.75 0 0 1 15 3.48V7c0 1.566-.32 3.182-1.303 4.682-.983 1.498-2.585 2.813-5.032 3.855a1.697 1.697 0 0 1-1.33 0c-2.447-1.042-4.049-2.357-5.032-3.855C1.32 10.182 1 8.566 1 7V3.48a1.75 1.75 0 0 1 1.217-1.667Zm.61 1.429a.25.25 0 0 0-.153 0l-5.25 1.68a.25.25 0 0 0-.174.238V7c0 1.358.275 2.666 1.057 3.86.784 1.194 2.121 2.34 4.366 3.297a.196.196 0 0 0 .154 0c2.245-.956 3.582-2.104 4.366-3.298C13.225 9.666 13.5 8.36 13.5 7V3.48a.251.251 0 0 0-.174-.237l-5.25-1.68ZM8.75 4.75v3a.75.75 0 0 1-1.5 0v-3a.75.75 0 0 1 1.5 0ZM9 10.5a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z">
                     </path>
                    </svg>
                   </span>
                   <span class="ActionListItem-label" data-view-component="true">
                    Security
                   </span>
                  </a>
                 </li>
                 <li class="ActionListItem" data-menu-item="i5insights-tab" data-targets="action-list.items" data-view-component="true" hidden="hidden" role="none">
                  <a class="ActionListContent ActionListContent--visual16" data-view-component="true" href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/pulse" id="item-4b683431-0335-4bd8-941e-a2b09e5ea08d" role="menuitem" tabindex="-1">
                   <span class="ActionListItem-visual ActionListItem-visual--leading">
                    <svg aria-hidden="true" class="octicon octicon-graph" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                     <path d="M1.5 1.75V13.5h13.75a.75.75 0 0 1 0 1.5H.75a.75.75 0 0 1-.75-.75V1.75a.75.75 0 0 1 1.5 0Zm14.28 2.53-5.25 5.25a.75.75 0 0 1-1.06 0L7 7.06 4.28 9.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.25-3.25a.75.75 0 0 1 1.06 0L10 7.94l4.72-4.72a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042Z">
                     </path>
                    </svg>
                   </span>
                   <span class="ActionListItem-label" data-view-component="true">
                    Insights
                   </span>
                  </a>
                 </li>
                </ul>
               </div>
              </action-list>
             </div>
            </div>
           </anchored-position>
          </focus-group>
         </action-menu>
        </div>
       </nav>
      </div>
      <turbo-frame class="" data-turbo-action="advance" id="repo-content-turbo-frame" target="_top">
       <div class="repository-content" id="repo-content-pjax-container">
        <h1 class="sr-only">
         AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset
        </h1>
        <div class="clearfix container-xl px-md-4 px-lg-5 px-3">
         <div>
          <div aria-hidden="" class="mt-0 pb-3" hidden="" id="spoof-warning">
           <div class="flash flash-warn mt-0 clearfix" data-view-component="true">
            <svg aria-hidden="true" class="octicon octicon-alert float-left mt-1" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
             <path d="M6.457 1.047c.659-1.234 2.427-1.234 3.086 0l6.082 11.378A1.75 1.75 0 0 1 14.082 15H1.918a1.75 1.75 0 0 1-1.543-2.575Zm1.763.707a.25.25 0 0 0-.44 0L1.698 13.132a.25.25 0 0 0 .22.368h12.164a.25.25 0 0 0 .22-.368Zm.53 3.996v2.5a.75.75 0 0 1-1.5 0v-2.5a.75.75 0 0 1 1.5 0ZM9 11a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z">
             </path>
            </svg>
            <div class="overflow-hidden">
             This commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.
            </div>
           </div>
          </div>
          <include-fragment data-test-selector="spoofed-commit-check" src="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/spoofed_commit_check/1f2e98840e9629e82d7f1b6fff86b1321c7ae906">
          </include-fragment>
          <div class="Layout Layout--flowRow-until-md react-repos-overview-margin Layout--sidebarPosition-end Layout--sidebarPosition-flowRow-end" data-view-component="true" style="max-width: 100%">
           <div class="Layout-main" data-view-component="true">
            <script crossorigin="anonymous" defer="defer" src="https://github.githubassets.com/assets/react-lib-1fbfc5be2c18.js" type="application/javascript">
            </script>
            <script crossorigin="anonymous" defer="defer" src="https://github.githubassets.com/assets/vendors-node_modules_primer_octicons-react_dist_index_esm_js-node_modules_primer_react_lib-es-541a38-ade861844008.js" type="application/javascript">
            </script>
            <script crossorigin="anonymous" defer="defer" src="https://github.githubassets.com/assets/vendors-node_modules_primer_react_lib-esm_Box_Box_js-8f8c5e2a2cbf.js" type="application/javascript">
            </script>
            <script crossorigin="anonymous" defer="defer" src="https://github.githubassets.com/assets/vendors-node_modules_primer_react_lib-esm_Button_Button_js-95a7748e3c39.js" type="application/javascript">
            </script>
            <script crossorigin="anonymous" defer="defer" src="https://github.githubassets.com/assets/vendors-node_modules_primer_react_lib-esm_TooltipV2_Tooltip_js-5c105bd4b6bc.js" type="application/javascript">
            </script>
            <script crossorigin="anonymous" defer="defer" src="https://github.githubassets.com/assets/vendors-node_modules_primer_react_node_modules_primer_octicons-react_dist_index_esm_mjs-cb996b1b8e38.js" type="application/javascript">
            </script>
            <script crossorigin="anonymous" defer="defer" src="https://github.githubassets.com/assets/vendors-node_modules_primer_react_lib-esm_ActionList_index_js-f41028bf9254.js" type="application/javascript">
            </script>
            <script crossorigin="anonymous" defer="defer" src="https://github.githubassets.com/assets/vendors-node_modules_primer_react_lib-esm_AnchoredOverlay_AnchoredOverlay_js-6305545ffa4a.js" type="application/javascript">
            </script>
            <script crossorigin="anonymous" defer="defer" src="https://github.githubassets.com/assets/vendors-node_modules_react-router-dom_dist_index_js-3b41341d50fe.js" type="application/javascript">
            </script>
            <script crossorigin="anonymous" defer="defer" src="https://github.githubassets.com/assets/vendors-node_modules_primer_react_lib-esm_Dialog_js-node_modules_primer_react_lib-esm_TabNav_-8321f5-8bee569a21c5.js" type="application/javascript">
            </script>
            <script crossorigin="anonymous" defer="defer" src="https://github.githubassets.com/assets/vendors-node_modules_primer_react_lib-esm_UnderlineNav_index_js-0345ef2c2625.js" type="application/javascript">
            </script>
            <script crossorigin="anonymous" defer="defer" src="https://github.githubassets.com/assets/vendors-node_modules_primer_react_lib-esm_AvatarStack_AvatarStack_js-node_modules_primer_reac-5f8982-a43826693bea.js" type="application/javascript">
            </script>
            <script crossorigin="anonymous" defer="defer" src="https://github.githubassets.com/assets/vendors-node_modules_primer_react_lib-esm_ActionMenu_ActionMenu_js-node_modules_primer_react_-04d00e-483d941beaca.js" type="application/javascript">
            </script>
            <script crossorigin="anonymous" defer="defer" src="https://github.githubassets.com/assets/ui_packages_react-core_create-browser-history_ts-ui_packages_safe-storage_safe-storage_ts-ui_-682c2c-e45e451173ec.js" type="application/javascript">
            </script>
            <script crossorigin="anonymous" defer="defer" src="https://github.githubassets.com/assets/ui_packages_paths_index_ts-ad6a9a567cec.js" type="application/javascript">
            </script>
            <script crossorigin="anonymous" defer="defer" src="https://github.githubassets.com/assets/ui_packages_ref-selector_RefSelector_tsx-b257014a1aab.js" type="application/javascript">
            </script>
            <script crossorigin="anonymous" defer="defer" src="https://github.githubassets.com/assets/app_assets_modules_react-shared_hooks_use-canonical-object_ts-ui_packages_code-view-shared_ho-e725dc-c2ba3df232d1.js" type="application/javascript">
            </script>
            <script crossorigin="anonymous" defer="defer" src="https://github.githubassets.com/assets/repos-overview-0713cf008af5.js" type="application/javascript">
            </script>
            <react-partial data-ssr="true" partial-name="repos-overview">
             <script data-target="react-partial.embeddedData" type="application/json">
              {"props":{"initialPayload":{"allShortcutsEnabled":false,"path":"/","repo":{"id":306616897,"defaultBranch":"master","name":"awesome-segmentation-saliency-dataset","ownerLogin":"AIMonk-Labs-Private-Limited","currentUserCanPush":false,"isFork":true,"isEmpty":false,"createdAt":"2020-10-23T11:32:59.000Z","ownerAvatar":"https://avatars.githubusercontent.com/u/72787137?v=4","public":true,"private":false,"isOrgOwned":true},"currentUser":null,"refInfo":{"name":"master","listCacheKey":"v0:1622788144.898206","canEdit":false,"refType":"branch","currentOid":"1f2e98840e9629e82d7f1b6fff86b1321c7ae906"},"tree":{"items":[{"name":"assets","path":"assets","contentType":"directory"},{"name":"css","path":"css","contentType":"directory"},{"name":"README.html","path":"README.html","contentType":"file"},{"name":"README.md","path":"README.md","contentType":"file"}],"templateDirectorySuggestionUrl":null,"readme":null,"totalCount":4,"showBranchInfobar":true},"fileTree":null,"fileTreeProcessingTime":null,"foldersToFetch":[],"treeExpanded":false,"symbolsExpanded":false,"isOverview":true,"overview":{"banners":{"shouldRecommendReadme":false,"isPersonalRepo":false,"showUseActionBanner":false,"actionSlug":null,"actionId":null,"showProtectBranchBanner":false,"publishBannersInfo":{"dismissActionNoticePath":"/settings/dismiss-notice/publish_action_from_repo","releasePath":"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/releases/new?marketplace=true","showPublishActionBanner":false},"interactionLimitBanner":null,"showInvitationBanner":false,"inviterName":null},"codeButton":{"contactPath":"/contact","isEnterprise":false,"local":{"protocolInfo":{"httpAvailable":true,"sshAvailable":null,"httpUrl":"https://github.com/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset.git","showCloneWarning":null,"sshUrl":null,"sshCertificatesRequired":null,"sshCertificatesAvailable":null,"ghCliUrl":"gh repo clone AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset","defaultProtocol":"http","newSshKeyUrl":"/settings/ssh/new","setProtocolPath":"/users/set_protocol"},"platformInfo":{"cloneUrl":"https://desktop.github.com","showVisualStudioCloneButton":false,"visualStudioCloneUrl":"https://windows.github.com","showXcodeCloneButton":false,"xcodeCloneUrl":"https://developer.apple.com","zipballUrl":"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/archive/refs/heads/master.zip"}},"newCodespacePath":"/codespaces/new?hide_repo_select=true\u0026repo=306616897"},"popovers":{"rename":null,"renamedParentRepo":null},"commitCount":"40","overviewFiles":[{"displayName":"README.md","repoName":"awesome-segmentation-saliency-dataset","refName":"master","path":"README.md","preferredFileType":"readme","tabName":"README","richText":"\u003carticle class=\"markdown-body entry-content container-lg\" itemprop=\"text\"\u003e\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch1 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eAnother Awesome Dataset List\u003c/h1\u003e\u003ca id=\"user-content-another-awesome-dataset-list\" class=\"anchor\" aria-label=\"Permalink: Another Awesome Dataset List\" href=\"#another-awesome-dataset-list\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003e\u003ca href=\"https://github.com/sindresorhus/awesome\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/50cf39121274b3db22bf1bd72cbe25af9078e037441cb5b5bdef1cc9dc5eb2f7/68747470733a2f2f63646e2e7261776769742e636f6d2f73696e647265736f726875732f617765736f6d652f643733303566333864323966656437386661383536353265336136336531353464643865383832392f6d656469612f62616467652e737667\" alt=\"Awesome\" data-canonical-src=\"https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e💖 Some Great Tools 💖:\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e⭐⭐ \u003ccode\u003eGoogle Dataset Search\u003c/code\u003e: \u003ca href=\"https://datasetsearch.research.google.com/\" rel=\"nofollow\"\u003ehttps://datasetsearch.research.google.com/\u003c/a\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003eAI开发者神器! 谷歌重磅推出数据集搜索 Dataset Search: \u003ca href=\"https://mp.weixin.qq.com/s/ErbwXAz-_AJrmUGMHZIcwg\" rel=\"nofollow\"\u003ehttps://mp.weixin.qq.com/s/ErbwXAz-_AJrmUGMHZIcwg\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eMaking it easier to discover datasets: \u003ca href=\"https://www.blog.google/products/search/making-it-easier-discover-datasets/\" rel=\"nofollow\"\u003ehttps://www.blog.google/products/search/making-it-easier-discover-datasets/\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e⭐⭐⭐ \u003ccode\u003eYet Another Computer Vision Index To Datasets (YACVID)\u003c/code\u003e: \u003ca href=\"http://yacvid.hayko.at/\" rel=\"nofollow\"\u003ehttp://yacvid.hayko.at/\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp dir=\"auto\"\u003ePlease \u003cstrong\u003ecite related paper\u003c/strong\u003e if you \u003cstrong\u003euse their dataset\u003c/strong\u003e 😄\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eI list some other datasets in the issue \u003ca class=\"issue-link js-issue-link\" data-error-text=\"Failed to load title\" data-id=\"484025908\" data-permission-text=\"Title is private\" data-url=\"https://github.com/lartpang/awesome-segmentation-saliency-dataset/issues/15\" data-hovercard-type=\"issue\" data-hovercard-url=\"/lartpang/awesome-segmentation-saliency-dataset/issues/15/hovercard\" href=\"https://github.com/lartpang/awesome-segmentation-saliency-dataset/issues/15\"\u003elartpang#15\u003c/a\u003e. I hope it works for you.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e\u003ca href=\"#another-awesome-dataset-list\"\u003eAnother Awesome Dataset List\u003c/a\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e\u003ca href=\"#saliency\"\u003eSaliency\u003c/a\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e\u003ca href=\"#rgb-saliency-detection\"\u003eRGB-Saliency Detection\u003c/a\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e\u003ca href=\"#msramsra10kmsra-b\"\u003eMSRA(MSRA10K/MSRA-B)\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#sed12\"\u003eSED1/2\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#asdmsra1000msra1kneed-some-images\"\u003eASD(MSRA1000/MSRA1K)[need some images]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#dut-omron\"\u003eDUT-OMRON\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#duts\"\u003eDUTS\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#hku-isneed-some-iamges\"\u003eHKU-IS[need some iamges]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#sod\"\u003eSOD\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#icoseg\"\u003eiCoSeg\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#infraredneed-help\"\u003eInfrared[need help]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#imgsal\"\u003eImgSal\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#ecssdcssd\"\u003eECSSD/CSSD\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#thur15k\"\u003eTHUR15K\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#bruce-aneed-help\"\u003eBruce-A[need help]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#judd-aneed-help\"\u003eJudd-A[need help]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#pascal-s\"\u003ePASCAL-S\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#ucsbneed-help\"\u003eUCSB[need help]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#osieneed-help\"\u003eOSIE[need help]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#acsd\"\u003eACSD\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#other-special-sod-datasets\"\u003eOther Special SOD Datasets\u003c/a\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e\u003ca href=\"#xpie\"\u003eXPIE\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#soc\"\u003eSOC\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#sosmosneed-some-images\"\u003eSOS/MOS[need some images]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#ilsoneed-some-images\"\u003eILSO[need some images]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#hs-sod\"\u003eHS-SOD\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#video-saliency-detection\"\u003eVideo Saliency Detection\u003c/a\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e\u003ca href=\"#rsdpku-rsd\"\u003eRSD(PKU-RSD)\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#stcneed-help\"\u003eSTC[need help]\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#rgbd-saliency-detection\"\u003eRGBD-Saliency Detection\u003c/a\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e\u003ca href=\"#sip\"\u003eSIP\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#nlprrgbd1000\"\u003eNLPR/RGBD1000\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#nju4002000\"\u003eNJU400/2000\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#stereossb\"\u003eSTEREO/SSB\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#lfsdnead-img\"\u003eLFSD[nead img]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#rgbd135des\"\u003eRGBD135/DES\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#dut-rgbd\"\u003eDUT-RGBD\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#ssd100\"\u003eSSD100\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#rgbt-saliency-detection-need-more-information\"\u003eRGBT-Saliency Detection [need more information...]\u003c/a\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e\u003ca href=\"#vt1000-dataset\"\u003eVT1000 Dataset\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#vt821-dataset\"\u003eVT821 Dataset\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#high-resolution-saliency-detection\"\u003eHigh-Resolution Saliency Detection\u003c/a\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e\u003ca href=\"#hrsoddavis-s\"\u003eHRSOD/DAVIS-S\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#other-saliency-dataset\"\u003eOther Saliency Dataset\u003c/a\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e\u003ca href=\"#kaist-salient-pedestrian-dataset\"\u003eKAIST Salient Pedestrian Dataset\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#segmentation\"\u003eSegmentation\u003c/a\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e\u003ca href=\"#generalneed-help\"\u003eGeneral[need help]\u003c/a\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e\u003ca href=\"#davis\"\u003eDAVIS\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#anyu\"\u003eaNYU\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#about-person\"\u003eAbout Person\u003c/a\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e\u003ca href=\"#supervisely%E4%BA%BA%E5%83%8F%E6%95%B0%E6%8D%AE%E9%9B%86\"\u003eSupervisely人像数据集\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#clothing-parsing\"\u003eClothing Parsing\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#humanparsing-dataset\"\u003eHumanParsing-Dataset\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#look-into-person-lip\"\u003eLook into Person (LIP)\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#taobao-commodity-dataset\"\u003eTaobao Commodity Dataset\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#object-extraction-dataset\"\u003eObject Extraction Dataset\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#clothing-co-parsing-ccp-dataset\"\u003eClothing Co-Parsing (CCP) Dataset\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#baidu-people-segmentation-datasetneed-help\"\u003eBaidu People segmentation dataset[need help]\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#matting\"\u003eMatting\u003c/a\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e\u003ca href=\"#alphamattingcom\"\u003ealphamatting.com\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#composition-1k-deep-image-matting\"\u003eComposition-1k: Deep Image Matting\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#semantic-human-matting\"\u003eSemantic Human Matting\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#matting-human-datasets\"\u003eMatting-Human-Datasets\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#pfcn\"\u003ePFCN\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#deep-automatic-portrait-matting\"\u003eDeep Automatic Portrait Matting\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#other\"\u003eOther\u003c/a\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e\u003ca href=\"#large-scale-fashion-deepfashion-database\"\u003eLarge-scale Fashion (DeepFashion) Database\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#ml-image\"\u003eML-Image\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#need-your-help\"\u003eneed your help...\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#reference\"\u003eReference\u003c/a\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e\u003ca href=\"#salient-object-detection-a-survey\"\u003eSalient Object Detection: A Survey\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#review-of-visual-saliency-detection-with-comprehensive-information\"\u003eReview of Visual Saliency Detection with Comprehensive Information\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#salient-object-detection-in-the-deep-learning-era-an-in-depth-survey\"\u003eSalient Object Detection in the Deep Learning Era: An In-Depth Survey\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#more\"\u003eMore\u003c/a\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e\u003ca href=\"#similiar-projects\"\u003eSimiliar Projects\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#research-institutes\"\u003eResearch Institutes\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#resource-websites\"\u003eResource Websites\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#about\"\u003eAbout\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eSaliency\u003c/h2\u003e\u003ca id=\"user-content-saliency\" class=\"anchor\" aria-label=\"Permalink: Saliency\" href=\"#saliency\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eRGB-Saliency Detection\u003c/h3\u003e\u003ca id=\"user-content-rgb-saliency-detection\" class=\"anchor\" aria-label=\"Permalink: RGB-Saliency Detection\" href=\"#rgb-saliency-detection\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch4 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eMSRA(MSRA10K/MSRA-B)\u003c/h4\u003e\u003ca id=\"user-content-msramsra10kmsra-b\" class=\"anchor\" aria-label=\"Permalink: MSRA(MSRA10K/MSRA-B)\" href=\"#msramsra10kmsra-b\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"https://camo.githubusercontent.com/3250531d515ac0cc26ef6edbe4cca3aa7338f54720a59159147d1594919fc8da/68747470733a2f2f6d6d6368656e672e6e65742f77702d636f6e74656e742f75706c6f6164732f323031342f30372f4d53524131304b2e6a7067\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/3250531d515ac0cc26ef6edbe4cca3aa7338f54720a59159147d1594919fc8da/68747470733a2f2f6d6d6368656e672e6e65742f77702d636f6e74656e742f75706c6f6164732f323031342f30372f4d53524131304b2e6a7067\" alt=\"img\" data-canonical-src=\"https://mmcheng.net/wp-content/uploads/2014/07/MSRA10K.jpg\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e论文: \u003ca href=\"http://mmlab.ie.cuhk.edu.hk/2007/CVPR07_detect.pdf\" rel=\"nofollow\"\u003eT. Liu, J. Sun, N. Zheng, X. Tang, and H.-Y. Shum, \"Learningto detect a salient object, \" inCVPR, 2007, pp.1–8\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e主页: 南开大学媒体计算实验室: \u003ca href=\"https://mmcheng.net/zh/msra10k/\" rel=\"nofollow\"\u003ehttps://mmcheng.net/zh/msra10k/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e下载:\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003eMSRA10K(formally named as THUS10000; \u003ca href=\"http://mftp.mmcheng.net/Data/MSRA10K_Imgs_GT.zip\" rel=\"nofollow\"\u003e195MB\u003c/a\u003e: images + binary masks):\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003ePixel accurate salient object labeling for \u003cstrong\u003e10000 images\u003c/strong\u003e from MSRA dataset.\u003c/li\u003e\n\u003cli\u003ePlease cite our paper [\u003ca href=\"https://mmcheng.net/SalObj/\" rel=\"nofollow\"\u003ehttps://mmcheng.net/SalObj/\u003c/a\u003e] if you use it.\u003c/li\u003e\n\u003cli\u003eSaliency maps and salient object region segmentation for other 20+ alternative methods are also available (\u003ca href=\"http://pan.baidu.com/s/1dEaQqlF#path=%252FShare%252FSalObjRes\" rel=\"nofollow\"\u003e百度网盘\u003c/a\u003e).\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eMSRA-B (\u003ca href=\"http://mftp.mmcheng.net/Data/MSRA-B.zip\" rel=\"nofollow\"\u003e111MB\u003c/a\u003e: images + binary masks):\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003ePixel accurate salient object labeling for \u003cstrong\u003e5000 images\u003c/strong\u003e from MSRA-B dataset.\u003c/li\u003e\n\u003cli\u003ePlease cite the corresponding paper [\u003ca href=\"https://mmcheng.net/drfi/\" rel=\"nofollow\"\u003ehttps://mmcheng.net/drfi/\u003c/a\u003e] if you use it.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp dir=\"auto\"\u003e我们通过检测输入图像中的显着对象来研究视觉注意力. 我们将显着对象检测表示为图像分割问题, 我们将显着对象与图像背景分开. 我们提出了一系列新颖的特征, 包括多尺度对比度, 中心环绕直方图和颜色空间分布, 以在本地, 区域和全局描述显着对象. 学习条件随机场以有效地组合这些特征以用于显着对象检测. 我们还构建了一个\u003cstrong\u003e包含由多个用户标记的数以万计的完全标记图像的图像数据库\u003c/strong\u003e. 据我们所知, 它是第一个用于视觉注意算法定量评估的大型图像数据库. 我们在此图像数据库上验证了我们的方法, 该数据库在本文中是公开的.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e人们可能对图像中的显着对象有不同的看法. 为了解决\"给定图像中可能是什么样的显着对象\"的问题, 我们通过在多个用户的图像中标记\"基础事实\"显着对象来进行投票策略. 在本文中, 我们关注图像中单个显着对象的情况.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e显著性对象表示. 通常, 我们\u003cstrong\u003e将给定对象表示为给定image I中的二元mask\u003c/strong\u003e \u003cmath-renderer class=\"js-inline-math\" style=\"display: inline\" data-static-url=\"https://github.githubassets.com/static\" data-run-id=\"41dafde2f9a470d0c64156772edde64a\"\u003e$A={a_x}$\u003c/math-renderer\u003e. 对于每个像素x, \u003cmath-renderer class=\"js-inline-math\" style=\"display: inline\" data-static-url=\"https://github.githubassets.com/static\" data-run-id=\"41dafde2f9a470d0c64156772edde64a\"\u003e$a_x∈{1, 0}$\u003c/math-renderer\u003e是二进制标签, 以指示像素是否属于显着对象.\u003cstrong\u003e为了标记和评估, 我们要求用户绘制一个矩形来指定一个显着对象. 我们的检测算法也输出一个矩形.\u003c/strong\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e图像来源. 我们收集了一个非常大的图像数据库, 其中130, 099个来自各种来源的高质量图像, 主要来自图像论坛和图像搜索引擎. 然后我们手动选择60, 000多个图像, 每个图像包含一个显着对象或一个独特的前景对象. 我们进一步选择了20, 840张图片进行标记. 在选择过程中, 我们\u003cstrong\u003e排除了包含非常大的显着对象的任何图像\u003c/strong\u003e, 从而可以更准确地评估检测的性能.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e标记一致性. 对于每个要标记的图像, 我们请用户绘制一个矩形, 该矩形包围图像中最大的对象根据他/她自己的理解. 由不同用户标记的矩形通常不相同. 为了减少标签的不一致性, 我们从多个用户绘制的矩形中选择一个\"真实\"标签.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch4 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eSED1/2\u003c/h4\u003e\u003ca id=\"user-content-sed12\" class=\"anchor\" aria-label=\"Permalink: SED1/2\" href=\"#sed12\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e单目标\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/2018-12-29-18-38-59.png\"\u003e\u003cimg src=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/2018-12-29-18-38-59.png\" alt=\"img\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e双目标\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/2018-12-29-18-39-30.png\"\u003e\u003cimg src=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/2018-12-29-18-39-30.png\" alt=\"img\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e真值\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp dir=\"auto\"\u003e给出的是每个图像由三个不同的人类对象分割的结果.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/2018-12-29-18-40-17.png\"\u003e\u003cimg src=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/2018-12-29-18-40-17.png\" alt=\"img\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/1501.02741\" rel=\"nofollow\"\u003eA. Borji, M.-M. Cheng, H. Jiang, and J. Li, \"Salient objectdetection: A benchmark, \"IEEE TIP, vol.24, no.12, pp.5706–5722, 2015.\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://www.wisdom.weizmann.ac.il/~meirav/Segmentation_Alpert_Galun_Brandt_Basri.pdf\" rel=\"nofollow\"\u003eImage Segmentation by Probabilistic Bottom-Up Aggregation and Cue Integration\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e项目: \u003ca href=\"http://www.wisdom.weizmann.ac.il/~vision/Seg_Evaluation_DB/index.html\" rel=\"nofollow\"\u003ehttp://www.wisdom.weizmann.ac.il/~vision/Seg_Evaluation_DB/index.html\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e下载: \u003ca href=\"http://www.wisdom.weizmann.ac.il/~vision/Seg_Evaluation_DB/dl.html\" rel=\"nofollow\"\u003ehttp://www.wisdom.weizmann.ac.il/~vision/Seg_Evaluation_DB/dl.html\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp dir=\"auto\"\u003e这项工作的目的是为图像分割研究提供经验和科学依据. 评估分割算法产生的结果具有挑战性, 因为很难提出提供基础真实分割的规范测试集. 这部分是因为在日常复杂图像中手动描绘片段可能是费力的. 此外, 人们往往倾向于将语义考虑纳入其分段中, 这超出了数据驱动的分割算法的范围. 因此, 许多现有算法仅显示很少的分割结果. 为了评估由不同算法产生的分割, 我们编制了一个数据库, 目前\u003cstrong\u003e包含200个灰度图像以及真实标注分割\u003c/strong\u003e. 该数据库专门设计用于避免潜在的模糊, 仅通过仅通过强度, 纹理或其他低水平线索合并清晰描绘前景中与其周围环境不同的一个或两个物体的图像. 通过要求人类对象手动地将灰度图像(还提供颜色源)分成两个或三个类别来获得地面真实分割, 其中\u003cstrong\u003e每个图像由三个不同的人类对象分割\u003c/strong\u003e. 通过评估其与真实分割的一致性及其碎片量来评估分割. 与此数据库评估一起, 我们提供了用于评估给定分割算法的代码. 这样, 不同的分割算法可能具有可比较的结果以获得更多细节, 请参阅\"评估测试\"部分.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch4 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eASD(MSRA1000/MSRA1K)[need some images]\u003c/h4\u003e\u003ca id=\"user-content-asdmsra1000msra1kneed-some-images\" class=\"anchor\" aria-label=\"Permalink: ASD(MSRA1000/MSRA1K)[need some images]\" href=\"#asdmsra1000msra1kneed-some-images\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e论文:\u003ca href=\"https://www.researchgate.net/publication/224312323_A_two-stage_approach_to_saliency_detection_in_images\" rel=\"nofollow\"\u003eA two-stage approach to saliency detection inimages\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e相关:\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003eT. Liu, J. Sun, N.-N. Zheng, X. Tang, and H.-Y. Shum, \"\u003ca href=\"http://research.microsoft.com/en-us/um/people/jiansun/salientobject/salient_object.htm\" rel=\"nofollow\"\u003eLearning to detect a salient object\u003c/a\u003e, \" in \u003cem\u003eProc. IEEE Conf. Comput. Vis. Pattern Recognit.\u003c/em\u003e, 2007, pp.1–8.\u003c/li\u003e\n\u003cli\u003eR. Achanta, S. Hemami, F. Estrada, and S. Süsstrunk, \"\u003ca href=\"http://ivrlwww.epfl.ch/supplementary_material/RK_CVPR09/\" rel=\"nofollow\"\u003eFrequency-tuned salient region detection\u003c/a\u003e, \" in \u003cem\u003eProc. IEEE Conf. Comput. Vis. Pattern Recognit.\u003c/em\u003e, 2009, pp.1597–1604.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e下载: \u003ca href=\"http://download.csdn.net/detail/wanyq07/9839322\" rel=\"nofollow\"\u003ehttp://download.csdn.net/detail/wanyq07/9839322\u003c/a\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e关于下载的说明: 因为基于MSRA的图片数据集, 在孙剑走了之后, MARA上就没了他的页面, 相关的资源也就找不到了. CSDN一篇博客有分享. 原图下载地址:\u003ca href=\"http://download.csdn.net/detail/tuconghuan/8357509\" rel=\"nofollow\"\u003eMSRA图像数据集(1000幅含真实标注)\u003c/a\u003e. 上面下载到的标注图尺寸被统一改为512*512, 所以这里在给个地址:\u003ca href=\"http://download.csdn.net/detail/zzb4702/9559378\" rel=\"nofollow\"\u003eASD尺寸一致\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp dir=\"auto\"\u003eASD contains 1, 000 images with pixel-wise ground-truths. The images are selected from the MSRA-A dataset, where only the bounding boxes around salient regions are provided. The accurate salient masks in ASD are created based on object contours.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e这个数据集包含有1000张图(MSRA1000)这个数据库来自于 该数据库的说明以及一些算法(IT, MZ, GB, SR, AC, IG ) 的结果可以在\u003ca href=\"http://ivrlwww.epfl.ch/supplementary_material/RK_CVPR09/index.html\" rel=\"nofollow\"\u003eFrequency-tuned Salient Region Detection\u003c/a\u003e (FT算法 =\u0026gt; 这里改进的数据集叫做ACSD, 相关可见\u003ca href=\"#ACSD\"\u003eACSD\u003c/a\u003e部分)下载, 此外其中还包含了这1000张测试图的真值图.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch4 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eDUT-OMRON\u003c/h4\u003e\u003ca id=\"user-content-dut-omron\" class=\"anchor\" aria-label=\"Permalink: DUT-OMRON\" href=\"#dut-omron\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/2019-03-22-18-45-56.png\"\u003e\u003cimg src=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/2019-03-22-18-45-56.png\" alt=\"img\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e论文: C. Yang, L. Zhang, H. Lu, X. Ruan, and M.-H. Yang, \"\u003ca href=\"http://saliencydetection.net/dut-omron/\" rel=\"nofollow\"\u003eSaliency detection via graph-based manifold ranking\u003c/a\u003e, \" in \u003cem\u003eProc. IEEE Conf. Comput. Vis. Pattern Recognit.\u003c/em\u003e, 2013, pp.3166–3173.\u003c/li\u003e\n\u003cli\u003e项目: \u003ca href=\"http://saliencydetection.net/dut-omron/#outline-container-org0e04792\" rel=\"nofollow\"\u003ehttp://saliencydetection.net/dut-omron/#outline-container-org0e04792\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e下载: \u003ca href=\"http://saliencydetection.net/dut-omron/download/DUT-OMRON-image.zip\" rel=\"nofollow\"\u003ehttp://saliencydetection.net/dut-omron/download/DUT-OMRON-image.zip\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp dir=\"auto\"\u003e数据库包括从超过140, 000张图像中手动选择的5, 168个高质量图像. 我们将图像的大小调整为宽为400或高为400像素, 其中另一条边小于400. 我们数据库的图像具有一个或多个显着对象和相对复杂的背景. 我们共有25名参与者, 用于汇总真值, 每个图像有五个参与者标签. 他们都有正常或矫正到正常的视力并且意识到我们实验的目标. 我们为提出的数据库构建像素方面的真实标注, 边界框, 和眼睛固定标注真值.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e我们的数据集是唯一一个具有眼睛固定, 边界框和像素方面的大规模真实标注的数据集. 与ASD和MSRA数据集以及其他一些眼睛固定数据集(即MIT和NUSEF数据集)相比, 数据集中的图像更加困难, 因此更具挑战性, 并为相关的显着性研究提供了更多的改进空间.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch4 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eDUTS\u003c/h4\u003e\u003ca id=\"user-content-duts\" class=\"anchor\" aria-label=\"Permalink: DUTS\" href=\"#duts\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e项目: \u003ca href=\"http://saliencydetection.net/duts/\" rel=\"nofollow\"\u003ehttp://saliencydetection.net/duts/\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp dir=\"auto\"\u003e...we contribute a large scale data set named DUTS, \u003cstrong\u003econtaining 10, 553 training images and 5, 019 test images\u003c/strong\u003e. All training images are collected from the ImageNet DET training/val sets, while test images are collected from the ImageNet DET test set and the SUN data set.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eBoth the training and test set contain very challenging scenarios for saliency detection. Accurate pixel-level ground truths are manually annotated by 50 subjects.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eTo our knowledge, DUTS is currently \u003cstrong\u003ethe largest saliency detection benchmark\u003c/strong\u003e with the explicit training/test evaluation protocol.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eFor fair comparison in the future research, the training set of DUTS serves as a good candidate for learning DNNs, while the test set and other public data sets can be used for evaluation.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch4 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eHKU-IS[need some iamges]\u003c/h4\u003e\u003ca id=\"user-content-hku-isneed-some-iamges\" class=\"anchor\" aria-label=\"Permalink: HKU-IS[need some iamges]\" href=\"#hku-isneed-some-iamges\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e项目: \u003ca href=\"https://i.cs.hku.hk/~gbli/deep_saliency.html\" rel=\"nofollow\"\u003ehttps://i.cs.hku.hk/~gbli/deep_saliency.html\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e论文: \u003ca href=\"http://i.cs.hku.hk/~yzyu/publication/mdfsaliency-cvpr15.pdf\" rel=\"nofollow\"\u003eVisual Saliency Based on Multiscale Deep Features\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e下载:\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e\u003ca href=\"https://drive.google.com/open?id=0BxNhBO0S5JCRQ1N6V25VeVh6cHc\u0026amp;authuser=0\" rel=\"nofollow\"\u003eGoogle Drive\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://pan.baidu.com/s/1c0EpNfM\" rel=\"nofollow\"\u003eBaidu Yun\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp dir=\"auto\"\u003e数据集包含4447个具有显着对象的像素注释的图像\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e视觉显着性是包括计算机视觉在内的认知和计算科学中的一个基本问题. 在本文中, 我们发现可以从使用深度卷积神经网络(CNN)提取的多尺度特征中学习高质量的视觉显着性模型. 视觉识别任务的成功. 为了学习这样的显着性模型, 我们引入了一种神经网络结构, 它在CNN顶部具有完全连接的层, 负责三个不同尺度的特征提取. 然后, 我们提出一种改进方法来增强我们的显着性结果的空间一致性. 最后, 针对不同级别的图像分割计算的聚合多个显着性图可以进一步提高性能, 从而产生比由单个分割产生的显着性图更好的显着性图. 为了促进对视觉显着性模型的进一步研究和评估, \u003cstrong\u003e我们还构建了一个新的大型数据库, 包括4447个具有挑战性的图像及其像素显着性注释\u003c/strong\u003e.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch4 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eSOD\u003c/h4\u003e\u003ca id=\"user-content-sod\" class=\"anchor\" aria-label=\"Permalink: SOD\" href=\"#sod\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/2019-03-22-18-46-40.png\"\u003e\u003cimg src=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/2019-03-22-18-46-40.png\" alt=\"img\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e项目: \u003ca href=\"http://elderlab.yorku.ca/SOD/\" rel=\"nofollow\"\u003ehttp://elderlab.yorku.ca/SOD/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e下载\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e官方: \u003ca href=\"http://elderlab.yorku.ca/SOD/SOD.zip\" rel=\"nofollow\"\u003ehttp://elderlab.yorku.ca/SOD/SOD.zip\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e百度云: \u003ca href=\"https://pan.baidu.com/s/1IMElTPwD4yTo2TMSRU-keQ\" rel=\"nofollow\"\u003ehttps://pan.baidu.com/s/1IMElTPwD4yTo2TMSRU-keQ\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp dir=\"auto\"\u003e此数据集是基于Berkeley Segmentation Dataset(BSD)的显着对象边界的集合. 要求七个对象选择BSD中使用的每个图像中的显着对象. 每个主题随机显示伯克利分割数据集的子集, 作为在相应图像上重叠的边界. 然后, 可以通过单击选择哪些区域或区段对应于显着对象.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e对于BSD中使用的300个图像的每个图像, 都有一个.mat文件可以由Matlab打开. 加载每个mat文件会将一个名为\"SES\"的结构读入内存, 该结构是从SOD中每个主题的会话中收集的数据数组.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e💝 that the original images are available from the Berkely Segmentation Dataset at: \u003ca href=\"http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/segbench/\" rel=\"nofollow\"\u003ehttp://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/segbench/\u003c/a\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch4 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eiCoSeg\u003c/h4\u003e\u003ca id=\"user-content-icoseg\" class=\"anchor\" aria-label=\"Permalink: iCoSeg\" href=\"#icoseg\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/1546085516505.png\"\u003e\u003cimg src=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/1546085516505.png\" alt=\"1546085516505\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e论文: \u003ca href=\"http://chenlab.ece.cornell.edu/projects/touch-coseg/iCoseg_dataset.pdf\" rel=\"nofollow\"\u003ehttp://chenlab.ece.cornell.edu/projects/touch-coseg/iCoseg_dataset.pdf\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e项目: \u003ca href=\"http://chenlab.ece.cornell.edu/projects/touch-coseg/\" rel=\"nofollow\"\u003ehttp://chenlab.ece.cornell.edu/projects/touch-coseg/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e下载: \u003ca href=\"http://chenlab.ece.cornell.edu/projects/touch-coseg/CMU_Cornell_iCoseg_dataset.zip\" rel=\"nofollow\"\u003ehttp://chenlab.ece.cornell.edu/projects/touch-coseg/CMU_Cornell_iCoseg_dataset.zip\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp dir=\"auto\"\u003e我们引入了38组(643幅图像)中最大的公开可用的 co-segmentation, 以及像素标注真值.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch4 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eInfrared[need help]\u003c/h4\u003e\u003ca id=\"user-content-infraredneed-help\" class=\"anchor\" aria-label=\"Permalink: Infrared[need help]\" href=\"#infraredneed-help\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e项目: \u003ca href=\"https://ivrl.epfl.ch/research-2/research-downloads/supplementary_material-cvpr11-index-html/\" rel=\"nofollow\"\u003ehttps://ivrl.epfl.ch/research-2/research-downloads/supplementary_material-cvpr11-index-html/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e论文: \u003ca href=\"http://infoscience.epfl.ch/record/167478\" rel=\"nofollow\"\u003ehttp://infoscience.epfl.ch/record/167478\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e下载: \u003ca href=\"http://ivrgwww.epfl.ch/supplementary_material/cvpr11/nirscene1.zip\" rel=\"nofollow\"\u003ehttp://ivrgwww.epfl.ch/supplementary_material/cvpr11/nirscene1.zip\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp dir=\"auto\"\u003e我们使用对传统SLR相机的简单修改来捕获数百个彩色(RGB)和近红外(NIR)场景的图像. 我们表明, 近红外信息的添加导致场景识别任务中的性能显着提高, 并且当使用适当的4维颜色表示时, 改进仍然更大. 特别地, 我们提出了MSIFT-一种多光谱SIFT描述符, 当与基于内核的分类器结合时, 超过了现有技术的场景识别技术(例如GIST)及其多光谱扩展的性能. 我们使用数百个RGB-NIR场景图像的新数据集对我们的算法进行了广泛的测试, 并对Torralba的场景分类数据集进行了基准测试.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch4 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eImgSal\u003c/h4\u003e\u003ca id=\"user-content-imgsal\" class=\"anchor\" aria-label=\"Permalink: ImgSal\" href=\"#imgsal\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/1546087781641.png\"\u003e\u003cimg src=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/1546087781641.png\" alt=\"1546087781641\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e项目: \u003ca href=\"https://sites.google.com/site/jianlinudt/saliency-database\" rel=\"nofollow\"\u003ehttps://sites.google.com/site/jianlinudt/saliency-database\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e作者主页: \u003ca href=\"http://www.escience.cn/people/jianli/DataBase.html\" rel=\"nofollow\"\u003ehttp://www.escience.cn/people/jianli/DataBase.html\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp dir=\"auto\"\u003e数据库的特点\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e1.235个彩色图像的集合, 分为六个不同的类别;\n2. 提供人类固定记录(扫视数据)和人类标记结果;\n3. 易于使用.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e我们将同时考虑不同大小的显着区域的检测. 实际上, 可接受的显着性检测器应该检测大的和小的显着区域. 此外, 显着性检测还应该定位杂乱背景中的显着区域和具有重复干扰物的区域. 我们还注意到, 对于任何显着性检测器, 不同的图像呈现不同的难度. 但是, 现有的显着性基准(例如Bruce的数据集, Hou'dataset, Harel的数据集等)是图像集合, 没有尝试对所需分析的难度进行分类. 因此, 我们为显着性模型验证创建了一个新的显着性基准. 该数据库提供REGION基础事实(人类标记)和FIXATION基础事实(通过眼动仪).\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e图像集使用Google以及参考最近的文献收集了包含235张图像的数据库. 此数据库中的图像为480 x 640像素, 分为6类:1)50个具有大显着区域的图像; 2)具有中间显着区域的80幅图像; 3)具有小显着区域的60幅图像; 4)背景杂乱的15幅图像; 5)带有重复干扰物的15张图像; 6)具有大和小显着区域的15个图像.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch4 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eECSSD/CSSD\u003c/h4\u003e\u003ca id=\"user-content-ecssdcssd\" class=\"anchor\" aria-label=\"Permalink: ECSSD/CSSD\" href=\"#ecssdcssd\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/2019-03-22-18-47-32.png\"\u003e\u003cimg src=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/2019-03-22-18-47-32.png\" alt=\"img\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e下载: \u003ca href=\"http://www.cse.cuhk.edu.hk/leojia/projects/hsaliency/dataset.html\" rel=\"nofollow\"\u003ehttp://www.cse.cuhk.edu.hk/leojia/projects/hsaliency/dataset.html\u003c/a\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003eECSSD (1000 images)\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e\u003ca href=\"http://www.cse.cuhk.edu.hk/leojia/projects/hsaliency/data/ECSSD/images.zip\" rel=\"nofollow\"\u003eECSSD images (64.6MB)\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://www.cse.cuhk.edu.hk/leojia/projects/hsaliency/data/ECSSD/ground_truth_mask.zip\" rel=\"nofollow\"\u003eECSSD ground truth masks (1.78MB)\u003c/a\u003e (Updated on 9 April, 2015)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eCSSD (200 images)\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e\u003ca href=\"http://www.cse.cuhk.edu.hk/leojia/projects/hsaliency/data/CSSD/images.zip\" rel=\"nofollow\"\u003eCSSD images (18.7MB)\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://www.cse.cuhk.edu.hk/leojia/projects/hsaliency/data/CSSD/ground_truth_mask.zip\" rel=\"nofollow\"\u003eCSSD groud truth masks (0.75MB)\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp dir=\"auto\"\u003e其中CSSD包含了200张图, 而ECSSD是前者的扩展集包含有1000张图\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp dir=\"auto\"\u003e虽然MSRA-1000的图像内容种类繁多, 但背景结构主要是简单而流畅. 为了表示自然图像通常落入的情况, 我们将[1]中的复杂场景显着性数据集(CSSD)扩展到包含1000个图像的更大数据集(ECSSD)[2], 其中包含许多语义上有意义但结构复杂的图像用于评估. 这些图像是从互联网上获取的, 并要求5名助手制作地面真相面具. 上面显示了几个带有相应掩模的例子.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch4 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eTHUR15K\u003c/h4\u003e\u003ca id=\"user-content-thur15k\" class=\"anchor\" aria-label=\"Permalink: THUR15K\" href=\"#thur15k\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/1546088375285.png\"\u003e\u003cimg src=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/1546088375285.png\" alt=\"1546088375285\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e论文: \u003ca href=\"https://mmcheng.net/zh/gsal/\" rel=\"nofollow\"\u003ehttps://mmcheng.net/zh/gsal/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e下载: \u003ca href=\"https://mmcheng.net/mftp/Data/THUR15000.zip\" rel=\"nofollow\"\u003ehttps://mmcheng.net/mftp/Data/THUR15000.zip\u003c/a\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e百度云: \u003ca href=\"https://pan.baidu.com/s/1u-E-8ujnxBz0mdmXsJglvg\" rel=\"nofollow\"\u003ehttps://pan.baidu.com/s/1u-E-8ujnxBz0mdmXsJglvg\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp dir=\"auto\"\u003e有效识别大型图像集中的显着对象对于许多应用是必不可少的, 包括图像检索, 监视, 图像注释和对象识别. 我们提出了一种简单, 快速, 有效的算法, 通过分析图像集合来定位和分割显着对象. 作为一个关键的新颖性, 我们通过提取最大化图像间相似性和图像内清晰度的显着对象(在预过滤图像的集合中)来引入群体显着性以实现优越的无监督显着对象分割. 为了评估我们的方法, 我们构建了一个大型基准数据集, \u003cstrong\u003e该数据集包含多个类别的15K图像, 适用于显着对象区域的6000多个像素精确的地面实况注释\u003c/strong\u003e. 在我们的所有测试中, group saliency 始终优于最先进的单图像显着性算法, 从而实现更高的精度和更好的回忆. 我们的算法成功处理了比任何现有基准数据集更大的订单的图像集合, 包括来自各种网络间源的各种异构图像.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e我们引入了分类图像的标记数据集, 用于评估基于草图的图像检索. 我们为5个关键字中的每一个下载了大约3000张图像:\"蝴蝶\", \"咖啡杯\", \"狗跳\", \"长颈鹿\"和\"平面\", 一起包括大约15000张图像.\u003cstrong\u003e对于每个图像, 如果存在具有与查询关键字匹配的正确内容的非模糊对象并且对象的大部分可见, 则我们标记这样的对象区域. 与MSRA10K类似, 显着区域以像素级别标记. 我们只标记几乎完全可见的对象的显着对象区域, 因为部分遮挡的对象对形状匹配不太有用. 与MSRA10K不同, THUR15K数据集不包含为数据集中的每个图像标记的显着区域, 即, 一些图像可能没有任何显着区域. 该数据集用于评估基于形状的图像检索性能.\u003c/strong\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch4 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eBruce-A[need help]\u003c/h4\u003e\u003ca id=\"user-content-bruce-aneed-help\" class=\"anchor\" aria-label=\"Permalink: Bruce-A[need help]\" href=\"#bruce-aneed-help\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e论文: \u003ca href=\"https://papers.nips.cc/paper/2830-saliency-based-on-information-maximization.pdf\" rel=\"nofollow\"\u003ehttps://papers.nips.cc/paper/2830-saliency-based-on-information-maximization.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch4 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eJudd-A[need help]\u003c/h4\u003e\u003ca id=\"user-content-judd-aneed-help\" class=\"anchor\" aria-label=\"Permalink: Judd-A[need help]\" href=\"#judd-aneed-help\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e论文: \u003ca href=\"http://people.csail.mit.edu/torralba/publications/wherepeoplelook.pdf\" rel=\"nofollow\"\u003ehttp://people.csail.mit.edu/torralba/publications/wherepeoplelook.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch4 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003ePASCAL-S\u003c/h4\u003e\u003ca id=\"user-content-pascal-s\" class=\"anchor\" aria-label=\"Permalink: PASCAL-S\" href=\"#pascal-s\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"https://camo.githubusercontent.com/98830481bf1fc0d5bfca896627d7b09ae46f09de45bfadcfe78d68eb2a1d9b72/68747470733a2f2f6363766c2e6a68752e6564752f64617461736574732f6173736574732f70617363616c5f73616c69656e745f6f626a6563742e6a7067\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/98830481bf1fc0d5bfca896627d7b09ae46f09de45bfadcfe78d68eb2a1d9b72/68747470733a2f2f6363766c2e6a68752e6564752f64617461736574732f6173736574732f70617363616c5f73616c69656e745f6f626a6563742e6a7067\" alt=\"img\" data-canonical-src=\"https://ccvl.jhu.edu/datasets/assets/pascal_salient_object.jpg\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e项目:\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e\u003ca href=\"https://ccvl.jhu.edu/datasets/\" rel=\"nofollow\"\u003ehttps://ccvl.jhu.edu/datasets/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://www.cbi.gatech.edu/salobj/\" rel=\"nofollow\"\u003ehttp://www.cbi.gatech.edu/salobj/\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e下载:\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e百度云盘: \u003ca href=\"https://pan.baidu.com/s/1DZcfwCYdeMW4EGawhXQyig\" rel=\"nofollow\"\u003ehttps://pan.baidu.com/s/1DZcfwCYdeMW4EGawhXQyig\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e页面: \u003ca href=\"http://academictorrents.com/details/6c49defd6f0e417c039637475cde638d1363037e\" rel=\"nofollow\"\u003ehttp://academictorrents.com/details/6c49defd6f0e417c039637475cde638d1363037e\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp dir=\"auto\"\u003e对来自PASCAL VOC的850张图像子集的自由修复. 收集8个主题, 3s观看时间, Eyelink II眼动仪. 大多数算法的性能表明PASCAL-S比大多数显着性数据集偏差更小.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e💔 由于其标注的真值有多个值, 常见的做法是使用 \u003ccode\u003e255/2\u003c/code\u003e 值作为阈值进行处理后, 再使用该数据集\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch4 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eUCSB[need help]\u003c/h4\u003e\u003ca id=\"user-content-ucsbneed-help\" class=\"anchor\" aria-label=\"Permalink: UCSB[need help]\" href=\"#ucsbneed-help\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e论文: \u003ca href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3954044/\" rel=\"nofollow\"\u003ehttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC3954044/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e下载: \u003ca href=\"https://labs.psych.ucsb.edu/eckstein/miguel/research_pages/saliencydata.html\" rel=\"nofollow\"\u003ehttps://labs.psych.ucsb.edu/eckstein/miguel/research_pages/saliencydata.html\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch4 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eOSIE[need help]\u003c/h4\u003e\u003ca id=\"user-content-osieneed-help\" class=\"anchor\" aria-label=\"Permalink: OSIE[need help]\" href=\"#osieneed-help\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e论文: \u003ca href=\"https://jov.arvojournals.org/article.aspx?articleid=2193943\" rel=\"nofollow\"\u003ehttps://jov.arvojournals.org/article.aspx?articleid=2193943\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp dir=\"auto\"\u003e大量先前的模型用于预测人们在自然场景中的外观, 侧重于像素级图像属性. 为了弥合计算显着性模型的预测能力与人类行为之间的语义差距, 我们提出了一种新的显着性体系结构, 它将信息分为三个层次: 像素级图像属性, 对象级属性和语义级属性. 通常忽略对象和语义级别信息, 或者仅讨论少数样本对象类别, 其中缩放到大量对象类别是不可行的, 也不是神经合理的. 为了解决这个问题, 这项工作构建了一个基本属性的原则词汇表来描述对象和语义级信息, 从而不限制有限数量的对象类别. 我们\u003cstrong\u003e建立了一个包含500个图像的新数据集, 其中包含15个观察者的眼动追踪数据和5, 551个具有精细轮廓和12个语义属性的分段对象的注释数据\u003c/strong\u003e(可在论文中公开获得). 实验结果证明了对象和语义级信息在预测视觉注意力方面的重要性.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch4 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eACSD\u003c/h4\u003e\u003ca id=\"user-content-acsd\" class=\"anchor\" aria-label=\"Permalink: ACSD\" href=\"#acsd\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/1546135560011.png\"\u003e\u003cimg src=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/1546135560011.png\" alt=\"1546135560011\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e论文: \u003ca href=\"https://infoscience.epfl.ch/record/135217/files/1708.pdf\" rel=\"nofollow\"\u003eR. Achanta, S. Hemami, F. Estrada, and S. Ssstrunk, \"Frequency-tuned salient region detection, \" in CVPR, 2009, pp.1597–1604\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e项目: \u003ca href=\"https://ivrl.epfl.ch/research-2/research-current/research-saliency/supplementary_material-rk_cvpr09-index-html/\" rel=\"nofollow\"\u003ehttps://ivrl.epfl.ch/research-2/research-current/research-saliency/supplementary_material-rk_cvpr09-index-html/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e下载: 官网只提供了\u003ca href=\"https://ivrl.epfl.ch/wp-content/uploads/2018/08/binarymasks.zip\" rel=\"nofollow\"\u003e真值标注的下载\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp dir=\"auto\"\u003e基于[ASD数据集(MSRA1K)]制作.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp dir=\"auto\"\u003e我们从[Z. Wang and B. Li. A two-stage approach to saliency detection in images. ICASSP 2008.]中提出的1000个图像中获得了一个真实数据库.[Z. Wang and B. Li. A two-stage approach to saliency detection in images. ICASSP 2008.]中的基本事实是在显着区域周围的用户绘制的矩形. 这是不准确的, 并将多个对象合二为一. 我们手动分割用户绘制的矩形内的显着对象以获得二进制掩码, 如下所示. 这样的掩膜既准确又允许我们清楚地处理多个显着对象.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eOther Special SOD Datasets\u003c/h3\u003e\u003ca id=\"user-content-other-special-sod-datasets\" class=\"anchor\" aria-label=\"Permalink: Other Special SOD Datasets\" href=\"#other-special-sod-datasets\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch4 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eXPIE\u003c/h4\u003e\u003ca id=\"user-content-xpie\" class=\"anchor\" aria-label=\"Permalink: XPIE\" href=\"#xpie\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/1546137404871.png\"\u003e\u003cimg src=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/1546137404871.png\" alt=\"1546137404871\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e链接: \u003ca href=\"https://www.researchgate.net/publication/320971838_What_is_and_What_is_Not_a_Salient_Object_Learning_Salient_Object_Detector_by_Ensembling_Linear_Exemplar_Regressors\" rel=\"nofollow\"\u003eC. Xia, J. Li, X. Chen, A. Zheng, and Y. Zhang, \"What  is  and  what is not a salient object? Learning salient object detector by ensembling linear exemplar regressors, \" in CVPR , 2017, pp.4142–4150.\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e团队: cvteam: \u003ca href=\"http://cvteam.net/\" rel=\"nofollow\"\u003ehttp://cvteam.net/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e项目:\u003ca href=\"http://cvteam.net/projects/CVPR17-ELE/ELE.html\" rel=\"nofollow\"\u003ehttp://cvteam.net/projects/CVPR17-ELE/ELE.html\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e下载: \u003ca href=\"http://cvteam.net/projects/CVPR17-ELE/XPIE.tar.gz\" rel=\"nofollow\"\u003ehttp://cvteam.net/projects/CVPR17-ELE/XPIE.tar.gz\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp dir=\"auto\"\u003e找出什么是什么和什么不是显着对象可以有助于在显着对象检测(SOD)中开发更好的特征和模型. 在本文中, 我们研究了在构建新的SOD数据集时选择和丢弃的图像, 发现许多相似的候选者, 复杂形状和低对象性是很多非显着对象的三个主要属性. 此外, 对象可能具有使其显着的不同属性.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e为了全面解释什么是什么和什么不是显着对象, 一个可行的解决方案是通过观察包含在数据集中或从数据集中丢弃的图像中的对象的主要特征来研究构建新SOD数据集的整个过程. 从这些观察中, 我们可以推断显着和非显着对象的关键属性以及基于图像的SOD数据集中可能存在的主观偏差. 为此, 我们构建了一个大的SOD数据集(称为XPIE)并记录构建过程中的所有细节.\u003c/p\u003e\n\u003col dir=\"auto\"\u003e\n\u003cli\u003e我们首先从三个来源收集三种图像, 包括Panoramio, ImageNet和两个fixation数据集. 这些操作是全自动的, 以避免引入太多的主观偏见.\u003c/li\u003e\n\u003cli\u003e之后, 我们调整每个图像的大小, 使其最大边长为300像素, 并丢弃所有最小边长小于128像素的灰度或彩色图像.\u003c/li\u003e\n\u003cli\u003e最后, 我们在三个图像子集中获得29, 600个彩色图像. 分别表示为Set-P, Set-I, Set-E.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eSet-P 包含8, 800具有地理信息的感兴趣地点的图像(例如, GPS和标签), 具有对象标签的Set-I包含19, 600图像, 以及Set-E包含1, 200个human fixations图像\u003c/strong\u003e.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e对于这些图像, 我们要求两位工程师通过两个阶段对其进行注释. 在第一阶段, 图像被分配一个二进制标记:'是'用于包含非明确对象, 否则为'否'. 在第一阶段之后, 我们将21, 002张图片标记为\"是\", 并且8, 598图像标记为\"否\". 在第二阶段, 这两位工程师进一步要求手动标记标记为\"是\"的10, 000张图像中的显着对象的准确边界. 注意我们有10名志愿者参与整个过程, 以检查注释的质量.\u003cstrong\u003e最后, 我们获得了10, 000张图像的二进制掩码\u003c/strong\u003e.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e可见论文内容第2节.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch4 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eSOC\u003c/h4\u003e\u003ca id=\"user-content-soc\" class=\"anchor\" aria-label=\"Permalink: SOC\" href=\"#soc\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/1546081178458.png\"\u003e\u003cimg src=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/1546081178458.png\" alt=\"1546081178458\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/1546081446332.png\"\u003e\u003cimg src=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/1546081446332.png\" alt=\"1546081446332\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e项目: \u003ca href=\"http://dpfan.net/SOCBenchmark/\" rel=\"nofollow\"\u003ehttp://dpfan.net/SOCBenchmark/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e论文:\u003ca href=\"http://dpfan.net/wp-content/uploads/2018/04/SOCBenchmark.pdf\" rel=\"nofollow\"\u003eSalient Objects in Clutter: Bringing Salient Object Detection to the Foreground\u003c/a\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e中文: \u003ca href=\"http://dpfan.net/wp-content/uploads/SOCBenchmarkCN.pdf\" rel=\"nofollow\"\u003ehttp://dpfan.net/wp-content/uploads/SOCBenchmarkCN.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e下载:\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003eOverall 6K SOC Dataset  (730.2MB)  \u003ca href=\"https://pan.baidu.com/s/1J8_CF7zE1zApqgAR9eS1Dw\" rel=\"nofollow\"\u003eBaidu\u003c/a\u003e\u003ca href=\"https://drive.google.com/file/d/1hfo33A7diED2dikTpN9o4KnZTxizGdLr/view?usp=sharing\" rel=\"nofollow\"\u003eGoogle\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e3.6K SOC Training Set (441.32MB) \u003ca href=\"http://dpfan.net/wp-content/uploads/TrainSet.zip\" rel=\"nofollow\"\u003eHere\u003c/a\u003e\u003ca href=\"https://pan.baidu.com/s/1Mao0piUuqVXAzmJoNtrtAw\" rel=\"nofollow\"\u003eBaidu\u003c/a\u003e\u003ca href=\"https://drive.google.com/open?id=16jlzeJJ1tawyBLBN5fRiWbh2y_F0iSyP\" rel=\"nofollow\"\u003eGoogle\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e1.2K  SOC Validation Set (146.56MB) \u003ca href=\"http://dpfan.net/wp-content/uploads/ValSet.zip\" rel=\"nofollow\"\u003eHere\u003c/a\u003e\u003ca href=\"https://pan.baidu.com/s/1mOmiezCpkr5NCQk8ecvGiQ\" rel=\"nofollow\"\u003eBaidu\u003c/a\u003e\u003ca href=\"https://drive.google.com/open?id=1vAfP8fCAo2a2KwgsmYLn8r8Rk4Lk7Urr\" rel=\"nofollow\"\u003eGoogle\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e1.2K  SOC Test Set (141.86MB) \u003ca href=\"http://dpfan.net/wp-content/uploads/TestSet.zip\" rel=\"nofollow\"\u003eHere\u003c/a\u003e\u003ca href=\"https://pan.baidu.com/s/10y-dx9HCPQm9fnp-Brswgw\" rel=\"nofollow\"\u003eBaidu\u003c/a\u003e\u003ca href=\"https://drive.google.com/open?id=1ZdKrsk-S4J6KQyjx-cPeL0HoKXy7CCxG\" rel=\"nofollow\"\u003eGoogle\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp dir=\"auto\"\u003e在本文中, 我们提供了显着对象检测(SOD)模型的综合评估. 我们的分析确定了现有SOD数据集的严重设计偏差, 假设每个图像在低杂波中包含至少一个明显突出的显着对象. 这是一个不切实际的假设. 在现有数据集上进行评估时, 设计偏差导致了最先进的SOD模型的饱和高性能. 然而, 当应用于现实世界的日常场景时, 这些模型仍然远远不能令人满意. 根据我们的分析, 我们首先确定了全面和平衡的数据集应该实现的7个关键方面. 然后, 我们提出一个新的高质量数据集并更新以前的显着性基准.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e具体来说, 我们的数据集称为SOC, Salient Objects in Clutter, \u003cstrong\u003e包括来自日常对象类别的显着和非显着对象的图像\u003c/strong\u003e. 除了对象类别注释之外, 每个突出图像都伴随着反映现实世界场景中常见挑战的属性(例如, 外观变化, 杂乱), 并且可以帮助 1)更深入地了解SOD问题, 2)调查专业人员和SOD模型的缺点, 3)从不同的角度客观地评估模型. 最后, 我们在SOC数据集上报告基于属性的性能评估. 我们相信, 我们的数据集和结果将为未来的显着物体检测研究开辟新的方向.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eSOC has 6, 000 images with 80 common categories. Half of the images contain salient objects and the others contain none.\u003cstrong\u003eEach salient-object-contained image is annotated with instance-level SOD ground-truth, object category (e.g., dog, book), and challenging factors\u003c/strong\u003e (e.g., big/small object).\u003cstrong\u003eThe non-salient object subset has 783 texture images and 2, 217 real-scene images\u003c/strong\u003e (e.g., aurora, sky).\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch4 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eSOS/MOS[need some images]\u003c/h4\u003e\u003ca id=\"user-content-sosmosneed-some-images\" class=\"anchor\" aria-label=\"Permalink: SOS/MOS[need some images]\" href=\"#sosmosneed-some-images\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e项目:\u003ca href=\"http://cs-people.bu.edu/jmzhang/sos.html\" rel=\"nofollow\"\u003ehttp://cs-people.bu.edu/jmzhang/sos.html\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e论文:\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003eSOS: J. Zhang, S. Ma, M. Sameki, S. Sclaroff, M. Betke, Z. Lin, X. Shen, B. Price, and R. Mech, \"Salient object subitizing, \" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2015, pp.4045–4054.\u003c/li\u003e\n\u003cli\u003eMOS: J. Zhang, S. Ma, M. Sameki, S. Sclaroff, M. Betke, Z. Lin, X. Shen, B. Price, and R. Mech, \"Salient object subitizing, \" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2015, pp.4045–4054.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp dir=\"auto\"\u003eSOS 10 is created for SOD subitizing [115], i.e., to predict the number of salient objects without an expensive detection process. It contains 6, 900 images selected from:\u003c/p\u003e\n\u003col dir=\"auto\"\u003e\n\u003cli\u003eA large-scale hierarchical image database\u003c/li\u003e\n\u003cli\u003eSun database: Large-scale scene recognition from abbey to zoo\u003c/li\u003e\n\u003cli\u003eMicrosoft coco: Common objects in context\u003c/li\u003e\n\u003cli\u003eThe pascal visual object classes (voc) challenge results\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp dir=\"auto\"\u003eEach image is labeled as containing 0, 1, 2, 3 or 4+ salient objects. SOS is randomly split into a training (5, 520 images) and a test set (1, 380 images).\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eMSO is a subset of the test set of SOS and contains 1, 224 images\u003c/strong\u003e. It has a more balanced distribution regarding the number of salient objects, and each object is annotated with a bounding box.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch4 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eILSO[need some images]\u003c/h4\u003e\u003ca id=\"user-content-ilsoneed-some-images\" class=\"anchor\" aria-label=\"Permalink: ILSO[need some images]\" href=\"#ilsoneed-some-images\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e项目:\u003ca href=\"http://www.sysu-hcp.net/instance-level-salient-object-segmentation/\" rel=\"nofollow\"\u003ehttp://www.sysu-hcp.net/instance-level-salient-object-segmentation/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e论文: G. Li, Y. Xie, L. Lin, and Y. Yu, \"Instance-level salient object segmentation, \" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2017, pp.247–256.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp dir=\"auto\"\u003eILSO has 1, 000 images with pixel-wise instancelevel saliency annotations and coarse contour labeling, where the benchmark results are generated using MSRNet [Instance-level salient object segmentation]. Most of the images in ILSO are selected from the following datasets to reduce ambiguity over the salient object regions.\u003c/p\u003e\n\u003col dir=\"auto\"\u003e\n\u003cli\u003eVisual saliency based on multiscale deep features\u003c/li\u003e\n\u003cli\u003eHierarchical saliency detection\u003c/li\u003e\n\u003cli\u003eSaliency detection via graph-based manifold ranking\u003c/li\u003e\n\u003cli\u003eSalient object subitizing\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/blockquote\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch4 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eHS-SOD\u003c/h4\u003e\u003ca id=\"user-content-hs-sod\" class=\"anchor\" aria-label=\"Permalink: HS-SOD\" href=\"#hs-sod\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/gistairc/HS-SOD/raw/master/images/poster-QoMEX2018.png\"\u003e\u003cimg src=\"https://github.com/gistairc/HS-SOD/raw/master/images/poster-QoMEX2018.png\" alt=\"img\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/2018-12-28-22-16-20.png\"\u003e\u003cimg src=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/2018-12-28-22-16-20.png\" alt=\"eva\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e项目: \u003ca href=\"https://github.com/gistairc/HS-SOD\"\u003ehttps://github.com/gistairc/HS-SOD\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e下载: \u003ca href=\"http://data.airc.aist.go.jp/HS-SOD/HS-SOD.zip\" rel=\"nofollow\"\u003ehttp://data.airc.aist.go.jp/HS-SOD/HS-SOD.zip\u003c/a\u003e 5.6G\u003c/li\u003e\n\u003cli\u003e论文: \u003ca href=\"https://arxiv.org/abs/1806.11314\" rel=\"nofollow\"\u003eHyperspectral Image Dataset for Benchmarking on Salient Object Detection\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp dir=\"auto\"\u003e使用有监督或无监督的方法对着色对象进行了显着的物体检测. 最近, 一些研究表明, 通过在来自天然景观的高光谱图像的可见光谱中使用光谱特征, 也可以实现有效的显着对象检测. 然而, 这些关于高光谱显着物体检测的模型使用从各种在线公共数据集中选择的极少数数据进行测试, 这些数据不是为了物体检测目的而特别创建的. 因此, 在这里, 我们的目标是通过发布\u003cstrong\u003e具有60个高光谱图像的集合的高光谱显着物体检测数据集\u003c/strong\u003e以及\u003cstrong\u003e相应的地面实况二值图像\u003c/strong\u003e和**代表性的彩色图像(sRGB)**来指导该领域. 我们在数据收集过程中考虑了几个方面, 例如对象大小的变化, 对象的数量, 前景-背景对比度, 图像上的对象位置等. 然后, 我们为每个高光谱数据准备了真值二进制图像, 其中显著性目标被标记为图像. 最后, 我们使用曲线下面积(AUC)度量对文献中一些现有的高光谱显着性检测模型进行了性能评估.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e这些数据是在东京港口码头公司的许可下, 在日本东京台场的东京海滨城市公园收集的. 我们在2017年8月至9月期间的几天内收集了数据, 当时天气晴朗或部分多云. 在每个数据收集日, 使用三脚架固定相机以最小化图像上的运动失真. 我们尝试根据日光条件尽可能地保持相机设置的曝光时间和增益, 同时保持像素值饱和度或图像可见性. 作为数据集用户的参考, 我们提供相机设置, 例如文本文件中每个图像的曝光时间和增益值以及相应的数据. 我们也没有对捕获的波段应用标准化. 它可以提高前景和背景区域之间色彩对比度更高的高光谱图像的质量; 但是, 它也可能降低数据集在显着对象检测任务上进行基准测试的难度.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e在获得各种高光谱图像后, 我们从大约50个不同的场景中选择了60个图像, 条件是:i)我们去除了由于场景中的运动引起的失真图像(取决于曝光时间, 一个图像可能需要几秒钟才能用于相机), ii)我们考虑了几个方面, 如显着物体大小的变化, 图像上物体的空间位置, 显着物体的数量, 前景 * 背景对比度, iii)一些图像具有相同的场景但物体位置, 物距或数量对象各不相同.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e为了便于显着物体检测任务, 我们在可见光谱周围裁剪光谱带, 并在传感器暗噪声校正后以\".mat\"文件格式保存每个场景的超立方体. 如[21]中所定义, 可见光谱具有380-780nm的良好可接受范围, 但也可以使用[3, 4]中的400-700nm范围. 为了保持范围广泛和灵活性, 想要使用数据集的人, 我们在[21]中为我们的数据集选择了380 * 780 nm的定义范围, 尽管在人类视觉系统的这些范围的边界处视觉刺激可能较弱. 然后, 我们使用高光谱图像渲染sRGB彩色图像, 通过标记显着对象的边界来创建地面真实显着对象二进制图像.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp dir=\"auto\"\u003eHS-SOD.zip file contains three folders:\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e1.hyperspectral: containing 60 hyperspectral images with #spatial rows:768 #spatial columns:1024 #spectral channels:81 (data only within visible spectrum: 380 nm -720 nm)\n2.color: 60 color images of hyperspectral dataset rendered in sRGB for visualization\n3.ground-truth: 60 ground-truth binary images for salient objects\u003c/p\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eVideo Saliency Detection\u003c/h3\u003e\u003ca id=\"user-content-video-saliency-detection\" class=\"anchor\" aria-label=\"Permalink: Video Saliency Detection\" href=\"#video-saliency-detection\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch4 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eRSD(PKU-RSD)\u003c/h4\u003e\u003ca id=\"user-content-rsdpku-rsd\" class=\"anchor\" aria-label=\"Permalink: RSD(PKU-RSD)\" href=\"#rsdpku-rsd\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"https://camo.githubusercontent.com/bad3b4eb081112c2e37582e1ab22e3e7a3281b61724a455b6dd615a93a79bf21/68747470733a2f2f706b756d6c2e6f72672f77702d636f6e74656e742f75706c6f6164732f323031342f31322f73616d706c65732d6f662d5253442d31303234783237312e706e67\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/bad3b4eb081112c2e37582e1ab22e3e7a3281b61724a455b6dd615a93a79bf21/68747470733a2f2f706b756d6c2e6f72672f77702d636f6e74656e742f75706c6f6164732f323031342f31322f73616d706c65732d6f662d5253442d31303234783237312e706e67\" alt=\"samples of RSD\" data-canonical-src=\"https://pkuml.org/wp-content/uploads/2014/12/samples-of-RSD-1024x271.png\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e论文: \u003ca href=\"https://ieeexplore.ieee.org/document/5202529\" rel=\"nofollow\"\u003eJ. Li, Y. Tian, T. Huang, and W. Gao, \"A dataset and evaluation methodology for visual saliency in video, \" in IEEE ICME, 2009, pp.442–445\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e项目: \u003ca href=\"https://pkuml.org/resources/dataset.html\" rel=\"nofollow\"\u003ehttps://pkuml.org/resources/dataset.html\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e下载: \u003ca href=\"https://pkuml.org/resources/pku-rsd.html\" rel=\"nofollow\"\u003ehttps://pkuml.org/resources/pku-rsd.html\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp dir=\"auto\"\u003e我们构建了这个PKU-RSD(区域显着性数据集)数据集, 可以捕获时空视觉显着性, 用于评估不同的视频显着性模型. 该数据集包含431个短视频, 其涵盖各种场景(监视, 广告, 新闻, 卡通, 电影等)以及由23个主题手动标记的采样关键帧中的显着对象的相应注释结果.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch4 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eSTC[need help]\u003c/h4\u003e\u003ca id=\"user-content-stcneed-help\" class=\"anchor\" aria-label=\"Permalink: STC[need help]\" href=\"#stcneed-help\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e论文: \u003ca href=\"https://pdfs.semanticscholar.org/3347/c330ac5586020ebea60823b1fd4e8d68e936.pdf?_ga=2.181072804.269179473.1546092428-61549168.1544104573\" rel=\"nofollow\"\u003eY. Wu, N. Zheng, Z. Yuan, H. Jiang, and T. Liu, \"Detection of salient objects with focused attention based on spatial and temporal coherence, \" Chinese Science Bulletin, vol.56, pp.1055–1062, 2011.\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e下载: This dataset is freely available from the author\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp dir=\"auto\"\u003e对视频内容的理解和分析对于众多应用程序来说至关重要, 包括视频摘要, 检索, 导航和编辑. 此过程的一个重要部分是检测视频片段中的显着(通常意味着重要和有趣)对象. 与现有方法不同, 我们提出了一种将显着性测量与空间和时间相干性相结合的方法. 空间和时间一致性的整合受到人类视觉中关注焦点的启发. 在所提出的方法中, 低级视觉分组线索的空间相干性(例如外观和运动)有助于每帧对象背景分离, 而对象属性的时间一致性(例如形状和外观)确保一致物体随时间定位, 因此该方法对于意外的环境变化和相机振动是鲁棒的. 在\u003cstrong\u003e开发了基于粗到细多尺度动态规划的有效优化策略之后, 我们使用可与本文一起免费获得的具有挑战性的数据集来评估我们的方法\u003c/strong\u003e. 我们展示了两种类型的一致性的有效性和互补性, 并证明它们可以显着提高视频中显着对象检测的性能.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eRGBD-Saliency Detection\u003c/h3\u003e\u003ca id=\"user-content-rgbd-saliency-detection\" class=\"anchor\" aria-label=\"Permalink: RGBD-Saliency Detection\" href=\"#rgbd-saliency-detection\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cblockquote\u003e\n\u003cp dir=\"auto\"\u003e致谢:\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e@JXingZhao, 在他的工作中整理并公开了多个数据集: \u003ca href=\"https://github.com/JXingZhao/ContrastPrior\"\u003ehttps://github.com/JXingZhao/ContrastPrior\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e@jiwei0921, 在他的工作中整理并公开了多个数据集: \u003ca href=\"https://github.com/jiwei0921/RGBD-SOD-datasets\"\u003ehttps://github.com/jiwei0921/RGBD-SOD-datasets\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e更全面的内容\u003c/strong\u003e可见 \u003ca href=\"http://dpfan.net/d3netbenchmark/\" rel=\"nofollow\"\u003ehttp://dpfan.net/d3netbenchmark/\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/blockquote\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch4 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eSIP\u003c/h4\u003e\u003ca id=\"user-content-sip\" class=\"anchor\" aria-label=\"Permalink: SIP\" href=\"#sip\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/2019-09-15-16-26-26.png\"\u003e\u003cimg src=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/2019-09-15-16-26-26.png\" alt=\"\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e论文: Rethinking RGB-D Salient Object Detection: Models, Datasets, and Large-Scale Benchmarks:\u003ca href=\"https://arxiv.org/pdf/1907.06781.pdf\" rel=\"nofollow\"\u003ehttps://arxiv.org/pdf/1907.06781.pdf\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e项目: \u003ca href=\"http://dpfan.net/d3netbenchmark/\" rel=\"nofollow\"\u003ehttp://dpfan.net/d3netbenchmark/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e下载: 请见项目主页\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp dir=\"auto\"\u003ewe carefully collect a new salient person (SIP) dataset, which consists of 1K high-resolution images that cover diverse real-world scenes from various viewpoints, poses, occlusion, illumination, and background.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch4 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eNLPR/RGBD1000\u003c/h4\u003e\u003ca id=\"user-content-nlprrgbd1000\" class=\"anchor\" aria-label=\"Permalink: NLPR/RGBD1000\" href=\"#nlprrgbd1000\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/1546138815074.png\"\u003e\u003cimg src=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/1546138815074.png\" alt=\"1546138815074\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e论文: \u003ca href=\"https://docs.google.com/uc?authuser=0\u0026amp;id=0B1wzzt1_uP1rb250d0t6dVFXWG8\u0026amp;export=download\" rel=\"nofollow\"\u003eRgbd salient object detection: a benchmark and algorithms\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e项目: \u003ca href=\"https://sites.google.com/site/rgbdsaliency/home\" rel=\"nofollow\"\u003ehttps://sites.google.com/site/rgbdsaliency/home\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e下载: \u003ca href=\"https://sites.google.com/site/rgbdsaliency/dataset\" rel=\"nofollow\"\u003ehttps://sites.google.com/site/rgbdsaliency/dataset\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp dir=\"auto\"\u003eNLPR is also called RGBD1000 dataset which including 1, 000 images. There may exist multiple salient objects in each image. The structured light depth images are obtained by the Microsoft Kinect under different illumination conditions.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e虽然深度信息在人类视觉系统中起着重要作用, 但在现有的视觉显着性计算模型中尚未得到很好的探索. 在这项工作中, \u003cstrong\u003e我们首先引入了一个大规模的RGBD图像数据集, 以解决目前RGBD显着目标检测研究中数据不足的问题\u003c/strong\u003e. 为了确保大多数现有的RGB显着模型在RGBD场景中仍然足够, 我们继续提供一个简单的融合框架, 将现有的RGB产生的显着性与新的深度诱导显着性相结合, 前者是从现有的RGB模型中估算的, 而前者是后者基于提出的多上下文对比模型. 此外, 还提出了一种专门的多阶段RGBD模型, 其考虑了来自低级特征对比度, 中级区域分组和高级先验增强的深度和外观线索. 大量实验表明, 我们的模型能够准确定位RGBD图像中的显着对象, 并为目标对象分配一致的显着性值.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch4 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eNJU400/2000\u003c/h4\u003e\u003ca id=\"user-content-nju4002000\" class=\"anchor\" aria-label=\"Permalink: NJU400/2000\" href=\"#nju4002000\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/1546139249376.png\"\u003e\u003cimg src=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/1546139249376.png\" alt=\"1546139249376\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e论文:\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e\u003ca href=\"http://mcg.nju.edu.cn/publication/2014/icip14-jur.pdf\" rel=\"nofollow\"\u003eNJU400: Depth saliency based on anisotropic center-surround difference\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://mcg.nju.edu.cn/publication/2015/spic15-jur.pdf\" rel=\"nofollow\"\u003eNJU2000: Depth-aware salient object detection using anisotropic center-surround difference\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e团队: \u003ca href=\"http://mcg.nju.edu.cn/index.html\" rel=\"nofollow\"\u003eMGG\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e项目: \u003ca href=\"http://mcg.nju.edu.cn/publication/2014/icip14-jur/index.html\" rel=\"nofollow\"\u003ehttp://mcg.nju.edu.cn/publication/2014/icip14-jur/index.html\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e下载:\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e\u003ca href=\"http://mcg.nju.edu.cn/resource.html\" rel=\"nofollow\"\u003ehttp://mcg.nju.edu.cn/resource.html\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://mcg.nju.edu.cn/dataset/nju400.zip\" rel=\"nofollow\"\u003ehttp://mcg.nju.edu.cn/dataset/nju400.zip\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://mcg.nju.edu.cn/dataset/nju2000.zip\" rel=\"nofollow\"\u003ehttp://mcg.nju.edu.cn/dataset/nju2000.zip\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp dir=\"auto\"\u003eNJU2000 contains 2003 stereo image pairs with diverse objects and complex, challenging scenarios, along with ground-truth map. The stereo images are gathered from 3D movies, the Internet, and photographs taken by a Fuji W3 stereo camera.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch4 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eSTEREO/SSB\u003c/h4\u003e\u003ca id=\"user-content-stereossb\" class=\"anchor\" aria-label=\"Permalink: STEREO/SSB\" href=\"#stereossb\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/2019-05-13-19-48-20.png\"\u003e\u003cimg src=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/2019-05-13-19-48-20.png\" alt=\"\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e论文: \u003ca href=\"http://web.cecs.pdx.edu/~fliu/papers/cvpr2012.pdf\" rel=\"nofollow\"\u003eLeveraging stereopsis for saliency analysis\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e项目: \u003ca href=\"http://web.cecs.pdx.edu/~fliu/\" rel=\"nofollow\"\u003ehttp://web.cecs.pdx.edu/~fliu/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e下载: 请到主页寻找, 需要联系作者.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp dir=\"auto\"\u003eSSB is also called STEREO dataset, which consists of 1000 pairs of binocular images.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch4 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eLFSD[nead img]\u003c/h4\u003e\u003ca id=\"user-content-lfsdnead-img\" class=\"anchor\" aria-label=\"Permalink: LFSD[nead img]\" href=\"#lfsdnead-img\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e论文: Saliency detection on light field\u003c/li\u003e\n\u003cli\u003e项目: \u003ca href=\"https://sites.duke.edu/nianyi/publication/saliency-detection-on-light-field/\" rel=\"nofollow\"\u003ehttps://sites.duke.edu/nianyi/publication/saliency-detection-on-light-field/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e下载: 请到主页寻找, 需要联系作者.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp dir=\"auto\"\u003eLFSD is a small dataset which contains 100 images with depth information and human labeled ground truths. The depth information was obtained via the Lytro light field camera.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch4 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eRGBD135/DES\u003c/h4\u003e\u003ca id=\"user-content-rgbd135des\" class=\"anchor\" aria-label=\"Permalink: RGBD135/DES\" href=\"#rgbd135des\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/2019-05-23-10-44-38.png\"\u003e\u003cimg src=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/2019-05-23-10-44-38.png\" alt=\"image\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/2019-05-23-10-44-15.png\"\u003e\u003cimg src=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/2019-05-23-10-44-15.png\" alt=\"depth\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/2019-05-23-10-44-59.png\"\u003e\u003cimg src=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/2019-05-23-10-44-59.png\" alt=\"mask\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e论文: \u003ca href=\"http://delivery.acm.org/10.1145/2640000/2632866/p23-cheng.pdf?ip=202.118.97.210\u0026amp;id=2632866\u0026amp;acc=ACTIVE%20SERVICE\u0026amp;key=BF85BBA5741FDC6E%2E5FC7500D8F9CB386%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35\u0026amp;__acm__=1557798709_a26e3faff3faccad6d62e02d79d1921a\" rel=\"nofollow\"\u003eDepth enhanced saliency detection method\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e项目: \u003ca href=\"https://github.com/HzFu/DES_code\"\u003ehttps://github.com/HzFu/DES_code\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e下载: 项目主页提供了下面的下载链接诶:\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e\u003ca href=\"https://onedrive.live.com/redir?resid=F3A8A31ABFAC51B0!256\u0026amp;authkey=!AC4-yOEjn0bgrCQ\u0026amp;ithint=file%2crar\" rel=\"nofollow\"\u003ehttps://onedrive.live.com/redir?resid=F3A8A31ABFAC51B0!256\u0026amp;authkey=!AC4-yOEjn0bgrCQ\u0026amp;ithint=file%2crar\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://pan.baidu.com/s/1pLv2B8n\" rel=\"nofollow\"\u003ehttps://pan.baidu.com/s/1pLv2B8n\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp dir=\"auto\"\u003eRGBD135 is also named DES which consists of seven indoor scenes and contains 135 indoor images collected by Microsoft Kinect.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch4 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eDUT-RGBD\u003c/h4\u003e\u003ca id=\"user-content-dut-rgbd\" class=\"anchor\" aria-label=\"Permalink: DUT-RGBD\" href=\"#dut-rgbd\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e论文: Depth-induced Multi-scale Recurrent Attention Network for Saliency Detection\u003c/li\u003e\n\u003cli\u003e项目: \u003ca href=\"https://github.com/jiwei0921/DMRA_RGBD-SOD\"\u003ehttps://github.com/jiwei0921/DMRA_RGBD-SOD\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e下载: 请见@jiwei0921的RGBD-SOD-datasets仓库\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch4 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eSSD100\u003c/h4\u003e\u003ca id=\"user-content-ssd100\" class=\"anchor\" aria-label=\"Permalink: SSD100\" href=\"#ssd100\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/2019-09-15-16-17-12.png\"\u003e\u003cimg src=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/2019-09-15-16-17-12.png\" alt=\"\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e论文: A Three-pathway Psychobiological Framework of Salient Object Detection Using Stereoscopic Technology: \u003ca href=\"http://dpfan.net/wp-content/uploads/SSD_dataset_ICCVW17.pdf\" rel=\"nofollow\"\u003ehttp://dpfan.net/wp-content/uploads/SSD_dataset_ICCVW17.pdf\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e下载: 请见@jiwei0921的RGBD-SOD-datasets仓库\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp dir=\"auto\"\u003eOur SSD100 dataset is built on three stereo movies. The movies contain both the indoors and outdoors scenes. We pick up one stereo image pair at each hundred frames. It totally has tens of thousands of stereo image pairs. We make the image acquisition and image annotation independent to each other, we can avoid dataset design bias, namely a specific type of bias that is caused by experimenters unnatural selection of dataset images. The chosen stereo image pairs are based on one principle: choose the one which the computer detect the salient objects within the complex scenes where even the human cannot tell the salient objects at once. After picking up the stereo image pairs, we divide the image pairs into left images and right images both in 960x1080 size. When we build the ground truth of salient objects, we adhere to the following rules: 1) we mark the salient objects, taking the advice of most people; 2) disconnected regions of the same object are labeled separately; 3) we use solid regions to approximate hollow objects, such as bike wheels. Besides, we will expand this dataset continually in future.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eRGBT-Saliency Detection [need more information...]\u003c/h3\u003e\u003ca id=\"user-content-rgbt-saliency-detection-need-more-information\" class=\"anchor\" aria-label=\"Permalink: RGBT-Saliency Detection [need more information...]\" href=\"#rgbt-saliency-detection-need-more-information\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch4 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eVT1000 Dataset\u003c/h4\u003e\u003ca id=\"user-content-vt1000-dataset\" class=\"anchor\" aria-label=\"Permalink: VT1000 Dataset\" href=\"#vt1000-dataset\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/2019-05-23-10-49-47.png\"\u003e\u003cimg src=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/2019-05-23-10-49-47.png\" alt=\"image\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/2019-05-23-10-50-23.png\"\u003e\u003cimg src=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/2019-05-23-10-50-23.png\" alt=\"thermal\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/2019-05-23-10-50-02.png\"\u003e\u003cimg src=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/2019-05-23-10-50-02.png\" alt=\"mask\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e论文: RGB-T Image Saliency Detection via Collaborative Graph Learning\u003c/li\u003e\n\u003cli\u003e项目:\u003ca href=\"http://chenglongli.cn/people/lcl/dataset-code.html\" rel=\"nofollow\"\u003ehttp://chenglongli.cn/people/lcl/dataset-code.html\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e下载: 具体信息可见项目主页\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e\u003ca href=\"https://drive.google.com/file/d/1NCPFNeiy1n6uY74L0FDInN27p6N_VCSd/view?usp=sharing\" rel=\"nofollow\"\u003ehttps://drive.google.com/file/d/1NCPFNeiy1n6uY74L0FDInN27p6N_VCSd/view?usp=sharing\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://pan.baidu.com/s/1eGQJhvnKnqV1KJ1GY_63NA\" rel=\"nofollow\"\u003ehttps://pan.baidu.com/s/1eGQJhvnKnqV1KJ1GY_63NA\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch4 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eVT821 Dataset\u003c/h4\u003e\u003ca id=\"user-content-vt821-dataset\" class=\"anchor\" aria-label=\"Permalink: VT821 Dataset\" href=\"#vt821-dataset\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/2019-05-23-10-50-42.png\"\u003e\u003cimg src=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/2019-05-23-10-50-42.png\" alt=\"image\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/2019-05-23-10-51-00.png\"\u003e\u003cimg src=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/2019-05-23-10-51-00.png\" alt=\"mask\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e论文: A Unified RGB-T Saliency Detection Benchmark: Dataset, Baselines, Analysis and A Novel Approach\u003c/li\u003e\n\u003cli\u003e项目:\u003ca href=\"http://chenglongli.cn/people/lcl/dataset-code.html\" rel=\"nofollow\"\u003ehttp://chenglongli.cn/people/lcl/dataset-code.html\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e下载: 具体信息可见项目主页\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e\u003ca href=\"https://drive.google.com/file/d/0B4fH4G1f-jjNR3NtQUkwWjFFREk/view?usp=sharing\" rel=\"nofollow\"\u003ehttps://drive.google.com/file/d/0B4fH4G1f-jjNR3NtQUkwWjFFREk/view?usp=sharing\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://pan.baidu.com/s/1bpEaeQV\" rel=\"nofollow\"\u003ehttp://pan.baidu.com/s/1bpEaeQV\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eHigh-Resolution Saliency Detection\u003c/h3\u003e\u003ca id=\"user-content-high-resolution-saliency-detection\" class=\"anchor\" aria-label=\"Permalink: High-Resolution Saliency Detection\" href=\"#high-resolution-saliency-detection\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch4 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eHRSOD/DAVIS-S\u003c/h4\u003e\u003ca id=\"user-content-hrsoddavis-s\" class=\"anchor\" aria-label=\"Permalink: HRSOD/DAVIS-S\" href=\"#hrsoddavis-s\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/2019-09-15-15-54-12.png\"\u003e\u003cimg src=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/2019-09-15-15-54-12.png\" alt=\"\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e论文: Towards High-Resolution Salient Object Detection: \u003ca href=\"https://arxiv.org/pdf/1908.07274.pdf\" rel=\"nofollow\"\u003ehttps://arxiv.org/pdf/1908.07274.pdf\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e项目:\u003ca href=\"https://github.com/yi94code/HRSOD\"\u003ehttps://github.com/yi94code/HRSOD\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e下载: 可见项目主页\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003eHRSOD: \u003ca href=\"https://drive.google.com/open?id=1bmDGlkzqHoduNigi_GO4Qy9sA9sIaZcY\" rel=\"nofollow\"\u003ehttps://drive.google.com/open?id=1bmDGlkzqHoduNigi_GO4Qy9sA9sIaZcY\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eDAVIS-S: \u003ca href=\"https://drive.google.com/open?id=1q1H7yoITLS6i2n-PhgYMIxLdjyhge5AR\" rel=\"nofollow\"\u003ehttps://drive.google.com/open?id=1q1H7yoITLS6i2n-PhgYMIxLdjyhge5AR\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp dir=\"auto\"\u003e...we contribute a High-Resolution Salient Object Detection (HRSOD) dataset, containing 1610 training images and 400 test images. The total 2010 images are collected from the website of Flickr with the license of all creative commons. Pixel-level ground truths are manually annotated by 40 subjects. The shortest edge of each image in our HRSOD is more than 1200 pixels.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eOther Saliency Dataset\u003c/h3\u003e\u003ca id=\"user-content-other-saliency-dataset\" class=\"anchor\" aria-label=\"Permalink: Other Saliency Dataset\" href=\"#other-saliency-dataset\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch4 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eKAIST Salient Pedestrian Dataset\u003c/h4\u003e\u003ca id=\"user-content-kaist-salient-pedestrian-dataset\" class=\"anchor\" aria-label=\"Permalink: KAIST Salient Pedestrian Dataset\" href=\"#kaist-salient-pedestrian-dataset\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/2019-05-23-10-53-57.png\"\u003e\u003cimg src=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/2019-05-23-10-53-57.png\" alt=\"\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e论文: Pedestrian Detection from Thermal Images using Saliency Maps\u003c/li\u003e\n\u003cli\u003e项目:\u003ca href=\"https://github.com/Information-Fusion-Lab-Umass/Salient-Pedestrian-Detection\"\u003ehttps://github.com/Information-Fusion-Lab-Umass/Salient-Pedestrian-Detection\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e下载: 具体详见项目页面\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp dir=\"auto\"\u003eWe select 1702 images from the training set of the KAIST Multispectral Pedestrian dataset, by sampling every 15th image from all the images captured during the day and every 10thimage from all the images captured during the night, which contain pedestrians. These images were selected in order to have approximately the same number of images captured on both times of the day (913 day images and 789 night images), containing 4170 instances of pedestrians. We manually annotate these images using the VGG Image Annotator tool to generate the ground truth saliency masks based on the location of the bounding boxes on pedestrians in the original dataset. Additionally, we create a set of 362 images with similar annotations from the test set to validate our deep saliency detection networks, with 193 day images and 169 night images, containing 1029 instances of pedestrians.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eSegmentation\u003c/h2\u003e\u003ca id=\"user-content-segmentation\" class=\"anchor\" aria-label=\"Permalink: Segmentation\" href=\"#segmentation\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eGeneral[need help]\u003c/h3\u003e\u003ca id=\"user-content-generalneed-help\" class=\"anchor\" aria-label=\"Permalink: General[need help]\" href=\"#generalneed-help\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch4 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eDAVIS\u003c/h4\u003e\u003ca id=\"user-content-davis\" class=\"anchor\" aria-label=\"Permalink: DAVIS\" href=\"#davis\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/2019-03-13-11-01-47.png\"\u003e\u003cimg src=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/2019-03-13-11-01-47.png\" alt=\"img\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e项目:\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e竞赛主页: \u003ca href=\"https://davischallenge.org/index.html\" rel=\"nofollow\"\u003ehttps://davischallenge.org/index.html\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e论文: \u003ca href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Perazzi_A_Benchmark_Dataset_CVPR_2016_paper.pdf\" rel=\"nofollow\"\u003eA Benchmark Dataset and Evaluation Methodology for Video Object Segmentation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e下载:\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e\u003ca href=\"https://davischallenge.org/davis2016/code.html\" rel=\"nofollow\"\u003eDAVIS 2016\u003c/a\u003e In each video sequence a single instance is annotated.\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://davischallenge.org/davis2017/code.html\" rel=\"nofollow\"\u003eDAVIS 2017\u003c/a\u003e In each video sequence multiple instances are annotated.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch4 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eaNYU\u003c/h4\u003e\u003ca id=\"user-content-anyu\" class=\"anchor\" aria-label=\"Permalink: aNYU\" href=\"#anyu\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/1546153000959.png\"\u003e\u003cimg src=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/1546153000959.png\" alt=\"img\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e项目: \u003ca href=\"https://kylezheng.org/research-projects/densesegattobj/\" rel=\"nofollow\"\u003ehttps://kylezheng.org/research-projects/densesegattobj/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e论文: \u003ca href=\"http://kylezheng.org/densesegattobjdataset/denseseg4objatt_CVPR2014_Kyle.pdf\" rel=\"nofollow\"\u003eDense Semantic Image Segmentation with Objects and Attributes\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e下载:\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003eNYU: \u003ca href=\"http://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html\" rel=\"nofollow\"\u003ehttp://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eaNYU: \u003ca href=\"http://www.robots.ox.ac.uk/~szheng/aNYU/aNYU.tar.gz\" rel=\"nofollow\"\u003ehttp://www.robots.ox.ac.uk/~szheng/aNYU/aNYU.tar.gz\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp dir=\"auto\"\u003e我们的第一组实验是关于来自NYU V2 dataset的RGB图像. 如图3所示, 我们添加了8个附加属性标签, 即木制, 彩绘, 棉花, 玻璃, 光面, 塑料, 闪亮和纹理. 我们要求3个注释者在每个分割地面真实区域上分配材料, 表面属性属性. Wethen将3名工作者的多数票作为我们的8个附加属性标签. 我们将此扩展数据集称为attribute NYU(aNYU)数据集.\u003cstrong\u003e该数据集从28个不同的室内场景中收集了1449个图像.\u003c/strong\u003e 在我们的实验中, 我们选择了具有足够数量实例的15个对象类和8个属性来训练unary potential. 此外, \u003cstrong\u003e我们随机地将数据集分成训练集的725个图像, 验证集的100个, 以及测试集的624个.\u003c/strong\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eAbout Person\u003c/h3\u003e\u003ca id=\"user-content-about-person\" class=\"anchor\" aria-label=\"Permalink: About Person\" href=\"#about-person\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch4 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eSupervisely人像数据集\u003c/h4\u003e\u003ca id=\"user-content-supervisely人像数据集\" class=\"anchor\" aria-label=\"Permalink: Supervisely人像数据集\" href=\"#supervisely人像数据集\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"https://camo.githubusercontent.com/26157d92880274fae7580e4fe1b936d25710e5a6d650d3d235d7ee2260ad6d62/68747470733a2f2f7374617469632e6c656970686f6e652e636f6d2f75706c6f6164732f6e65772f61727469636c652f3734305f3734302f3230313830342f356163623137313961363235322e706e673f696d6167654d6f6772322f666f726d61742f6a70672f7175616c6974792f3930\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/26157d92880274fae7580e4fe1b936d25710e5a6d650d3d235d7ee2260ad6d62/68747470733a2f2f7374617469632e6c656970686f6e652e636f6d2f75706c6f6164732f6e65772f61727469636c652f3734305f3734302f3230313830342f356163623137313961363235322e706e673f696d6167654d6f6772322f666f726d61742f6a70672f7175616c6974792f3930\" alt=\"img\" data-canonical-src=\"https://static.leiphone.com/uploads/new/article/740_740/201804/5acb1719a6252.png?imageMogr2/format/jpg/quality/90\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e项目: \u003ca href=\"https://supervise.ly/\" rel=\"nofollow\"\u003ehttps://supervise.ly/\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e数据集 \u003cstrong\u003e由5711张图片组成, 有6884个高质量的标注的人体实例\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003e下面的所有步骤在Supervisely内部完成的, 没有任何编码.\u003c/li\u003e\n\u003cli\u003e更重要的是, 这些步骤是被我内部的注释器执行的, 没有任何机器学习专业知识. 数据科学家仅仅只是控制和管理这过程.\u003c/li\u003e\n\u003cli\u003e注释组由两名成员组成并且这整个过程只花了4天.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/blockquote\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch4 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eClothing Parsing\u003c/h4\u003e\u003ca id=\"user-content-clothing-parsing\" class=\"anchor\" aria-label=\"Permalink: Clothing Parsing\" href=\"#clothing-parsing\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"https://camo.githubusercontent.com/4b0564db9b324a86d7964d205d1c5b3a4506a405e46a417a89fd83d3d34b53d3/687474703a2f2f766973696f6e2e69732e746f686f6b752e61632e6a702f7e6b79616d6167752f72657365617263682f636c6f7468696e675f70617273696e672f636c6f7468696e672e706e67\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/4b0564db9b324a86d7964d205d1c5b3a4506a405e46a417a89fd83d3d34b53d3/687474703a2f2f766973696f6e2e69732e746f686f6b752e61632e6a702f7e6b79616d6167752f72657365617263682f636c6f7468696e675f70617273696e672f636c6f7468696e672e706e67\" alt=\"img\" data-canonical-src=\"http://vision.is.tohoku.ac.jp/~kyamagu/research/clothing_parsing/clothing.png\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e项目 :\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e\u003ca href=\"http://vision.is.tohoku.ac.jp/~kyamagu/research/clothing_parsing/\" rel=\"nofollow\"\u003ehttp://vision.is.tohoku.ac.jp/~kyamagu/research/clothing_parsing/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://vision.is.tohoku.ac.jp/~kyamagu/research/paperdoll/\" rel=\"nofollow\"\u003ehttp://vision.is.tohoku.ac.jp/~kyamagu/research/paperdoll/\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp dir=\"auto\"\u003eIn this paper we demonstrate an effective method for parsing clothing in fashion photographs, an extremely challenging problem due to the large number of possible garment items, variations in configuration, garment appearance, layering, and occlusion. In addition, we provide a large novel dataset and tools for labeling garment items, to enable future research on clothing estimation. Finally, we present intriguing initial results on using clothing estimates to improve pose identification, and demonstrate a prototype application for pose-independent visual garment retrieval.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch4 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eHumanParsing-Dataset\u003c/h4\u003e\u003ca id=\"user-content-humanparsing-dataset\" class=\"anchor\" aria-label=\"Permalink: HumanParsing-Dataset\" href=\"#humanparsing-dataset\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/2019-03-22-19-14-03.png\"\u003e\u003cimg src=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/2019-03-22-19-14-03.png\" alt=\"img\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e项目:\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/lemondan/HumanParsing-Dataset\"\u003ehttps://github.com/lemondan/HumanParsing-Dataset\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://www.sysu-hcp.net/deep-human-parsing/\" rel=\"nofollow\"\u003ehttp://www.sysu-hcp.net/deep-human-parsing/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://vuhcs.github.io/\" rel=\"nofollow\"\u003ehttps://vuhcs.github.io/\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e组织: \u003ca href=\"http://sysu-hcp.net/\" rel=\"nofollow\"\u003ehttp://sysu-hcp.net/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e下载:  \u003ca href=\"http://pan.baidu.com/s/1qY8bToS\" rel=\"nofollow\"\u003ehttp://pan.baidu.com/s/1qY8bToS\u003c/a\u003e (kjgk)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp dir=\"auto\"\u003eThis human parsing dataset includes the detailed pixel-wise annotations for fashion images, which is proposed in our TPAMI paper \"Deep Human Parsing with Active Template Regression\", and ICCV 2015 paper \"Human Parsing with Contextualized Convolutional Neural Network\". This dataset contains 7700 images. We use 6000 images for training, 1000 for testing and 700 as the validation set.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch4 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eLook into Person (LIP)\u003c/h4\u003e\u003ca id=\"user-content-look-into-person-lip\" class=\"anchor\" aria-label=\"Permalink: Look into Person (LIP)\" href=\"#look-into-person-lip\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/2019-03-12-11-18-29.png\"\u003e\u003cimg src=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/2019-03-12-11-18-29.png\" alt=\"img\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e项目: \u003ca href=\"http://sysu-hcp.net/lip/overview.php\" rel=\"nofollow\"\u003ehttp://sysu-hcp.net/lip/overview.php\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e下载: 不同任务有不同部分, 具体可见\u003ca href=\"http://sysu-hcp.net/lip/overview.php\" rel=\"nofollow\"\u003eDataset\u003c/a\u003e页面\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp dir=\"auto\"\u003eLook into Person (LIP) is a new large-scale dataset, focus on semantic understanding of person. Following are the detailed descriptions.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eThe dataset contains 50, 000 images with elaborated pixel-wise annotations with 19 semantic human part labels and 2D human poses with 16 key points.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003eThe annotated 50, 000 images are cropped person instances from COCO dataset with size larger than 50 * 50. The images collected from the real-world scenarios contain human appearing with challenging poses and views, heavily occlusions, various appearances and low-resolutions. We are working on collecting and annotating more images to increase diversity.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch4 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eTaobao Commodity Dataset\u003c/h4\u003e\u003ca id=\"user-content-taobao-commodity-dataset\" class=\"anchor\" aria-label=\"Permalink: Taobao Commodity Dataset\" href=\"#taobao-commodity-dataset\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/2019-03-22-19-09-55.png\"\u003e\u003cimg src=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/2019-03-22-19-09-55.png\" alt=\"img\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e项目: \u003ca href=\"http://www.sysu-hcp.net/taobao-commodity-dataset/\" rel=\"nofollow\"\u003ehttp://www.sysu-hcp.net/taobao-commodity-dataset/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e下载:\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e\u003ca href=\"http://www.sysu-hcp.net/wp-content/uploads/2016/03/Imgs_TCD.zip\" rel=\"nofollow\"\u003ehttp://www.sysu-hcp.net/wp-content/uploads/2016/03/Imgs_TCD.zip\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://www.sysu-hcp.net/wp-content/uploads/2016/03/Mask_TCD.zip\" rel=\"nofollow\"\u003ehttp://www.sysu-hcp.net/wp-content/uploads/2016/03/Mask_TCD.zip\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp dir=\"auto\"\u003eTCD contains 800 commodity images (dresses, jeans, T-shirts, shoes and hats) from the shops on the Taobao website. The ground truth masks of the TCD dataset are obtained by inviting common sellers of Taobao website to annotate their commodities, i.e., masking salient objects that they want to show from their exhibition. These images include all kind   s of commodity with and without human models, thus having complex backgrounds and scenes with highly complex foregrounds. Pixel-accurate ground truth masks are given. These images including all kinds of commodities with and without human models have complex backgrounds and scenes with large foregrounds for evaluation. Figure 1 illustrates some of them.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch4 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eObject Extraction Dataset\u003c/h4\u003e\u003ca id=\"user-content-object-extraction-dataset\" class=\"anchor\" aria-label=\"Permalink: Object Extraction Dataset\" href=\"#object-extraction-dataset\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"https://camo.githubusercontent.com/829ac09125e91b21f8e3562b06d148472843f3b1209e45bf3c6061d8b54010fa/68747470733a2f2f6f626a65637465787472616374696f6e2e6769746875622e696f2f696d67732f696d616765735f6d61736b732e706e67\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/829ac09125e91b21f8e3562b06d148472843f3b1209e45bf3c6061d8b54010fa/68747470733a2f2f6f626a65637465787472616374696f6e2e6769746875622e696f2f696d67732f696d616765735f6d61736b732e706e67\" alt=\"img\" data-canonical-src=\"https://objectextraction.github.io/imgs/images_masks.png\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e项目: \u003ca href=\"https://objectextraction.github.io/\" rel=\"nofollow\"\u003ehttps://objectextraction.github.io/\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp dir=\"auto\"\u003eThis Object Extraction newly collected by us contains 10183 images with groundtruth segmentation masks. We selected the images from the PASCAL, iCoseg, Internet dataset as well as other data (most of them are about people and clothes) from the web. We randomly split the dataset with 8230 images for training and 1953 images for testing.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch4 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eClothing Co-Parsing (CCP) Dataset\u003c/h4\u003e\u003ca id=\"user-content-clothing-co-parsing-ccp-dataset\" class=\"anchor\" aria-label=\"Permalink: Clothing Co-Parsing (CCP) Dataset\" href=\"#clothing-co-parsing-ccp-dataset\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/2019-03-12-11-12-28.png\"\u003e\u003cimg src=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/2019-03-12-11-12-28.png\" alt=\"img\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e项目: \u003ca href=\"https://github.com/bearpaw/clothing-co-parsing\"\u003ehttps://github.com/bearpaw/clothing-co-parsing\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp dir=\"auto\"\u003eClothing Co-Parsing (CCP) dataset is a new clothing database including elaborately annotated clothing items.\n2, 098 high-resolution street fashion photos with totally 59 tags\nWide range of styles, accessaries, garments, and pose\nAll images are with image-level annotations\n1000+ images are with pixel-level annotations\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch4 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eBaidu People segmentation dataset[need help]\u003c/h4\u003e\u003ca id=\"user-content-baidu-people-segmentation-datasetneed-help\" class=\"anchor\" aria-label=\"Permalink: Baidu People segmentation dataset[need help]\" href=\"#baidu-people-segmentation-datasetneed-help\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e下载: \u003ca href=\"http://www.cbsr.ia.ac.cn/users/ynyu/dataset/\" rel=\"nofollow\"\u003ehttp://www.cbsr.ia.ac.cn/users/ynyu/dataset/\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp dir=\"auto\"\u003e这个数据集主要是用于人体整体分割. 它由5387张训练图片组成, 但是测试图片没有公布. 因此训练时可以从5387中随机挑选500张作为验证集, 然后4887张作为训练集. 参考论文《Early Hierarchical Contexts Learned by CNN for image segmentation》.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e原文:\u003ca href=\"https://blog.csdn.net/mou_it/article/details/82225505\" rel=\"nofollow\"\u003ehttps://blog.csdn.net/mou_it/article/details/82225505\u003c/a\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eMatting\u003c/h2\u003e\u003ca id=\"user-content-matting\" class=\"anchor\" aria-label=\"Permalink: Matting\" href=\"#matting\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003ealphamatting.com\u003c/h3\u003e\u003ca id=\"user-content-alphamattingcom\" class=\"anchor\" aria-label=\"Permalink: alphamatting.com\" href=\"#alphamattingcom\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/1546154705536.png\"\u003e\u003cimg src=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/1546154705536.png\" alt=\"1546154705536\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e项目: \u003ca href=\"http://alphamatting.com/datasets.php\" rel=\"nofollow\"\u003ehttp://alphamatting.com/datasets.php\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e下载:\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e\u003ca href=\"http://alphamatting.com/datasets/zip/input_training_lowres.zip\" rel=\"nofollow\"\u003einput_training_lowres.zip\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://alphamatting.com/datasets/zip/input_training_highres.zip\" rel=\"nofollow\"\u003einput_training_highres.zip\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://alphamatting.com/datasets/zip/trimap_training_lowres.zip\" rel=\"nofollow\"\u003etrimap_training_lowres.zip\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://alphamatting.com/datasets/zip/trimap_training_highres.zip\" rel=\"nofollow\"\u003etrimap_training_highres.zip\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://alphamatting.com/datasets/zip/gt_training_lowres.zip\" rel=\"nofollow\"\u003egt_training_lowres.zip\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://alphamatting.com/datasets/zip/gt_training_highres.zip\" rel=\"nofollow\"\u003egt_training_highres.zip\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp dir=\"auto\"\u003e这是图像matting方法的现有基准. 它\u003cstrong\u003e包括8个测试图像, 每个图像有3个不同的三维图形\u003c/strong\u003e, 即\"small\", \"large\"和\"user\"\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eComposition-1k: Deep Image Matting\u003c/h3\u003e\u003ca id=\"user-content-composition-1k-deep-image-matting\" class=\"anchor\" aria-label=\"Permalink: Composition-1k: Deep Image Matting\" href=\"#composition-1k-deep-image-matting\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/1546154519720.png\"\u003e\u003cimg src=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/1546154519720.png\" alt=\"1546154519720\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e项目: \u003ca href=\"https://sites.google.com/view/deepimagematting\" rel=\"nofollow\"\u003ehttps://sites.google.com/view/deepimagematting\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e论文: \u003ca href=\"https://arxiv.org/abs/1703.03872\" rel=\"nofollow\"\u003ehttps://arxiv.org/abs/1703.03872\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e下载: Please contact Brian Price (\u003ca href=\"mailto:bprice@adobe.com\"\u003ebprice@adobe.com\u003c/a\u003e) for the dataset.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp dir=\"auto\"\u003e抠图是一个基本的计算机视觉问题, 有许多应用. 当图像具有相似的前景色和背景色或复杂的纹理时, 先前的算法具有差的性能. 主要原因是先前的方法 1)仅使用低级功能和2)缺乏高级上下文. 在本文中, 我们提出了一种新的基于深度学习的算法, 可以解决这两个问题. 我们的深层模型有两个部分. 第一部分是深度卷积编码器 * 解码器网络, 它将图像和相应的trimap作为输入并预测图像的alpha遮罩. 第二部分是一个小的卷积网络, 它改进了第一个网络的alpha遮罩预测, 以获得更准确的alpha值和更清晰的边缘. 此外, \u003cstrong\u003e我们还创建了一个大型图像抠图数据集, 包括49300个训练图像和1000个测试图像\u003c/strong\u003e. 我们在抠图基准, 我们的测试集和各种真实图像上评估我们的算法. 实验结果清楚地证明了我们的算法优于以前的方法.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e我们使用合成创建一个大规模的matting数据集. 仔细提取具有简单背景上的对象的图像并将其合成到新的背景图像上以创建具有49300(45500)个训练图像和1000个测试图像的数据集.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e......\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e我们将评估3个数据集上的方法.1)我们评估alphamatting.com数据集, 这是图像matting方法的现有基准. 它\u003cstrong\u003e包括8个测试图像, 每个图像有3个不同的三维图形\u003c/strong\u003e, 即\"小\", \"大\"和\"用户\".2)由于alphamatting.com数据集中对象的大小和范围有限, **我们提出了Composition-1k测试集. 我们基于作品的数据集包括1000个图像和50个独特的前景. 此数据集具有更广泛的对象类型和背景场景.**3)为了测量我们在自然图像上的表现, 我们还收集了包括31个自然图像的第三个数据集.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eSemantic Human Matting\u003c/h3\u003e\u003ca id=\"user-content-semantic-human-matting\" class=\"anchor\" aria-label=\"Permalink: Semantic Human Matting\" href=\"#semantic-human-matting\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/1546156688347.png\"\u003e\u003cimg src=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/1546156688347.png\" alt=\"1546156688347\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/2018-12-27-11-47-26.png\"\u003e\u003cimg src=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/2018-12-27-11-47-26.png\" alt=\"dataset\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e论文: \u003ca href=\"https://arxiv.org/abs/1809.01354\" rel=\"nofollow\"\u003ehttps://arxiv.org/abs/1809.01354\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003ealpha matting 的資料庫樣本過少, 對於深度學習來說首要條件就是資料樣本要多\u003c/li\u003e\n\u003cli\u003eShen et al. 此資料庫是透過 CF 以及 KNN 的方式所製造的, 因此有可能該資料庫有bias, 不採用.(這部分可搜尋幾個關鍵字: deep learning dataset bias).\u003c/li\u003e\n\u003cli\u003eDIM 的資料庫雖然有 493 個物件, 但是物件中包含人物的只有 202 個.\u003c/li\u003e\n\u003cli\u003eOur dataset 從電子商務網站中搜集圖片, 將35, 513個人物透過人工標注他的Annotation, 此資料集有遵循DIM的方法收集.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp dir=\"auto\"\u003e\u003ca href=\"https://medium.com/@xiaosean5408/%E6%B7%98%E5%AF%B6%E7%B6%B2%E7%9A%84%E4%BA%BA%E7%89%A9%E6%8F%90%E5%8F%96%E8%AB%96%E6%96%87%E7%B0%A1%E4%BB%8B-semantic-human-matting-52591c3f8e0c\" rel=\"nofollow\"\u003ehttps://medium.com/@xiaosean5408/%E6%B7%98%E5%AF%B6%E7%B6%B2%E7%9A%84%E4%BA%BA%E7%89%A9%E6%8F%90%E5%8F%96%E8%AB%96%E6%96%87%E7%B0%A1%E4%BB%8B-semantic-human-matting-52591c3f8e0c\u003c/a\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eMatting-Human-Datasets\u003c/h3\u003e\u003ca id=\"user-content-matting-human-datasets\" class=\"anchor\" aria-label=\"Permalink: Matting-Human-Datasets\" href=\"#matting-human-datasets\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/aisegmentcn/matting_human_datasets/raw/master/1.png\"\u003e\u003cimg src=\"https://github.com/aisegmentcn/matting_human_datasets/raw/master/1.png\" alt=\"\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/aisegmentcn/matting_human_datasets/raw/master/2.png\"\u003e\u003cimg src=\"https://github.com/aisegmentcn/matting_human_datasets/raw/master/2.png\" alt=\"\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e项目:\u003ca href=\"https://github.com/aisegmentcn/matting_human_datasets\"\u003ehttps://github.com/aisegmentcn/matting_human_datasets\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e下载:\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e百度云盘:\u003ca href=\"https://pan.baidu.com/s/1R9PJJRT-KjSxh-2-3wCGxQ\" rel=\"nofollow\"\u003ehttps://pan.baidu.com/s/1R9PJJRT-KjSxh-2-3wCGxQ\u003c/a\u003e 提取码:dzsn\u003c/li\u003e\n\u003cli\u003emega:\u003ca href=\"https://mega.nz/#F!Gh8CFAyb!e2ppUh-copP76GbE8IWAEQ\" rel=\"nofollow\"\u003ehttps://mega.nz/#F!Gh8CFAyb!e2ppUh-copP76GbE8IWAEQ\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003ekaggle:\u003ca href=\"https://www.kaggle.com/laurentmih/aisegmentcom-matting-human-datasets/\" rel=\"nofollow\"\u003ehttps://www.kaggle.com/laurentmih/aisegmentcom-matting-human-datasets/\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp dir=\"auto\"\u003e本数据集为目前已知最大的人像matting数据集, 包含34427张图像和对应的matting结果图. 数据集由北京玩星汇聚科技有限公司高质量标注, 使用该数据集所训练的人像软分割模型已商用.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e数据集中的原始图片来源于Flickr, 百度, 淘宝. 经过人脸检测和区域裁剪后生成了600*800的半身人像.\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003eclip_img目录为半身人像图像, 格式为jpg;\u003c/li\u003e\n\u003cli\u003ematting目录为对应的matting文件(方便确认matting质量), 格式为png, 您训练前应该先从png图像提取alpha图. 例如使用opencv可以这样获得alpha图:\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/blockquote\u003e\n\u003cdiv class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"in_image = cv2.imread('png图像文件路径', cv2.IMREAD_UNCHANGED)\nalpha = in_image[:,:,3]\"\u003e\u003cpre\u003e\u003cspan class=\"pl-s1\"\u003ein_image\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003ecv2\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eimread\u003c/span\u003e(\u003cspan class=\"pl-s\"\u003e'png图像文件路径'\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003ecv2\u003c/span\u003e.\u003cspan class=\"pl-v\"\u003eIMREAD_UNCHANGED\u003c/span\u003e)\n\u003cspan class=\"pl-s1\"\u003ealpha\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003ein_image\u003c/span\u003e[:,:,\u003cspan class=\"pl-c1\"\u003e3\u003c/span\u003e]\u003c/pre\u003e\u003c/div\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003ePFCN\u003c/h3\u003e\u003ca id=\"user-content-pfcn\" class=\"anchor\" aria-label=\"Permalink: PFCN\" href=\"#pfcn\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/1546173669466.png\"\u003e\u003cimg src=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/1546173669466.png\" alt=\"1546173669466\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e项目: \u003ca href=\"http://xiaoyongshen.me/webpage_portrait/index.html\" rel=\"nofollow\"\u003ehttp://xiaoyongshen.me/webpage_portrait/index.html\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e论文: \u003ca href=\"http://xiaoyongshen.me/webpage_portrait/papers/portrait_eg16.pdf\" rel=\"nofollow\"\u003eAutomatic Portrait Segmentation for Image Stylization\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e下载: Please download from \u003ca href=\"https://1drv.ms/u/s!ApwdOxIIFBH19TzDv7nRfH5ZsMNL\" rel=\"nofollow\"\u003eOneDrive\u003c/a\u003e or \u003ca href=\"http://pan.baidu.com/s/1bQ4yHC\" rel=\"nofollow\"\u003eBaiduyun\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp dir=\"auto\"\u003e肖像画是摄影和绘画的主要艺术形式. 在大多数情况下, 艺术家试图使主体从周围突出, 例如, 使其更亮或更锐利. 在数字世界中, 通过使用适合于图像语义的照相或绘画滤镜处理肖像图像, 可以实现类似的效果. 虽然存在许多成功的用户指导方法来描绘该主题, 但缺乏全自动技术并且产生不令人满意的结果. 我们的论文首先通过引入专用于肖像的新自动分割算法来解决这个问题. 然后, 我们在此结果的基础上, 描述了几个利用我们的自动分割算法生成高质量肖像的肖像滤镜.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eDeep Automatic Portrait Matting\u003c/h3\u003e\u003ca id=\"user-content-deep-automatic-portrait-matting\" class=\"anchor\" aria-label=\"Permalink: Deep Automatic Portrait Matting\" href=\"#deep-automatic-portrait-matting\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/2019-01-01-19-31-55.png\"\u003e\u003cimg src=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/2019-01-01-19-31-55.png\" alt=\"img\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e论文: \u003ca href=\"http://www.cse.cuhk.edu.hk/~leojia/projects/automatting/papers/deepmatting.pdf\" rel=\"nofollow\"\u003ehttp://www.cse.cuhk.edu.hk/~leojia/projects/automatting/papers/deepmatting.pdf\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e项目:\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e\u003ca href=\"http://www.cse.cuhk.edu.hk/~leojia/projects/automatting/\" rel=\"nofollow\"\u003ehttp://www.cse.cuhk.edu.hk/~leojia/projects/automatting/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://xiaoyongshen.me/webpages/webpage_automatting/\" rel=\"nofollow\"\u003ehttp://xiaoyongshen.me/webpages/webpage_automatting/\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e下载:\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e[Data(zip, 1.15GB)] Please send Email to \u003ca href=\"mailto:goodshenxy@gmail.com\"\u003egoodshenxy@gmail.com\u003c/a\u003e to request it.\u003c/li\u003e\n\u003cli\u003e作者自己公开了: \u003ca href=\"https://1drv.ms/u/s!ApwdOxIIFBH19Ts5EuFd9gVJrKTo\" rel=\"nofollow\"\u003ehttps://1drv.ms/u/s!ApwdOxIIFBH19Ts5EuFd9gVJrKTo\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp dir=\"auto\"\u003e我们提出了一种用于性状图像的自动图像matting方法. 该方法不需要用户交互, 这在大多数先前的方法中是必不可少的. 为了实现这一目标, 提出了一种新的端到端卷积神经网络(CNN)框架, 其采用肖像图像的输入. 它输出matting的结果. 我们的方法不仅考虑图像语义预测, 还考虑像素级图像matte优化. 一个新的肖像image dataset与我们标记的matting基础事实构成. 我们的自动方法通过最先进的方法获得了可比较的结果, 该方法需要指定的前景和背景区域或像素.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e我们从Flickr收集了肖像图像. 然后选择它们以确保肖像具有各种年龄, 颜色, 衣服, 配饰, 发型, 头部位置, 背景场景等.matting区域主要是由于景深引起的头发和柔软边缘. 裁剪所有图像, 使得面部矩形具有相似的尺寸. 通过选定的肖像图像, 我们创建了具有密集用户交互的alpha matte, 以确保它们具有高质量.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e首先, 我们标记每个图像放大到局部区域的三元组.\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e然后我们计算mattes, 使用闭式matting[1]和KNN matting[2].\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e每个图像的两个计算遮罩覆盖背景图像以手动检查质量. 我们为数据集选择更好的一个. 如果两个mattes都不符合我们的高标准, 结果将被丢弃. 必要时, 小错误可以通过Photoshop[31]来解决. 在此标签处理后, 我们收集了2, 000张高质量遮罩图像. 这些图像被随机分成训练和测试集, 分别具有1, 700和300个图像.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eOther\u003c/h2\u003e\u003ca id=\"user-content-other\" class=\"anchor\" aria-label=\"Permalink: Other\" href=\"#other\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eLarge-scale Fashion (DeepFashion) Database\u003c/h3\u003e\u003ca id=\"user-content-large-scale-fashion-deepfashion-database\" class=\"anchor\" aria-label=\"Permalink: Large-scale Fashion (DeepFashion) Database\" href=\"#large-scale-fashion-deepfashion-database\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/2019-03-22-18-57-36.png\"\u003e\u003cimg src=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/2019-03-22-18-57-36.png\" alt=\"img\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e项目: \u003ca href=\"http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html\" rel=\"nofollow\"\u003ehttp://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e组织:\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e\u003ca href=\"http://mmlab.ie.cuhk.edu.hk/\" rel=\"nofollow\"\u003eMultimedia Laboratory\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://www.cuhk.edu.hk/english/index.html\" rel=\"nofollow\"\u003eThe Chinese University of Hong Kong\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e下载: \u003ca href=\"http://pan.baidu.com/s/1i43pnZR\" rel=\"nofollow\"\u003ehttp://pan.baidu.com/s/1i43pnZR\u003c/a\u003e (更多细节请见项目主页)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp dir=\"auto\"\u003e我们提供DeepFashion数据库, 这是一个大型服装数据库, 它有几个吸引人的特性:\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e首先, DeepFashion包含超过800, 000种不同的时尚图像, 从精美的商店图像到无约束的消费者照片.\u003c/li\u003e\n\u003cli\u003e其次, DeepFashion注释了丰富的服装商品信息. 此数据集中的每个图像都标有50个类别, 1, 000个描述性属性, 边界框和服装标记.\u003c/li\u003e\n\u003cli\u003e第三, DeepFashion包含超过300, 000个交叉姿势/跨域图像对. 使用DeepFashion数据库开发了四个基准, 包括属性预测, 消费者到商店的衣服检索, 店内衣服检索和地标检测.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp dir=\"auto\"\u003e这些基准的数据和注释也可以用作以下计算机视觉任务的训练和测试集, 例如衣服检测, 衣服识别和图像检索. 请阅读\"下载说明\"以访问数据集.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eML-Image\u003c/h3\u003e\u003ca id=\"user-content-ml-image\" class=\"anchor\" aria-label=\"Permalink: ML-Image\" href=\"#ml-image\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e项目: \u003ca href=\"https://github.com/Tencent/tencent-ml-images#download-images-from-open-images\"\u003ehttps://github.com/Tencent/tencent-ml-images#download-images-from-open-images\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp dir=\"auto\"\u003eML-Images: the largest open-source multi-label image database, including 17, 609, 752 training and 88, 739 validation image URLs, which are annotated with up to 11, 166 categories\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eneed your help...\u003c/h2\u003e\u003ca id=\"user-content-need-your-help\" class=\"anchor\" aria-label=\"Permalink: need your help...\" href=\"#need-your-help\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cblockquote\u003e\n\u003cp dir=\"auto\"\u003e有些数据集已经忘记了出处, 大家有见过的, 希望可以补充下.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003eImage Pair\u003c/li\u003e\n\u003cli\u003eCosal2015\u003c/li\u003e\n\u003cli\u003eINCT2016\u003c/li\u003e\n\u003cli\u003eRGBDCoseg183\u003c/li\u003e\n\u003cli\u003e06RGBDCosal150\u003c/li\u003e\n\u003cli\u003eSegTrackV1/V2\u003c/li\u003e\n\u003cli\u003eViSal\u003c/li\u003e\n\u003cli\u003eMCL\u003c/li\u003e\n\u003cli\u003eUVSD\u003c/li\u003e\n\u003cli\u003eVOS\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eReference\u003c/h2\u003e\u003ca id=\"user-content-reference\" class=\"anchor\" aria-label=\"Permalink: Reference\" href=\"#reference\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003e\u003ca href=\"https://arxiv.org/abs/1411.5878\" rel=\"nofollow\"\u003eSalient Object Detection: A Survey\u003c/a\u003e\u003c/h3\u003e\u003ca id=\"user-content-salient-object-detection-a-survey\" class=\"anchor\" aria-label=\"Permalink: Salient Object Detection: A Survey\" href=\"#salient-object-detection-a-survey\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/2018-12-29-17-06-42.png\"\u003e\u003cimg src=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/2018-12-29-17-06-42.png\" alt=\"img\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e详细评估: \u003ca href=\"https://mmcheng.net/zh/salobjbenchmark/\" rel=\"nofollow\"\u003ehttps://mmcheng.net/zh/salobjbenchmark/\u003c/a\u003e (这里展示了{THUR15K, JuddDB, DUT-OMRON, SED2, MSRA10K, ECSSD}六种数据集的一个榜单).\u003c/p\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003e\u003ca href=\"https://arxiv.org/abs/1803.03391\" rel=\"nofollow\"\u003eReview of Visual Saliency Detection with Comprehensive Information\u003c/a\u003e\u003c/h3\u003e\u003ca id=\"user-content-review-of-visual-saliency-detection-with-comprehensive-information\" class=\"anchor\" aria-label=\"Permalink: Review of Visual Saliency Detection with Comprehensive Information\" href=\"#review-of-visual-saliency-detection-with-comprehensive-information\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/2018-12-27-11-05-49.png\"\u003e\u003cimg src=\"/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/2018-12-27-11-05-49.png\" alt=\"dataset\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003e\u003ca href=\"https://www.researchgate.net/publication/332553805_Salient_Object_Detection_in_the_Deep_Learning_Era_An_In-Depth_Survey\" rel=\"nofollow\"\u003eSalient Object Detection in the Deep Learning Era: An In-Depth Survey\u003c/a\u003e\u003c/h3\u003e\u003ca id=\"user-content-salient-object-detection-in-the-deep-learning-era-an-in-depth-survey\" class=\"anchor\" aria-label=\"Permalink: Salient Object Detection in the Deep Learning Era: An In-Depth Survey\" href=\"#salient-object-detection-in-the-deep-learning-era-an-in-depth-survey\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e项目: \u003ca href=\"https://github.com/wenguanwang/SODsurvey\"\u003ehttps://github.com/wenguanwang/SODsurvey\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e说明: 本文档于2019年07月07日修改的内容主要参考自该综述论文, 感谢作者的工作, 总结的非常详细!\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eMore\u003c/h2\u003e\u003ca id=\"user-content-more\" class=\"anchor\" aria-label=\"Permalink: More\" href=\"#more\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eSimiliar Projects\u003c/h3\u003e\u003ca id=\"user-content-similiar-projects\" class=\"anchor\" aria-label=\"Permalink: Similiar Projects\" href=\"#similiar-projects\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/mrgloom/awesome-semantic-segmentation\"\u003eawesome-semantic-segmentation\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eResearch Institutes\u003c/h3\u003e\u003ca id=\"user-content-research-institutes\" class=\"anchor\" aria-label=\"Permalink: Research Institutes\" href=\"#research-institutes\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e百度研究院: \u003ca href=\"https://ai.baidu.com/broad/introduction\" rel=\"nofollow\"\u003ehttps://ai.baidu.com/broad/introduction\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e中山大学人机物智能融合实验室: \u003ca href=\"http://www.sysu-hcp.net/resources/\" rel=\"nofollow\"\u003ehttp://www.sysu-hcp.net/resources/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e大连理工大学IIAU-LAB: \u003ca href=\"http://ice.dlut.edu.cn/lu/publications.html\" rel=\"nofollow\"\u003ehttp://ice.dlut.edu.cn/lu/publications.html\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eCUHK Multimedia Laboratory: \u003ca href=\"http://mmlab.ie.cuhk.edu.hk/datasets.html\" rel=\"nofollow\"\u003ehttp://mmlab.ie.cuhk.edu.hk/datasets.html\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eResource Websites\u003c/h3\u003e\u003ca id=\"user-content-resource-websites\" class=\"anchor\" aria-label=\"Permalink: Resource Websites\" href=\"#resource-websites\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003eTC-11 Online Resources: \u003ca href=\"http://tc11.cvc.uab.es/datasets/type/\" rel=\"nofollow\"\u003ehttp://tc11.cvc.uab.es/datasets/type/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eCVonline: Image Databases: \u003ca href=\"http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm\" rel=\"nofollow\"\u003ehttp://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm\u003c/a\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e中文: \u003ca href=\"https://blog.csdn.net/zhaoliang027/article/details/83376167\" rel=\"nofollow\"\u003ehttps://blog.csdn.net/zhaoliang027/article/details/83376167\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eMediaEval Benchmark: \u003ca href=\"http://www.multimediaeval.org/datasets/\" rel=\"nofollow\"\u003ehttp://www.multimediaeval.org/datasets/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eMit Saliency Benchmark: \u003ca href=\"http://saliency.mit.edu/datasets.html\" rel=\"nofollow\"\u003ehttp://saliency.mit.edu/datasets.html\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eDatasets for machine learning: \u003ca href=\"https://www.datasetlist.com/\" rel=\"nofollow\"\u003ehttps://www.datasetlist.com/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eUCI machine learning repository: \u003ca href=\"https://archive.ics.uci.edu/ml/datasets.html\" rel=\"nofollow\"\u003ehttps://archive.ics.uci.edu/ml/datasets.html\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eKaggle datasets: \u003ca href=\"https://www.kaggle.com/datasets\" rel=\"nofollow\"\u003ehttps://www.kaggle.com/datasets\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eGoogle\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003eDataset Seaerch: \u003ca href=\"https://toolbox.google.com/datasetsearch\" rel=\"nofollow\"\u003ehttps://toolbox.google.com/datasetsearch\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://ai.google/tools/datasets/\" rel=\"nofollow\"\u003ehttps://ai.google/tools/datasets/\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eYet Another Computer Vision Index To Datasets (YACVID): This website provides a list of frequently used computer vision datasets. Wait, there is more! There is also a description containing common problems, pitfalls and characteristics and now a searchable TAG cloud.: \u003ca href=\"http://yacvid.hayko.at/\" rel=\"nofollow\"\u003ehttp://yacvid.hayko.at/\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eAbout\u003c/h2\u003e\u003ca id=\"user-content-about\" class=\"anchor\" aria-label=\"Permalink: About\" href=\"#about\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003eEdited by Lart Pang\u003c/li\u003e\n\u003cli\u003eTools: VSCode\u003c/li\u003e\n\u003cli\u003ePlugins:\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003eMarkdown All in One\u003c/li\u003e\n\u003cli\u003emarkdown-formatter(随着不断地提了一些issue(\u003ca href=\"https://github.com/sumnow/markdown-formatter/issues/5\" data-hovercard-type=\"issue\" data-hovercard-url=\"/sumnow/markdown-formatter/issues/5/hovercard\"\u003e#5\u003c/a\u003e, \u003ca href=\"https://github.com/sumnow/markdown-formatter/issues/6\" data-hovercard-type=\"issue\" data-hovercard-url=\"/sumnow/markdown-formatter/issues/6/hovercard\"\u003e#6\u003c/a\u003e, \u003ca href=\"https://github.com/sumnow/markdown-formatter/issues/7\" data-hovercard-type=\"issue\" data-hovercard-url=\"/sumnow/markdown-formatter/issues/7/hovercard\"\u003e#7\u003c/a\u003e, \u003ca href=\"https://github.com/sumnow/markdown-formatter/issues/8\" data-hovercard-type=\"issue\" data-hovercard-url=\"/sumnow/markdown-formatter/issues/8/hovercard\"\u003e#8\u003c/a\u003e, \u003ca href=\"https://github.com/sumnow/markdown-formatter/issues/9\" data-hovercard-type=\"issue\" data-hovercard-url=\"/sumnow/markdown-formatter/issues/9/hovercard\"\u003e#9\u003c/a\u003e), 越来越好用了, 强烈推荐)\u003c/li\u003e\n\u003cli\u003ePaste Image\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/article\u003e","loaded":true,"timedOut":false,"errorMessage":null,"headerInfo":{"toc":[{"level":1,"text":"Another Awesome Dataset List","anchor":"another-awesome-dataset-list","htmlText":"Another Awesome Dataset List"},{"level":2,"text":"Saliency","anchor":"saliency","htmlText":"Saliency"},{"level":3,"text":"RGB-Saliency Detection","anchor":"rgb-saliency-detection","htmlText":"RGB-Saliency Detection"},{"level":4,"text":"MSRA(MSRA10K/MSRA-B)","anchor":"msramsra10kmsra-b","htmlText":"MSRA(MSRA10K/MSRA-B)"},{"level":4,"text":"SED1/2","anchor":"sed12","htmlText":"SED1/2"},{"level":4,"text":"ASD(MSRA1000/MSRA1K)[need some images]","anchor":"asdmsra1000msra1kneed-some-images","htmlText":"ASD(MSRA1000/MSRA1K)[need some images]"},{"level":4,"text":"DUT-OMRON","anchor":"dut-omron","htmlText":"DUT-OMRON"},{"level":4,"text":"DUTS","anchor":"duts","htmlText":"DUTS"},{"level":4,"text":"HKU-IS[need some iamges]","anchor":"hku-isneed-some-iamges","htmlText":"HKU-IS[need some iamges]"},{"level":4,"text":"SOD","anchor":"sod","htmlText":"SOD"},{"level":4,"text":"iCoSeg","anchor":"icoseg","htmlText":"iCoSeg"},{"level":4,"text":"Infrared[need help]","anchor":"infraredneed-help","htmlText":"Infrared[need help]"},{"level":4,"text":"ImgSal","anchor":"imgsal","htmlText":"ImgSal"},{"level":4,"text":"ECSSD/CSSD","anchor":"ecssdcssd","htmlText":"ECSSD/CSSD"},{"level":4,"text":"THUR15K","anchor":"thur15k","htmlText":"THUR15K"},{"level":4,"text":"Bruce-A[need help]","anchor":"bruce-aneed-help","htmlText":"Bruce-A[need help]"},{"level":4,"text":"Judd-A[need help]","anchor":"judd-aneed-help","htmlText":"Judd-A[need help]"},{"level":4,"text":"PASCAL-S","anchor":"pascal-s","htmlText":"PASCAL-S"},{"level":4,"text":"UCSB[need help]","anchor":"ucsbneed-help","htmlText":"UCSB[need help]"},{"level":4,"text":"OSIE[need help]","anchor":"osieneed-help","htmlText":"OSIE[need help]"},{"level":4,"text":"ACSD","anchor":"acsd","htmlText":"ACSD"},{"level":3,"text":"Other Special SOD Datasets","anchor":"other-special-sod-datasets","htmlText":"Other Special SOD Datasets"},{"level":4,"text":"XPIE","anchor":"xpie","htmlText":"XPIE"},{"level":4,"text":"SOC","anchor":"soc","htmlText":"SOC"},{"level":4,"text":"SOS/MOS[need some images]","anchor":"sosmosneed-some-images","htmlText":"SOS/MOS[need some images]"},{"level":4,"text":"ILSO[need some images]","anchor":"ilsoneed-some-images","htmlText":"ILSO[need some images]"},{"level":4,"text":"HS-SOD","anchor":"hs-sod","htmlText":"HS-SOD"},{"level":3,"text":"Video Saliency Detection","anchor":"video-saliency-detection","htmlText":"Video Saliency Detection"},{"level":4,"text":"RSD(PKU-RSD)","anchor":"rsdpku-rsd","htmlText":"RSD(PKU-RSD)"},{"level":4,"text":"STC[need help]","anchor":"stcneed-help","htmlText":"STC[need help]"},{"level":3,"text":"RGBD-Saliency Detection","anchor":"rgbd-saliency-detection","htmlText":"RGBD-Saliency Detection"},{"level":4,"text":"SIP","anchor":"sip","htmlText":"SIP"},{"level":4,"text":"NLPR/RGBD1000","anchor":"nlprrgbd1000","htmlText":"NLPR/RGBD1000"},{"level":4,"text":"NJU400/2000","anchor":"nju4002000","htmlText":"NJU400/2000"},{"level":4,"text":"STEREO/SSB","anchor":"stereossb","htmlText":"STEREO/SSB"},{"level":4,"text":"LFSD[nead img]","anchor":"lfsdnead-img","htmlText":"LFSD[nead img]"},{"level":4,"text":"RGBD135/DES","anchor":"rgbd135des","htmlText":"RGBD135/DES"},{"level":4,"text":"DUT-RGBD","anchor":"dut-rgbd","htmlText":"DUT-RGBD"},{"level":4,"text":"SSD100","anchor":"ssd100","htmlText":"SSD100"},{"level":3,"text":"RGBT-Saliency Detection [need more information...]","anchor":"rgbt-saliency-detection-need-more-information","htmlText":"RGBT-Saliency Detection [need more information...]"},{"level":4,"text":"VT1000 Dataset","anchor":"vt1000-dataset","htmlText":"VT1000 Dataset"},{"level":4,"text":"VT821 Dataset","anchor":"vt821-dataset","htmlText":"VT821 Dataset"},{"level":3,"text":"High-Resolution Saliency Detection","anchor":"high-resolution-saliency-detection","htmlText":"High-Resolution Saliency Detection"},{"level":4,"text":"HRSOD/DAVIS-S","anchor":"hrsoddavis-s","htmlText":"HRSOD/DAVIS-S"},{"level":3,"text":"Other Saliency Dataset","anchor":"other-saliency-dataset","htmlText":"Other Saliency Dataset"},{"level":4,"text":"KAIST Salient Pedestrian Dataset","anchor":"kaist-salient-pedestrian-dataset","htmlText":"KAIST Salient Pedestrian Dataset"},{"level":2,"text":"Segmentation","anchor":"segmentation","htmlText":"Segmentation"},{"level":3,"text":"General[need help]","anchor":"generalneed-help","htmlText":"General[need help]"},{"level":4,"text":"DAVIS","anchor":"davis","htmlText":"DAVIS"},{"level":4,"text":"aNYU","anchor":"anyu","htmlText":"aNYU"},{"level":3,"text":"About Person","anchor":"about-person","htmlText":"About Person"},{"level":4,"text":"Supervisely人像数据集","anchor":"supervisely人像数据集","htmlText":"Supervisely人像数据集"},{"level":4,"text":"Clothing Parsing","anchor":"clothing-parsing","htmlText":"Clothing Parsing"},{"level":4,"text":"HumanParsing-Dataset","anchor":"humanparsing-dataset","htmlText":"HumanParsing-Dataset"},{"level":4,"text":"Look into Person (LIP)","anchor":"look-into-person-lip","htmlText":"Look into Person (LIP)"},{"level":4,"text":"Taobao Commodity Dataset","anchor":"taobao-commodity-dataset","htmlText":"Taobao Commodity Dataset"},{"level":4,"text":"Object Extraction Dataset","anchor":"object-extraction-dataset","htmlText":"Object Extraction Dataset"},{"level":4,"text":"Clothing Co-Parsing (CCP) Dataset","anchor":"clothing-co-parsing-ccp-dataset","htmlText":"Clothing Co-Parsing (CCP) Dataset"},{"level":4,"text":"Baidu People segmentation dataset[need help]","anchor":"baidu-people-segmentation-datasetneed-help","htmlText":"Baidu People segmentation dataset[need help]"},{"level":2,"text":"Matting","anchor":"matting","htmlText":"Matting"},{"level":3,"text":"alphamatting.com","anchor":"alphamattingcom","htmlText":"alphamatting.com"},{"level":3,"text":"Composition-1k: Deep Image Matting","anchor":"composition-1k-deep-image-matting","htmlText":"Composition-1k: Deep Image Matting"},{"level":3,"text":"Semantic Human Matting","anchor":"semantic-human-matting","htmlText":"Semantic Human Matting"},{"level":3,"text":"Matting-Human-Datasets","anchor":"matting-human-datasets","htmlText":"Matting-Human-Datasets"},{"level":3,"text":"PFCN","anchor":"pfcn","htmlText":"PFCN"},{"level":3,"text":"Deep Automatic Portrait Matting","anchor":"deep-automatic-portrait-matting","htmlText":"Deep Automatic Portrait Matting"},{"level":2,"text":"Other","anchor":"other","htmlText":"Other"},{"level":3,"text":"Large-scale Fashion (DeepFashion) Database","anchor":"large-scale-fashion-deepfashion-database","htmlText":"Large-scale Fashion (DeepFashion) Database"},{"level":3,"text":"ML-Image","anchor":"ml-image","htmlText":"ML-Image"},{"level":2,"text":"need your help...","anchor":"need-your-help","htmlText":"need your help..."},{"level":2,"text":"Reference","anchor":"reference","htmlText":"Reference"},{"level":3,"text":"Salient Object Detection: A Survey","anchor":"salient-object-detection-a-survey","htmlText":"Salient Object Detection: A Survey"},{"level":3,"text":"Review of Visual Saliency Detection with Comprehensive Information","anchor":"review-of-visual-saliency-detection-with-comprehensive-information","htmlText":"Review of Visual Saliency Detection with Comprehensive Information"},{"level":3,"text":"Salient Object Detection in the Deep Learning Era: An In-Depth Survey","anchor":"salient-object-detection-in-the-deep-learning-era-an-in-depth-survey","htmlText":"Salient Object Detection in the Deep Learning Era: An In-Depth Survey"},{"level":2,"text":"More","anchor":"more","htmlText":"More"},{"level":3,"text":"Similiar Projects","anchor":"similiar-projects","htmlText":"Similiar Projects"},{"level":3,"text":"Research Institutes","anchor":"research-institutes","htmlText":"Research Institutes"},{"level":3,"text":"Resource Websites","anchor":"resource-websites","htmlText":"Resource Websites"},{"level":2,"text":"About","anchor":"about","htmlText":"About"}],"siteNavLoginPath":"/login?return_to=https%3A%2F%2Fgithub.com%2FAIMonk-Labs-Private-Limited%2Fawesome-segmentation-saliency-dataset"}}],"overviewFilesProcessingTime":320.727386}},"appPayload":{"helpUrl":"https://docs.github.com","findFileWorkerPath":"/assets-cdn/worker/find-file-worker-a007d7f370d6.js","findInFileWorkerPath":"/assets-cdn/worker/find-in-file-worker-d0f0ff069004.js","githubDevUrl":null,"enabled_features":{"code_nav_ui_events":false,"react_blob_overlay":false,"copilot_conversational_ux_embedding_update":false,"copilot_popover_file_editor_header":true,"copilot_smell_icebreaker_ux":true,"copilot_workspace":false}}}}
             </script>
             <div data-target="react-partial.reactRoot">
              <style data-styled="true" data-styled-version="5.3.6">
               .cgQnMS{font-weight:600;font-size:32px;margin:0;}/*!sc*/
data-styled.g1[id="Heading__StyledHeading-sc-1c1dgg0-0"]{content:"cgQnMS,"}/*!sc*/
.izjvBm{margin-top:16px;margin-bottom:16px;}/*!sc*/
.rPQgy{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;}/*!sc*/
.eUMEDg{margin-bottom:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;row-gap:16px;}/*!sc*/
.eLcVee{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;padding-bottom:16px;padding-top:8px;}/*!sc*/
.hsfLlq{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;gap:8px;}/*!sc*/
@media screen and (max-width:320px){.hsfLlq{-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;}}/*!sc*/
.gpKoUz{position:relative;}/*!sc*/
@media screen and (max-width:380px){.gpKoUz .ref-selector-button-text-container{max-width:80px;}}/*!sc*/
@media screen and (max-width:320px){.gpKoUz{-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;}.gpKoUz .overview-ref-selector{width:100%;}.gpKoUz .overview-ref-selector > span{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:start;-webkit-justify-content:flex-start;-ms-flex-pack:start;justify-content:flex-start;}.gpKoUz .overview-ref-selector > span > span[data-component="text"]{-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;}}/*!sc*/
.kkrdEu{-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;}/*!sc*/
.bKgizp{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;width:100%;}/*!sc*/
.iPGYsi{margin-right:4px;color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/
.dKmYfk{font-size:14px;min-width:0;max-width:125px;overflow:hidden;text-overflow:ellipsis;white-space:nowrap;}/*!sc*/
.trpoQ{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;pointer-events:none;}/*!sc*/
.laYubZ{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}/*!sc*/
@media screen and (max-width:1079px){.laYubZ{display:none;}}/*!sc*/
.swnaL{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}/*!sc*/
@media screen and (min-width:1080px){.swnaL{display:none;}}/*!sc*/
@media screen and (max-width:543px){.swnaL{display:none;}}/*!sc*/
.bWpuBf{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;padding-left:8px;gap:8px;}/*!sc*/
.grHjNb{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;gap:8px;}/*!sc*/
@media screen and (max-width:543px){.grHjNb{display:none;}}/*!sc*/
.dXTsqj{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}/*!sc*/
@media screen and (max-width:1011px){.dXTsqj{display:none;}}/*!sc*/
.dCOrmu{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}/*!sc*/
@media screen and (min-width:1012px){.dCOrmu{display:none;}}/*!sc*/
@media screen and (max-width:544px){.bVvbgP{display:none;}}/*!sc*/
.bNDvfp{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}/*!sc*/
@media screen and (min-width:544px){.bNDvfp{display:none;}}/*!sc*/
.vhgA{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;gap:8px;background-color:var(--bgColor-muted,var(--color-canvas-subtle,#f6f8fa));font-size:14px;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;border:solid 1px;border-color:var(--borderColor-default,var(--color-border-default,#d0d7de));border-radius:6px;padding-left:16px;padding-right:8px;padding-top:8px;padding-bottom:8px;margin-bottom:16px;}/*!sc*/
.yfPnm{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;gap:16px;}/*!sc*/
.cAQuiW{width:100%;border-collapse:separate;border-spacing:0;border:1px solid;border-color:var(--borderColor-default,var(--color-border-default,#d0d7de));border-radius:6px;table-layout:fixed;overflow:unset;}/*!sc*/
.iiUlLN{height:0px;line-height:0px;}/*!sc*/
.iiUlLN tr{height:0px;font-size:0px;}/*!sc*/
.jmggSN{padding:16px;color:var(--fgColor-muted,var(--color-fg-muted,#656d76));font-size:12px;text-align:left;height:40px;}/*!sc*/
.jmggSN th{padding-left:16px;background-color:var(--bgColor-muted,var(--color-canvas-subtle,#f6f8fa));}/*!sc*/
.kvYunM{width:100%;border-top-left-radius:6px;}/*!sc*/
@media screen and (min-width:544px){.kvYunM{display:none;}}/*!sc*/
.hrLuxA{width:40%;border-top-left-radius:6px;}/*!sc*/
@media screen and (max-width:543px){.hrLuxA{display:none;}}/*!sc*/
@media screen and (max-width:543px){.ePjhhA{display:none;}}/*!sc*/
.cuEKae{text-align:right;padding-right:16px;width:136px;border-top-right-radius:6px;}/*!sc*/
.jEbBOT{color:var(--fgColor-muted,var(--color-fg-muted,#656d76));font-size:12px;height:40px;}/*!sc*/
.bTxCvM{background-color:var(--bgColor-muted,var(--color-canvas-subtle,#f6f8fa));padding:4px;border-top-left-radius:6px;border-top-right-radius:6px;}/*!sc*/
.eYedVD{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;gap:8px;min-width:273px;padding-right:8px;padding-left:16px;padding-top:8px;padding-bottom:8px;}/*!sc*/
.lhFvfi{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.jGfYmh{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;gap:8px;}/*!sc*/
.bqgLjk{display:inherit;}/*!sc*/
@media screen and (min-width:544px){.bqgLjk{display:none;}}/*!sc*/
@media screen and (min-width:768px){.bqgLjk{display:none;}}/*!sc*/
.epsqEd{text-align:center;vertical-align:center;height:40px;border-top:1px solid;border-color:var(--borderColor-default,var(--color-border-default,#d0d7de));}/*!sc*/
.ldpruc{border-top:1px solid var(--borderColor-default,var(--color-border-default));cursor:pointer;}/*!sc*/
.ehcSsh{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;gap:16px;}/*!sc*/
.iGmlUb{border:1px solid;border-color:var(--borderColor-default,var(--color-border-default,#d0d7de));border-radius:6px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;}/*!sc*/
@media screen and (max-width:543px){.iGmlUb{margin-left:-16px;margin-right:-16px;max-width:calc(100% + 32px);}}/*!sc*/
@media screen and (min-width:544px){.iGmlUb{max-width:100%;}}/*!sc*/
.iRQGXA{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;border-bottom:1px solid;border-bottom-color:var(--borderColor-default,var(--color-border-default,#d0d7de));-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;padding-right:8px;position:-webkit-sticky;position:sticky;top:0;background-color:var(--bgColor-default,var(--color-canvas-default,#ffffff));z-index:1;border-top-left-radius:6px;border-top-right-radius:6px;}/*!sc*/
.dvTdPK{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;padding-left:8px;padding-right:8px;-webkit-box-pack:start;-webkit-justify-content:flex-start;-ms-flex-pack:start;justify-content:flex-start;border-bottom:none;border-bottom-color:var(--borderColor-muted,var(--color-border-muted,hsla(210,18%,87%,1)));align:row;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;min-height:48px;-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;max-width:100%;}/*!sc*/
.gwuIGu{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.kOxwQs{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;margin-right:8px;}/*!sc*/
.kOgeFj{font-weight:600;}/*!sc*/
.bJMeLZ{padding:32px;overflow:auto;}/*!sc*/
data-styled.g2[id="Box-sc-g0xbh4-0"]{content:"izjvBm,rPQgy,eUMEDg,eLcVee,hsfLlq,gpKoUz,kkrdEu,bKgizp,iPGYsi,dKmYfk,trpoQ,laYubZ,swnaL,bWpuBf,grHjNb,dXTsqj,dCOrmu,bVvbgP,bNDvfp,vhgA,yfPnm,cAQuiW,iiUlLN,jmggSN,kvYunM,hrLuxA,ePjhhA,cuEKae,jEbBOT,bTxCvM,eYedVD,lhFvfi,jGfYmh,bqgLjk,epsqEd,ldpruc,ehcSsh,iGmlUb,iRQGXA,dvTdPK,gwuIGu,kOxwQs,kOgeFj,bJMeLZ,"}/*!sc*/
.bOMzPg{min-width:0;}/*!sc*/
.eUGNHp{font-weight:600;}/*!sc*/
.dALsKK{color:var(--fgColor-default,var(--color-fg-default,#1F2328));}/*!sc*/
data-styled.g6[id="Text-sc-17v1xeu-0"]{content:"bOMzPg,eUGNHp,dALsKK,"}/*!sc*/
.dheQRw{color:var(--fgColor-accent,var(--color-accent-fg,#0969da));-webkit-text-decoration:none;text-decoration:none;}/*!sc*/
[data-a11y-link-underlines='true'] .Link__StyledLink-sc-14289xe-0[data-inline='true']{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.dheQRw:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.dheQRw:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.vLMkZ{color:var(--fgColor-accent,var(--color-accent-fg,#0969da));-webkit-text-decoration:none;text-decoration:none;position:relative;display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;color:var(--fgColor-default,var(--color-fg-default,#1F2328));text-align:center;-webkit-text-decoration:none;text-decoration:none;line-height:calc(20/14);border-radius:6px;font-size:14px;padding-left:8px;padding-right:8px;padding-top:calc((2rem - 1.25rem) / 2);padding-bottom:calc((2rem - 1.25rem) / 2);}/*!sc*/
[data-a11y-link-underlines='true'] .Link__StyledLink-sc-14289xe-0[data-inline='true']{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.vLMkZ:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.vLMkZ:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.vLMkZ span[data-component="icon"]{color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/
@media (hover:hover){.vLMkZ:hover{background-color:var(--bgColor-neutral-muted,var(--color-neutral-muted,rgba(175,184,193,0.2)));-webkit-transition:background .12s ease-out;transition:background .12s ease-out;-webkit-text-decoration:none;text-decoration:none;}}/*!sc*/
.vLMkZ:focus{outline:2px solid transparent;}/*!sc*/
.vLMkZ:focus{box-shadow:inset 0 0 0 2px var(--fgColor-accent,var(--color-accent-fg,#0969da));}/*!sc*/
.vLMkZ:focus:not(:focus-visible){box-shadow:none;}/*!sc*/
.vLMkZ:focus-visible{outline:2px solid transparent;box-shadow:inset 0 0 0 2px var(--fgColor-accent,var(--color-accent-fg,#0969da));}/*!sc*/
.vLMkZ span[data-content]::before{content:attr(data-content);display:block;height:0;font-weight:600;visibility:hidden;white-space:nowrap;}/*!sc*/
.vLMkZ::after{position:absolute;right:50%;bottom:calc(50% - 25px);width:100%;height:2px;content:"";background-color:var(--underlineNav-borderColor-active,var(--color-primer-border-active,#fd8c73));border-radius:0;-webkit-transform:translate(50%,-50%);-ms-transform:translate(50%,-50%);transform:translate(50%,-50%);}/*!sc*/
@media (forced-colors:active){.vLMkZ::after{background-color:LinkText;}}/*!sc*/
data-styled.g8[id="Link__StyledLink-sc-14289xe-0"]{content:"dheQRw,vLMkZ,"}/*!sc*/
.hILMMl{border-radius:6px;border:1px solid;border-color:var(--button-default-borderColor-rest,var(--button-default-borderColor-rest,var(--color-btn-border,rgba(31,35,40,0.15))));font-family:inherit;font-weight:500;font-size:14px;cursor:pointer;-webkit-appearance:none;-moz-appearance:none;appearance:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;-webkit-text-decoration:none;text-decoration:none;text-align:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;height:32px;padding:0 12px;gap:8px;min-width:-webkit-max-content;min-width:-moz-max-content;min-width:max-content;-webkit-transition:80ms cubic-bezier(0.65,0,0.35,1);transition:80ms cubic-bezier(0.65,0,0.35,1);-webkit-transition-property:color,fill,background-color,border-color;transition-property:color,fill,background-color,border-color;color:var(--button-default-fgColor-rest,var(--color-btn-text,#24292f));background-color:var(--button-default-bgColor-rest,var(--color-btn-bg,#f6f8fa));box-shadow:var(--button-default-shadow-resting,var(--color-btn-shadow,0 1px 0 rgba(31,35,40,0.04))),var(--button-default-shadow-inset,var(--color-btn-inset-shadow,inset 0 1px 0 rgba(255,255,255,0.25)));}/*!sc*/
.hILMMl:focus:not(:disabled){box-shadow:none;outline:2px solid var(--fgColor-accent,var(--color-accent-fg,#0969da));outline-offset:-2px;}/*!sc*/
.hILMMl:focus:not(:disabled):not(:focus-visible){outline:solid 1px transparent;}/*!sc*/
.hILMMl:focus-visible:not(:disabled){box-shadow:none;outline:2px solid var(--fgColor-accent,var(--color-accent-fg,#0969da));outline-offset:-2px;}/*!sc*/
.hILMMl[href]{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;}/*!sc*/
.hILMMl[href]:hover{-webkit-text-decoration:none;text-decoration:none;}/*!sc*/
.hILMMl:hover{-webkit-transition-duration:80ms;transition-duration:80ms;}/*!sc*/
.hILMMl:active{-webkit-transition:none;transition:none;}/*!sc*/
.hILMMl[data-inactive]{cursor:auto;}/*!sc*/
.hILMMl:disabled{cursor:not-allowed;box-shadow:none;color:var(--fgColor-disabled,var(--color-primer-fg-disabled,#8c959f));border-color:var(--button-default-borderColor-disabled,var(--button-default-borderColor-rest,var(--color-btn-border,rgba(31,35,40,0.15))));background-color:var(--button-default-bgColor-disabled,var(--control-bgColor-disabled,var(--color-input-disabled-bg,rgba(175,184,193,0.2))));}/*!sc*/
.hILMMl:disabled [data-component=ButtonCounter]{color:inherit;}/*!sc*/
@media (forced-colors:active){.hILMMl:focus{outline:solid 1px transparent;}}/*!sc*/
.hILMMl [data-component=ButtonCounter]{font-size:12px;background-color:var(--buttonCounter-default-bgColor-rest,var(--color-btn-counter-bg,rgba(31,35,40,0.08)));}/*!sc*/
.hILMMl[data-component=IconButton]{display:inline-grid;padding:unset;place-content:center;width:32px;min-width:unset;}/*!sc*/
.hILMMl[data-size="small"]{padding:0 8px;height:28px;gap:4px;font-size:12px;}/*!sc*/
.hILMMl[data-size="small"] [data-component="text"]{line-height:calc(20 / 12);}/*!sc*/
.hILMMl[data-size="small"] [data-component=ButtonCounter]{font-size:12px;}/*!sc*/
.hILMMl[data-size="small"] [data-component="buttonContent"] > :not(:last-child){margin-right:4px;}/*!sc*/
.hILMMl[data-size="small"][data-component=IconButton]{width:28px;padding:unset;}/*!sc*/
.hILMMl[data-size="large"]{padding:0 16px;height:40px;gap:8px;}/*!sc*/
.hILMMl[data-size="large"] [data-component="buttonContent"] > :not(:last-child){margin-right:8px;}/*!sc*/
.hILMMl[data-size="large"][data-component=IconButton]{width:40px;padding:unset;}/*!sc*/
.hILMMl[data-block="block"]{width:100%;}/*!sc*/
.hILMMl[data-inactive]:not([disabled]){background-color:var(--button-inactive-bgColor,var(--button-inactive-bgColor-rest,var(--color-btn-inactive-bg,#eaeef2)));border-color:var(--button-inactive-bgColor,var(--button-inactive-bgColor-rest,var(--color-btn-inactive-bg,#eaeef2)));color:var(--button-inactive-fgColor,var(--button-inactive-fgColor-rest,var(--color-btn-inactive-text,#57606a)));}/*!sc*/
.hILMMl[data-inactive]:not([disabled]):focus-visible{box-shadow:none;}/*!sc*/
.hILMMl [data-component="leadingVisual"]{grid-area:leadingVisual;}/*!sc*/
.hILMMl [data-component="text"]{grid-area:text;line-height:calc(20/14);white-space:nowrap;}/*!sc*/
.hILMMl [data-component="trailingVisual"]{grid-area:trailingVisual;}/*!sc*/
.hILMMl [data-component="trailingAction"]{margin-right:-4px;}/*!sc*/
.hILMMl [data-component="buttonContent"]{-webkit-flex:1 0 auto;-ms-flex:1 0 auto;flex:1 0 auto;display:grid;grid-template-areas:"leadingVisual text trailingVisual";grid-template-columns:min-content minmax(0,auto) min-content;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-align-content:center;-ms-flex-line-pack:center;align-content:center;}/*!sc*/
.hILMMl [data-component="buttonContent"] > :not(:last-child){margin-right:8px;}/*!sc*/
.hILMMl:hover:not([disabled]):not([data-inactive]){background-color:var(--button-default-bgColor-hover,var(--color-btn-hover-bg,#f3f4f6));border-color:var(--button-default-borderColor-hover,var(--button-default-borderColor-hover,var(--color-btn-hover-border,rgba(31,35,40,0.15))));}/*!sc*/
.hILMMl:active:not([disabled]):not([data-inactive]){background-color:var(--button-default-bgColor-active,var(--color-btn-active-bg,hsla(220,14%,93%,1)));border-color:var(--button-default-borderColor-active,var(--button-default-borderColor-active,var(--color-btn-active-border,rgba(31,35,40,0.15))));}/*!sc*/
.hILMMl[aria-expanded=true]{background-color:var(--button-default-bgColor-active,var(--color-btn-active-bg,hsla(220,14%,93%,1)));border-color:var(--button-default-borderColor-active,var(--button-default-borderColor-active,var(--color-btn-active-border,rgba(31,35,40,0.15))));}/*!sc*/
.hILMMl [data-component="leadingVisual"],.hILMMl [data-component="trailingVisual"],.hILMMl [data-component="trailingAction"]{color:var(--button-color,var(--fgColor-muted,var(--color-fg-muted,#656d76)));}/*!sc*/
.hILMMl[data-component="IconButton"][data-no-visuals]{color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/
.hILMMl{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}/*!sc*/
.hILMMl svg{color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/
.hILMMl > span{width:inherit;}/*!sc*/
.cuOWTR{border-radius:6px;border:1px solid;border-color:transparent;font-family:inherit;font-weight:500;font-size:14px;cursor:pointer;-webkit-appearance:none;-moz-appearance:none;appearance:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;-webkit-text-decoration:none;text-decoration:none;text-align:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;height:32px;padding:0 12px;gap:8px;min-width:-webkit-max-content;min-width:-moz-max-content;min-width:max-content;-webkit-transition:80ms cubic-bezier(0.65,0,0.35,1);transition:80ms cubic-bezier(0.65,0,0.35,1);-webkit-transition-property:color,fill,background-color,border-color;transition-property:color,fill,background-color,border-color;color:var(--button-default-fgColor-rest,var(--color-btn-text,#24292f));background-color:transparent;box-shadow:none;}/*!sc*/
.cuOWTR:focus:not(:disabled){box-shadow:none;outline:2px solid var(--fgColor-accent,var(--color-accent-fg,#0969da));outline-offset:-2px;}/*!sc*/
.cuOWTR:focus:not(:disabled):not(:focus-visible){outline:solid 1px transparent;}/*!sc*/
.cuOWTR:focus-visible:not(:disabled){box-shadow:none;outline:2px solid var(--fgColor-accent,var(--color-accent-fg,#0969da));outline-offset:-2px;}/*!sc*/
.cuOWTR[href]{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;}/*!sc*/
.cuOWTR[href]:hover{-webkit-text-decoration:none;text-decoration:none;}/*!sc*/
.cuOWTR:hover{-webkit-transition-duration:80ms;transition-duration:80ms;}/*!sc*/
.cuOWTR:active{-webkit-transition:none;transition:none;}/*!sc*/
.cuOWTR[data-inactive]{cursor:auto;}/*!sc*/
.cuOWTR:disabled{cursor:not-allowed;box-shadow:none;color:var(--fgColor-disabled,var(--color-primer-fg-disabled,#8c959f));}/*!sc*/
.cuOWTR:disabled [data-component=ButtonCounter],.cuOWTR:disabled [data-component="leadingVisual"],.cuOWTR:disabled [data-component="trailingAction"]{color:inherit;}/*!sc*/
@media (forced-colors:active){.cuOWTR:focus{outline:solid 1px transparent;}}/*!sc*/
.cuOWTR [data-component=ButtonCounter]{font-size:12px;}/*!sc*/
.cuOWTR[data-component=IconButton]{display:inline-grid;padding:unset;place-content:center;width:32px;min-width:unset;}/*!sc*/
.cuOWTR[data-size="small"]{padding:0 8px;height:28px;gap:4px;font-size:12px;}/*!sc*/
.cuOWTR[data-size="small"] [data-component="text"]{line-height:calc(20 / 12);}/*!sc*/
.cuOWTR[data-size="small"] [data-component=ButtonCounter]{font-size:12px;}/*!sc*/
.cuOWTR[data-size="small"] [data-component="buttonContent"] > :not(:last-child){margin-right:4px;}/*!sc*/
.cuOWTR[data-size="small"][data-component=IconButton]{width:28px;padding:unset;}/*!sc*/
.cuOWTR[data-size="large"]{padding:0 16px;height:40px;gap:8px;}/*!sc*/
.cuOWTR[data-size="large"] [data-component="buttonContent"] > :not(:last-child){margin-right:8px;}/*!sc*/
.cuOWTR[data-size="large"][data-component=IconButton]{width:40px;padding:unset;}/*!sc*/
.cuOWTR[data-block="block"]{width:100%;}/*!sc*/
.cuOWTR[data-inactive]:not([disabled]){background-color:var(--button-inactive-bgColor,var(--button-inactive-bgColor-rest,var(--color-btn-inactive-bg,#eaeef2)));border-color:var(--button-inactive-bgColor,var(--button-inactive-bgColor-rest,var(--color-btn-inactive-bg,#eaeef2)));color:var(--button-inactive-fgColor,var(--button-inactive-fgColor-rest,var(--color-btn-inactive-text,#57606a)));}/*!sc*/
.cuOWTR[data-inactive]:not([disabled]):focus-visible{box-shadow:none;}/*!sc*/
.cuOWTR [data-component="leadingVisual"]{grid-area:leadingVisual;color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/
.cuOWTR [data-component="text"]{grid-area:text;line-height:calc(20/14);white-space:nowrap;}/*!sc*/
.cuOWTR [data-component="trailingVisual"]{grid-area:trailingVisual;}/*!sc*/
.cuOWTR [data-component="trailingAction"]{margin-right:-4px;color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/
.cuOWTR [data-component="buttonContent"]{-webkit-flex:1 0 auto;-ms-flex:1 0 auto;flex:1 0 auto;display:grid;grid-template-areas:"leadingVisual text trailingVisual";grid-template-columns:min-content minmax(0,auto) min-content;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-align-content:center;-ms-flex-line-pack:center;align-content:center;}/*!sc*/
.cuOWTR [data-component="buttonContent"] > :not(:last-child){margin-right:8px;}/*!sc*/
.cuOWTR:hover:not([disabled]){background-color:var(--control-transparent-bgColor-hover,var(--color-action-list-item-default-hover-bg,rgba(208,215,222,0.32)));}/*!sc*/
.cuOWTR:active:not([disabled]){background-color:var(--control-transparent-bgColor-active,var(--color-action-list-item-default-active-bg,rgba(208,215,222,0.48)));}/*!sc*/
.cuOWTR[aria-expanded=true]{background-color:var(--control-transparent-bgColor-selected,var(--color-action-list-item-default-selected-bg,rgba(208,215,222,0.24)));}/*!sc*/
.cuOWTR[data-component="IconButton"][data-no-visuals]{color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/
.cuOWTR[data-no-visuals]{color:var(--fgColor-accent,var(--color-accent-fg,#0969da));}/*!sc*/
.cuOWTR:has([data-component="ButtonCounter"]){color:var(--button-default-fgColor-rest,var(--color-btn-text,#24292f));}/*!sc*/
.cuOWTR:disabled[data-no-visuals]{color:var(--fgColor-disabled,var(--color-primer-fg-disabled,#8c959f));}/*!sc*/
.cuOWTR:disabled[data-no-visuals] [data-component=ButtonCounter]{color:inherit;}/*!sc*/
.cuOWTR{color:var(--fgColor-muted,var(--color-fg-muted,#656d76));padding-left:4px;padding-right:4px;}/*!sc*/
.cuOWTR span[data-component="leadingVisual"]{margin-right:4px !important;}/*!sc*/
.tDSzd{border-radius:6px;border:1px solid;border-color:transparent;font-family:inherit;font-weight:500;font-size:14px;cursor:pointer;-webkit-appearance:none;-moz-appearance:none;appearance:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;-webkit-text-decoration:none;text-decoration:none;text-align:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;height:32px;padding:0 12px;gap:8px;min-width:-webkit-max-content;min-width:-moz-max-content;min-width:max-content;-webkit-transition:80ms cubic-bezier(0.65,0,0.35,1);transition:80ms cubic-bezier(0.65,0,0.35,1);-webkit-transition-property:color,fill,background-color,border-color;transition-property:color,fill,background-color,border-color;color:var(--button-default-fgColor-rest,var(--color-btn-text,#24292f));background-color:transparent;box-shadow:none;}/*!sc*/
.tDSzd:focus:not(:disabled){box-shadow:none;outline:2px solid var(--fgColor-accent,var(--color-accent-fg,#0969da));outline-offset:-2px;}/*!sc*/
.tDSzd:focus:not(:disabled):not(:focus-visible){outline:solid 1px transparent;}/*!sc*/
.tDSzd:focus-visible:not(:disabled){box-shadow:none;outline:2px solid var(--fgColor-accent,var(--color-accent-fg,#0969da));outline-offset:-2px;}/*!sc*/
.tDSzd[href]{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;}/*!sc*/
.tDSzd[href]:hover{-webkit-text-decoration:none;text-decoration:none;}/*!sc*/
.tDSzd:hover{-webkit-transition-duration:80ms;transition-duration:80ms;}/*!sc*/
.tDSzd:active{-webkit-transition:none;transition:none;}/*!sc*/
.tDSzd[data-inactive]{cursor:auto;}/*!sc*/
.tDSzd:disabled{cursor:not-allowed;box-shadow:none;color:var(--fgColor-disabled,var(--color-primer-fg-disabled,#8c959f));}/*!sc*/
.tDSzd:disabled [data-component=ButtonCounter],.tDSzd:disabled [data-component="leadingVisual"],.tDSzd:disabled [data-component="trailingAction"]{color:inherit;}/*!sc*/
@media (forced-colors:active){.tDSzd:focus{outline:solid 1px transparent;}}/*!sc*/
.tDSzd [data-component=ButtonCounter]{font-size:12px;}/*!sc*/
.tDSzd[data-component=IconButton]{display:inline-grid;padding:unset;place-content:center;width:32px;min-width:unset;}/*!sc*/
.tDSzd[data-size="small"]{padding:0 8px;height:28px;gap:4px;font-size:12px;}/*!sc*/
.tDSzd[data-size="small"] [data-component="text"]{line-height:calc(20 / 12);}/*!sc*/
.tDSzd[data-size="small"] [data-component=ButtonCounter]{font-size:12px;}/*!sc*/
.tDSzd[data-size="small"] [data-component="buttonContent"] > :not(:last-child){margin-right:4px;}/*!sc*/
.tDSzd[data-size="small"][data-component=IconButton]{width:28px;padding:unset;}/*!sc*/
.tDSzd[data-size="large"]{padding:0 16px;height:40px;gap:8px;}/*!sc*/
.tDSzd[data-size="large"] [data-component="buttonContent"] > :not(:last-child){margin-right:8px;}/*!sc*/
.tDSzd[data-size="large"][data-component=IconButton]{width:40px;padding:unset;}/*!sc*/
.tDSzd[data-block="block"]{width:100%;}/*!sc*/
.tDSzd[data-inactive]:not([disabled]){background-color:var(--button-inactive-bgColor,var(--button-inactive-bgColor-rest,var(--color-btn-inactive-bg,#eaeef2)));border-color:var(--button-inactive-bgColor,var(--button-inactive-bgColor-rest,var(--color-btn-inactive-bg,#eaeef2)));color:var(--button-inactive-fgColor,var(--button-inactive-fgColor-rest,var(--color-btn-inactive-text,#57606a)));}/*!sc*/
.tDSzd[data-inactive]:not([disabled]):focus-visible{box-shadow:none;}/*!sc*/
.tDSzd [data-component="leadingVisual"]{grid-area:leadingVisual;color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/
.tDSzd [data-component="text"]{grid-area:text;line-height:calc(20/14);white-space:nowrap;}/*!sc*/
.tDSzd [data-component="trailingVisual"]{grid-area:trailingVisual;}/*!sc*/
.tDSzd [data-component="trailingAction"]{margin-right:-4px;color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/
.tDSzd [data-component="buttonContent"]{-webkit-flex:1 0 auto;-ms-flex:1 0 auto;flex:1 0 auto;display:grid;grid-template-areas:"leadingVisual text trailingVisual";grid-template-columns:min-content minmax(0,auto) min-content;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-align-content:center;-ms-flex-line-pack:center;align-content:center;}/*!sc*/
.tDSzd [data-component="buttonContent"] > :not(:last-child){margin-right:8px;}/*!sc*/
.tDSzd:hover:not([disabled]){background-color:var(--control-transparent-bgColor-hover,var(--color-action-list-item-default-hover-bg,rgba(208,215,222,0.32)));}/*!sc*/
.tDSzd:active:not([disabled]){background-color:var(--control-transparent-bgColor-active,var(--color-action-list-item-default-active-bg,rgba(208,215,222,0.48)));}/*!sc*/
.tDSzd[aria-expanded=true]{background-color:var(--control-transparent-bgColor-selected,var(--color-action-list-item-default-selected-bg,rgba(208,215,222,0.24)));}/*!sc*/
.tDSzd[data-component="IconButton"][data-no-visuals]{color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/
.tDSzd[data-no-visuals]{color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/
.tDSzd:has([data-component="ButtonCounter"]){color:var(--button-default-fgColor-rest,var(--color-btn-text,#24292f));}/*!sc*/
.tDSzd:disabled[data-no-visuals]{color:var(--fgColor-disabled,var(--color-primer-fg-disabled,#8c959f));}/*!sc*/
.tDSzd:disabled[data-no-visuals] [data-component=ButtonCounter]{color:inherit;}/*!sc*/
.pyyxt{border-radius:6px;border:1px solid;border-color:var(--button-default-borderColor-rest,var(--button-default-borderColor-rest,var(--color-btn-border,rgba(31,35,40,0.15))));font-family:inherit;font-weight:500;font-size:14px;cursor:pointer;-webkit-appearance:none;-moz-appearance:none;appearance:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;-webkit-text-decoration:none;text-decoration:none;text-align:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;height:32px;padding:0 12px;gap:8px;min-width:-webkit-max-content;min-width:-moz-max-content;min-width:max-content;-webkit-transition:80ms cubic-bezier(0.65,0,0.35,1);transition:80ms cubic-bezier(0.65,0,0.35,1);-webkit-transition-property:color,fill,background-color,border-color;transition-property:color,fill,background-color,border-color;color:var(--button-default-fgColor-rest,var(--color-btn-text,#24292f));background-color:var(--button-default-bgColor-rest,var(--color-btn-bg,#f6f8fa));box-shadow:var(--button-default-shadow-resting,var(--color-btn-shadow,0 1px 0 rgba(31,35,40,0.04))),var(--button-default-shadow-inset,var(--color-btn-inset-shadow,inset 0 1px 0 rgba(255,255,255,0.25)));}/*!sc*/
.pyyxt:focus:not(:disabled){box-shadow:none;outline:2px solid var(--fgColor-accent,var(--color-accent-fg,#0969da));outline-offset:-2px;}/*!sc*/
.pyyxt:focus:not(:disabled):not(:focus-visible){outline:solid 1px transparent;}/*!sc*/
.pyyxt:focus-visible:not(:disabled){box-shadow:none;outline:2px solid var(--fgColor-accent,var(--color-accent-fg,#0969da));outline-offset:-2px;}/*!sc*/
.pyyxt[href]{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;}/*!sc*/
.pyyxt[href]:hover{-webkit-text-decoration:none;text-decoration:none;}/*!sc*/
.pyyxt:hover{-webkit-transition-duration:80ms;transition-duration:80ms;}/*!sc*/
.pyyxt:active{-webkit-transition:none;transition:none;}/*!sc*/
.pyyxt[data-inactive]{cursor:auto;}/*!sc*/
.pyyxt:disabled{cursor:not-allowed;box-shadow:none;color:var(--fgColor-disabled,var(--color-primer-fg-disabled,#8c959f));border-color:var(--button-default-borderColor-disabled,var(--button-default-borderColor-rest,var(--color-btn-border,rgba(31,35,40,0.15))));background-color:var(--button-default-bgColor-disabled,var(--control-bgColor-disabled,var(--color-input-disabled-bg,rgba(175,184,193,0.2))));}/*!sc*/
.pyyxt:disabled [data-component=ButtonCounter]{color:inherit;}/*!sc*/
@media (forced-colors:active){.pyyxt:focus{outline:solid 1px transparent;}}/*!sc*/
.pyyxt [data-component=ButtonCounter]{font-size:12px;background-color:var(--buttonCounter-default-bgColor-rest,var(--color-btn-counter-bg,rgba(31,35,40,0.08)));}/*!sc*/
.pyyxt[data-component=IconButton]{display:inline-grid;padding:unset;place-content:center;width:32px;min-width:unset;}/*!sc*/
.pyyxt[data-size="small"]{padding:0 8px;height:28px;gap:4px;font-size:12px;}/*!sc*/
.pyyxt[data-size="small"] [data-component="text"]{line-height:calc(20 / 12);}/*!sc*/
.pyyxt[data-size="small"] [data-component=ButtonCounter]{font-size:12px;}/*!sc*/
.pyyxt[data-size="small"] [data-component="buttonContent"] > :not(:last-child){margin-right:4px;}/*!sc*/
.pyyxt[data-size="small"][data-component=IconButton]{width:28px;padding:unset;}/*!sc*/
.pyyxt[data-size="large"]{padding:0 16px;height:40px;gap:8px;}/*!sc*/
.pyyxt[data-size="large"] [data-component="buttonContent"] > :not(:last-child){margin-right:8px;}/*!sc*/
.pyyxt[data-size="large"][data-component=IconButton]{width:40px;padding:unset;}/*!sc*/
.pyyxt[data-block="block"]{width:100%;}/*!sc*/
.pyyxt[data-inactive]:not([disabled]){background-color:var(--button-inactive-bgColor,var(--button-inactive-bgColor-rest,var(--color-btn-inactive-bg,#eaeef2)));border-color:var(--button-inactive-bgColor,var(--button-inactive-bgColor-rest,var(--color-btn-inactive-bg,#eaeef2)));color:var(--button-inactive-fgColor,var(--button-inactive-fgColor-rest,var(--color-btn-inactive-text,#57606a)));}/*!sc*/
.pyyxt[data-inactive]:not([disabled]):focus-visible{box-shadow:none;}/*!sc*/
.pyyxt [data-component="leadingVisual"]{grid-area:leadingVisual;}/*!sc*/
.pyyxt [data-component="text"]{grid-area:text;line-height:calc(20/14);white-space:nowrap;}/*!sc*/
.pyyxt [data-component="trailingVisual"]{grid-area:trailingVisual;}/*!sc*/
.pyyxt [data-component="trailingAction"]{margin-right:-4px;}/*!sc*/
.pyyxt [data-component="buttonContent"]{-webkit-flex:1 0 auto;-ms-flex:1 0 auto;flex:1 0 auto;display:grid;grid-template-areas:"leadingVisual text trailingVisual";grid-template-columns:min-content minmax(0,auto) min-content;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-align-content:center;-ms-flex-line-pack:center;align-content:center;}/*!sc*/
.pyyxt [data-component="buttonContent"] > :not(:last-child){margin-right:8px;}/*!sc*/
.pyyxt:hover:not([disabled]):not([data-inactive]){background-color:var(--button-default-bgColor-hover,var(--color-btn-hover-bg,#f3f4f6));border-color:var(--button-default-borderColor-hover,var(--button-default-borderColor-hover,var(--color-btn-hover-border,rgba(31,35,40,0.15))));}/*!sc*/
.pyyxt:active:not([disabled]):not([data-inactive]){background-color:var(--button-default-bgColor-active,var(--color-btn-active-bg,hsla(220,14%,93%,1)));border-color:var(--button-default-borderColor-active,var(--button-default-borderColor-active,var(--color-btn-active-border,rgba(31,35,40,0.15))));}/*!sc*/
.pyyxt[aria-expanded=true]{background-color:var(--button-default-bgColor-active,var(--color-btn-active-bg,hsla(220,14%,93%,1)));border-color:var(--button-default-borderColor-active,var(--button-default-borderColor-active,var(--color-btn-active-border,rgba(31,35,40,0.15))));}/*!sc*/
.pyyxt [data-component="leadingVisual"],.pyyxt [data-component="trailingVisual"],.pyyxt [data-component="trailingAction"]{color:var(--button-color,var(--fgColor-muted,var(--color-fg-muted,#656d76)));}/*!sc*/
.pyyxt[data-component="IconButton"][data-no-visuals]{color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/
.gYvpXq{border-radius:6px;border:1px solid;border-color:var(--button-primary-borderColor-rest,var(--color-btn-primary-border,rgba(31,35,40,0.15)));font-family:inherit;font-weight:500;font-size:14px;cursor:pointer;-webkit-appearance:none;-moz-appearance:none;appearance:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;-webkit-text-decoration:none;text-decoration:none;text-align:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;height:32px;padding:0 12px;gap:8px;min-width:-webkit-max-content;min-width:-moz-max-content;min-width:max-content;-webkit-transition:80ms cubic-bezier(0.65,0,0.35,1);transition:80ms cubic-bezier(0.65,0,0.35,1);-webkit-transition-property:color,fill,background-color,border-color;transition-property:color,fill,background-color,border-color;color:var(--button-primary-fgColor-rest,var(--color-btn-primary-text,#ffffff));background-color:var(--button-primary-bgColor-rest,var(--color-btn-primary-bg,#1f883d));box-shadow:var(--shadow-resting-small,var(--color-btn-primary-shadow,0 1px 0 rgba(31,35,40,0.1)));}/*!sc*/
.gYvpXq:focus:not(:disabled){box-shadow:none;outline:2px solid var(--fgColor-accent,var(--color-accent-fg,#0969da));outline-offset:-2px;}/*!sc*/
.gYvpXq:focus:not(:disabled):not(:focus-visible){outline:solid 1px transparent;}/*!sc*/
.gYvpXq:focus-visible:not(:disabled){box-shadow:none;outline:2px solid var(--fgColor-accent,var(--color-accent-fg,#0969da));outline-offset:-2px;}/*!sc*/
.gYvpXq[href]{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;}/*!sc*/
.gYvpXq[href]:hover{-webkit-text-decoration:none;text-decoration:none;}/*!sc*/
.gYvpXq:hover{-webkit-transition-duration:80ms;transition-duration:80ms;}/*!sc*/
.gYvpXq:active{-webkit-transition:none;transition:none;}/*!sc*/
.gYvpXq[data-inactive]{cursor:auto;}/*!sc*/
.gYvpXq:disabled{cursor:not-allowed;box-shadow:none;color:var(--button-primary-fgColor-disabled,var(--color-btn-primary-disabled-text,rgba(255,255,255,0.8)));background-color:var(--button-primary-bgColor-disabled,var(--color-btn-primary-disabled-bg,#94d3a2));border-color:var(--button-primary-borderColor-disabled,var(--color-btn-primary-disabled-border,rgba(31,35,40,0.15)));}/*!sc*/
.gYvpXq:disabled [data-component=ButtonCounter]{color:inherit;}/*!sc*/
@media (forced-colors:active){.gYvpXq:focus{outline:solid 1px transparent;}}/*!sc*/
.gYvpXq [data-component=ButtonCounter]{font-size:12px;background-color:var(--buttonCounter-primary-bgColor-rest,var(--color-btn-primary-counter-bg,rgba(0,45,17,0.2)));color:var(--button-primary-fgColor-rest,var(--color-btn-primary-text,#ffffff));}/*!sc*/
.gYvpXq[data-component=IconButton]{display:inline-grid;padding:unset;place-content:center;width:32px;min-width:unset;}/*!sc*/
.gYvpXq[data-size="small"]{padding:0 8px;height:28px;gap:4px;font-size:12px;}/*!sc*/
.gYvpXq[data-size="small"] [data-component="text"]{line-height:calc(20 / 12);}/*!sc*/
.gYvpXq[data-size="small"] [data-component=ButtonCounter]{font-size:12px;}/*!sc*/
.gYvpXq[data-size="small"] [data-component="buttonContent"] > :not(:last-child){margin-right:4px;}/*!sc*/
.gYvpXq[data-size="small"][data-component=IconButton]{width:28px;padding:unset;}/*!sc*/
.gYvpXq[data-size="large"]{padding:0 16px;height:40px;gap:8px;}/*!sc*/
.gYvpXq[data-size="large"] [data-component="buttonContent"] > :not(:last-child){margin-right:8px;}/*!sc*/
.gYvpXq[data-size="large"][data-component=IconButton]{width:40px;padding:unset;}/*!sc*/
.gYvpXq[data-block="block"]{width:100%;}/*!sc*/
.gYvpXq[data-inactive]:not([disabled]){background-color:var(--button-inactive-bgColor,var(--button-inactive-bgColor-rest,var(--color-btn-inactive-bg,#eaeef2)));border-color:var(--button-inactive-bgColor,var(--button-inactive-bgColor-rest,var(--color-btn-inactive-bg,#eaeef2)));color:var(--button-inactive-fgColor,var(--button-inactive-fgColor-rest,var(--color-btn-inactive-text,#57606a)));}/*!sc*/
.gYvpXq[data-inactive]:not([disabled]):focus-visible{box-shadow:none;}/*!sc*/
.gYvpXq [data-component="leadingVisual"]{grid-area:leadingVisual;}/*!sc*/
.gYvpXq [data-component="text"]{grid-area:text;line-height:calc(20/14);white-space:nowrap;}/*!sc*/
.gYvpXq [data-component="trailingVisual"]{grid-area:trailingVisual;}/*!sc*/
.gYvpXq [data-component="trailingAction"]{margin-right:-4px;}/*!sc*/
.gYvpXq [data-component="buttonContent"]{-webkit-flex:1 0 auto;-ms-flex:1 0 auto;flex:1 0 auto;display:grid;grid-template-areas:"leadingVisual text trailingVisual";grid-template-columns:min-content minmax(0,auto) min-content;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-align-content:center;-ms-flex-line-pack:center;align-content:center;}/*!sc*/
.gYvpXq [data-component="buttonContent"] > :not(:last-child){margin-right:8px;}/*!sc*/
.gYvpXq:hover:not([disabled]):not([data-inactive]){color:btn.primary.hoverText;background-color:var(--button-primary-bgColor-hover,var(--color-btn-primary-hover-bg,#1a7f37));}/*!sc*/
.gYvpXq:focus:not([disabled]){box-shadow:inset 0 0 0 3px;}/*!sc*/
.gYvpXq:focus-visible:not([disabled]){box-shadow:inset 0 0 0 3px;}/*!sc*/
.gYvpXq:active:not([disabled]):not([data-inactive]){background-color:var(--button-primary-bgColor-active,var(--color-btn-primary-selected-bg,hsla(137,66%,28%,1)));box-shadow:var(--button-primary-shadow-selected,var(--color-btn-primary-selected-shadow,inset 0 1px 0 rgba(0,45,17,0.2)));}/*!sc*/
.gYvpXq[aria-expanded=true]{background-color:var(--button-primary-bgColor-active,var(--color-btn-primary-selected-bg,hsla(137,66%,28%,1)));box-shadow:var(--button-primary-shadow-selected,var(--color-btn-primary-selected-shadow,inset 0 1px 0 rgba(0,45,17,0.2)));}/*!sc*/
.gYvpXq svg{color:fg.primary;}/*!sc*/
.fAkXQN{border-radius:6px;border:1px solid;border-color:transparent;font-family:inherit;font-weight:500;font-size:14px;cursor:pointer;-webkit-appearance:none;-moz-appearance:none;appearance:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;-webkit-text-decoration:none;text-decoration:none;text-align:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;height:32px;padding:0 12px;gap:8px;min-width:-webkit-max-content;min-width:-moz-max-content;min-width:max-content;-webkit-transition:80ms cubic-bezier(0.65,0,0.35,1);transition:80ms cubic-bezier(0.65,0,0.35,1);-webkit-transition-property:color,fill,background-color,border-color;transition-property:color,fill,background-color,border-color;color:var(--fgColor-default,var(--color-fg-default,#1F2328));background-color:transparent;box-shadow:none;}/*!sc*/
.fAkXQN:focus:not(:disabled){box-shadow:none;outline:2px solid var(--fgColor-accent,var(--color-accent-fg,#0969da));outline-offset:-2px;}/*!sc*/
.fAkXQN:focus:not(:disabled):not(:focus-visible){outline:solid 1px transparent;}/*!sc*/
.fAkXQN:focus-visible:not(:disabled){box-shadow:none;outline:2px solid var(--fgColor-accent,var(--color-accent-fg,#0969da));outline-offset:-2px;}/*!sc*/
.fAkXQN[href]{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;}/*!sc*/
.fAkXQN[href]:hover{-webkit-text-decoration:none;text-decoration:none;}/*!sc*/
.fAkXQN:hover{-webkit-transition-duration:80ms;transition-duration:80ms;}/*!sc*/
.fAkXQN:active{-webkit-transition:none;transition:none;}/*!sc*/
.fAkXQN[data-inactive]{cursor:auto;}/*!sc*/
.fAkXQN:disabled{cursor:not-allowed;box-shadow:none;color:var(--fgColor-disabled,var(--color-primer-fg-disabled,#8c959f));}/*!sc*/
.fAkXQN:disabled [data-component=ButtonCounter],.fAkXQN:disabled [data-component="leadingVisual"],.fAkXQN:disabled [data-component="trailingAction"]{color:inherit;}/*!sc*/
@media (forced-colors:active){.fAkXQN:focus{outline:solid 1px transparent;}}/*!sc*/
.fAkXQN [data-component=ButtonCounter]{font-size:12px;}/*!sc*/
.fAkXQN[data-component=IconButton]{display:inline-grid;padding:unset;place-content:center;width:32px;min-width:unset;}/*!sc*/
.fAkXQN[data-size="small"]{padding:0 8px;height:28px;gap:4px;font-size:12px;}/*!sc*/
.fAkXQN[data-size="small"] [data-component="text"]{line-height:calc(20 / 12);}/*!sc*/
.fAkXQN[data-size="small"] [data-component=ButtonCounter]{font-size:12px;}/*!sc*/
.fAkXQN[data-size="small"] [data-component="buttonContent"] > :not(:last-child){margin-right:4px;}/*!sc*/
.fAkXQN[data-size="small"][data-component=IconButton]{width:28px;padding:unset;}/*!sc*/
.fAkXQN[data-size="large"]{padding:0 16px;height:40px;gap:8px;}/*!sc*/
.fAkXQN[data-size="large"] [data-component="buttonContent"] > :not(:last-child){margin-right:8px;}/*!sc*/
.fAkXQN[data-size="large"][data-component=IconButton]{width:40px;padding:unset;}/*!sc*/
.fAkXQN[data-block="block"]{width:100%;}/*!sc*/
.fAkXQN[data-inactive]:not([disabled]){background-color:var(--button-inactive-bgColor,var(--button-inactive-bgColor-rest,var(--color-btn-inactive-bg,#eaeef2)));border-color:var(--button-inactive-bgColor,var(--button-inactive-bgColor-rest,var(--color-btn-inactive-bg,#eaeef2)));color:var(--button-inactive-fgColor,var(--button-inactive-fgColor-rest,var(--color-btn-inactive-text,#57606a)));}/*!sc*/
.fAkXQN[data-inactive]:not([disabled]):focus-visible{box-shadow:none;}/*!sc*/
.fAkXQN [data-component="leadingVisual"]{grid-area:leadingVisual;color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/
.fAkXQN [data-component="text"]{grid-area:text;line-height:calc(20/14);white-space:nowrap;}/*!sc*/
.fAkXQN [data-component="trailingVisual"]{grid-area:trailingVisual;}/*!sc*/
.fAkXQN [data-component="trailingAction"]{margin-right:-4px;color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/
.fAkXQN [data-component="buttonContent"]{-webkit-flex:1 0 auto;-ms-flex:1 0 auto;flex:1 0 auto;display:grid;grid-template-areas:"leadingVisual text trailingVisual";grid-template-columns:min-content minmax(0,auto) min-content;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-align-content:center;-ms-flex-line-pack:center;align-content:center;}/*!sc*/
.fAkXQN [data-component="buttonContent"] > :not(:last-child){margin-right:8px;}/*!sc*/
.fAkXQN:hover:not([disabled]){background-color:var(--control-transparent-bgColor-hover,var(--color-action-list-item-default-hover-bg,rgba(208,215,222,0.32)));-webkit-text-decoration:none;text-decoration:none;}/*!sc*/
.fAkXQN:active:not([disabled]){background-color:var(--control-transparent-bgColor-active,var(--color-action-list-item-default-active-bg,rgba(208,215,222,0.48)));-webkit-text-decoration:none;text-decoration:none;}/*!sc*/
.fAkXQN[aria-expanded=true]{background-color:var(--control-transparent-bgColor-selected,var(--color-action-list-item-default-selected-bg,rgba(208,215,222,0.24)));}/*!sc*/
.fAkXQN[data-component="IconButton"][data-no-visuals]{color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/
.fAkXQN[data-no-visuals]{color:var(--fgColor-accent,var(--color-accent-fg,#0969da));}/*!sc*/
.fAkXQN:has([data-component="ButtonCounter"]){color:var(--button-default-fgColor-rest,var(--color-btn-text,#24292f));}/*!sc*/
.fAkXQN:disabled[data-no-visuals]{color:var(--fgColor-disabled,var(--color-primer-fg-disabled,#8c959f));}/*!sc*/
.fAkXQN:disabled[data-no-visuals] [data-component=ButtonCounter]{color:inherit;}/*!sc*/
.fAkXQN:focus:not([disabled]){-webkit-text-decoration:none;text-decoration:none;}/*!sc*/
.jPraEl{border-radius:6px;border:1px solid;border-color:transparent;font-family:inherit;font-weight:500;font-size:14px;cursor:pointer;-webkit-appearance:none;-moz-appearance:none;appearance:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;-webkit-text-decoration:none;text-decoration:none;text-align:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;height:32px;padding:0 12px;gap:8px;min-width:-webkit-max-content;min-width:-moz-max-content;min-width:max-content;-webkit-transition:80ms cubic-bezier(0.65,0,0.35,1);transition:80ms cubic-bezier(0.65,0,0.35,1);-webkit-transition-property:color,fill,background-color,border-color;transition-property:color,fill,background-color,border-color;color:var(--button-default-fgColor-rest,var(--color-btn-text,#24292f));background-color:transparent;box-shadow:none;}/*!sc*/
.jPraEl:focus:not(:disabled){box-shadow:none;outline:2px solid var(--fgColor-accent,var(--color-accent-fg,#0969da));outline-offset:-2px;}/*!sc*/
.jPraEl:focus:not(:disabled):not(:focus-visible){outline:solid 1px transparent;}/*!sc*/
.jPraEl:focus-visible:not(:disabled){box-shadow:none;outline:2px solid var(--fgColor-accent,var(--color-accent-fg,#0969da));outline-offset:-2px;}/*!sc*/
.jPraEl[href]{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;}/*!sc*/
.jPraEl[href]:hover{-webkit-text-decoration:none;text-decoration:none;}/*!sc*/
.jPraEl:hover{-webkit-transition-duration:80ms;transition-duration:80ms;}/*!sc*/
.jPraEl:active{-webkit-transition:none;transition:none;}/*!sc*/
.jPraEl[data-inactive]{cursor:auto;}/*!sc*/
.jPraEl:disabled{cursor:not-allowed;box-shadow:none;color:var(--fgColor-disabled,var(--color-primer-fg-disabled,#8c959f));}/*!sc*/
.jPraEl:disabled [data-component=ButtonCounter],.jPraEl:disabled [data-component="leadingVisual"],.jPraEl:disabled [data-component="trailingAction"]{color:inherit;}/*!sc*/
@media (forced-colors:active){.jPraEl:focus{outline:solid 1px transparent;}}/*!sc*/
.jPraEl [data-component=ButtonCounter]{font-size:12px;}/*!sc*/
.jPraEl[data-component=IconButton]{display:inline-grid;padding:unset;place-content:center;width:32px;min-width:unset;}/*!sc*/
.jPraEl[data-size="small"]{padding:0 8px;height:28px;gap:4px;font-size:12px;}/*!sc*/
.jPraEl[data-size="small"] [data-component="text"]{line-height:calc(20 / 12);}/*!sc*/
.jPraEl[data-size="small"] [data-component=ButtonCounter]{font-size:12px;}/*!sc*/
.jPraEl[data-size="small"] [data-component="buttonContent"] > :not(:last-child){margin-right:4px;}/*!sc*/
.jPraEl[data-size="small"][data-component=IconButton]{width:28px;padding:unset;}/*!sc*/
.jPraEl[data-size="large"]{padding:0 16px;height:40px;gap:8px;}/*!sc*/
.jPraEl[data-size="large"] [data-component="buttonContent"] > :not(:last-child){margin-right:8px;}/*!sc*/
.jPraEl[data-size="large"][data-component=IconButton]{width:40px;padding:unset;}/*!sc*/
.jPraEl[data-block="block"]{width:100%;}/*!sc*/
.jPraEl[data-inactive]:not([disabled]){background-color:var(--button-inactive-bgColor,var(--button-inactive-bgColor-rest,var(--color-btn-inactive-bg,#eaeef2)));border-color:var(--button-inactive-bgColor,var(--button-inactive-bgColor-rest,var(--color-btn-inactive-bg,#eaeef2)));color:var(--button-inactive-fgColor,var(--button-inactive-fgColor-rest,var(--color-btn-inactive-text,#57606a)));}/*!sc*/
.jPraEl[data-inactive]:not([disabled]):focus-visible{box-shadow:none;}/*!sc*/
.jPraEl [data-component="leadingVisual"]{grid-area:leadingVisual;color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/
.jPraEl [data-component="text"]{grid-area:text;line-height:calc(20/14);white-space:nowrap;}/*!sc*/
.jPraEl [data-component="trailingVisual"]{grid-area:trailingVisual;}/*!sc*/
.jPraEl [data-component="trailingAction"]{margin-right:-4px;color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/
.jPraEl [data-component="buttonContent"]{-webkit-flex:1 0 auto;-ms-flex:1 0 auto;flex:1 0 auto;display:grid;grid-template-areas:"leadingVisual text trailingVisual";grid-template-columns:min-content minmax(0,auto) min-content;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-align-content:center;-ms-flex-line-pack:center;align-content:center;}/*!sc*/
.jPraEl [data-component="buttonContent"] > :not(:last-child){margin-right:8px;}/*!sc*/
.jPraEl:hover:not([disabled]){background-color:var(--control-transparent-bgColor-hover,var(--color-action-list-item-default-hover-bg,rgba(208,215,222,0.32)));}/*!sc*/
.jPraEl:active:not([disabled]){background-color:var(--control-transparent-bgColor-active,var(--color-action-list-item-default-active-bg,rgba(208,215,222,0.48)));}/*!sc*/
.jPraEl[aria-expanded=true]{background-color:var(--control-transparent-bgColor-selected,var(--color-action-list-item-default-selected-bg,rgba(208,215,222,0.24)));}/*!sc*/
.jPraEl[data-component="IconButton"][data-no-visuals]{color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/
.jPraEl[data-no-visuals]{color:var(--fgColor-accent,var(--color-accent-fg,#0969da));}/*!sc*/
.jPraEl:has([data-component="ButtonCounter"]){color:var(--button-default-fgColor-rest,var(--color-btn-text,#24292f));}/*!sc*/
.jPraEl:disabled[data-no-visuals]{color:var(--fgColor-disabled,var(--color-primer-fg-disabled,#8c959f));}/*!sc*/
.jPraEl:disabled[data-no-visuals] [data-component=ButtonCounter]{color:inherit;}/*!sc*/
.jPraEl{color:var(--fgColor-muted,var(--color-fg-subtle,#6e7781));padding-left:8px;padding-right:8px;}/*!sc*/
data-styled.g9[id="types__StyledButton-sc-ws60qy-0"]{content:"hILMMl,cuOWTR,tDSzd,pyyxt,gYvpXq,fAkXQN,jPraEl,"}/*!sc*/
.rTZSs{position:absolute;width:1px;height:1px;padding:0;margin:-1px;overflow:hidden;-webkit-clip:rect(0,0,0,0);clip:rect(0,0,0,0);white-space:nowrap;border-width:0;}/*!sc*/
data-styled.g10[id="_VisuallyHidden__VisuallyHidden-sc-11jhm7a-0"]{content:"rTZSs,"}/*!sc*/
.gNgnVl{position:relative;display:inline-block;}/*!sc*/
.gNgnVl::after{position:absolute;z-index:1000000;display:none;padding:0.5em 0.75em;font:normal normal 11px/1.5 -apple-system,BlinkMacSystemFont,"Segoe UI","Noto Sans",Helvetica,Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji";-webkit-font-smoothing:subpixel-antialiased;color:var(--fgColor-onEmphasis,var(--color-fg-on-emphasis,#ffffff));text-align:center;-webkit-text-decoration:none;text-decoration:none;text-shadow:none;text-transform:none;-webkit-letter-spacing:normal;-moz-letter-spacing:normal;-ms-letter-spacing:normal;letter-spacing:normal;word-wrap:break-word;white-space:pre;pointer-events:none;content:attr(aria-label);background:var(--bgColor-emphasis,var(--color-neutral-emphasis-plus,#24292f));border-radius:6px;opacity:0;}/*!sc*/
@-webkit-keyframes tooltip-appear{from{opacity:0;}to{opacity:1;}}/*!sc*/
@keyframes tooltip-appear{from{opacity:0;}to{opacity:1;}}/*!sc*/
.gNgnVl:hover::after,.gNgnVl:active::after,.gNgnVl:focus::after,.gNgnVl:focus-within::after{display:inline-block;-webkit-text-decoration:none;text-decoration:none;-webkit-animation-name:tooltip-appear;animation-name:tooltip-appear;-webkit-animation-duration:0.1s;animation-duration:0.1s;-webkit-animation-fill-mode:forwards;animation-fill-mode:forwards;-webkit-animation-timing-function:ease-in;animation-timing-function:ease-in;-webkit-animation-delay:0s;animation-delay:0s;}/*!sc*/
.gNgnVl.tooltipped-no-delay:hover::after,.gNgnVl.tooltipped-no-delay:active::after,.gNgnVl.tooltipped-no-delay:focus::after,.gNgnVl.tooltipped-no-delay:focus-within::after{-webkit-animation-delay:0s;animation-delay:0s;}/*!sc*/
.gNgnVl.tooltipped-multiline:hover::after,.gNgnVl.tooltipped-multiline:active::after,.gNgnVl.tooltipped-multiline:focus::after,.gNgnVl.tooltipped-multiline:focus-within::after{display:table-cell;}/*!sc*/
.gNgnVl.tooltipped-s::after,.gNgnVl.tooltipped-se::after,.gNgnVl.tooltipped-sw::after{top:100%;right:50%;margin-top:6px;}/*!sc*/
.gNgnVl.tooltipped-se::after{right:auto;left:50%;margin-left:-16px;}/*!sc*/
.gNgnVl.tooltipped-sw::after{margin-right:-16px;}/*!sc*/
.gNgnVl.tooltipped-n::after,.gNgnVl.tooltipped-ne::after,.gNgnVl.tooltipped-nw::after{right:50%;bottom:100%;margin-bottom:6px;}/*!sc*/
.gNgnVl.tooltipped-ne::after{right:auto;left:50%;margin-left:-16px;}/*!sc*/
.gNgnVl.tooltipped-nw::after{margin-right:-16px;}/*!sc*/
.gNgnVl.tooltipped-s::after,.gNgnVl.tooltipped-n::after{-webkit-transform:translateX(50%);-ms-transform:translateX(50%);transform:translateX(50%);}/*!sc*/
.gNgnVl.tooltipped-w::after{right:100%;bottom:50%;margin-right:6px;-webkit-transform:translateY(50%);-ms-transform:translateY(50%);transform:translateY(50%);}/*!sc*/
.gNgnVl.tooltipped-e::after{bottom:50%;left:100%;margin-left:6px;-webkit-transform:translateY(50%);-ms-transform:translateY(50%);transform:translateY(50%);}/*!sc*/
.gNgnVl.tooltipped-multiline::after{width:-webkit-max-content;width:-moz-max-content;width:max-content;max-width:250px;word-wrap:break-word;white-space:pre-line;border-collapse:separate;}/*!sc*/
.gNgnVl.tooltipped-multiline.tooltipped-s::after,.gNgnVl.tooltipped-multiline.tooltipped-n::after{right:auto;left:50%;-webkit-transform:translateX(-50%);-ms-transform:translateX(-50%);transform:translateX(-50%);}/*!sc*/
.gNgnVl.tooltipped-multiline.tooltipped-w::after,.gNgnVl.tooltipped-multiline.tooltipped-e::after{right:100%;}/*!sc*/
.gNgnVl.tooltipped-align-right-2::after{right:0;margin-right:0;}/*!sc*/
.gNgnVl.tooltipped-align-left-2::after{left:0;margin-left:0;}/*!sc*/
data-styled.g14[id="Tooltip__TooltipBase-sc-17tf59c-0"]{content:"gNgnVl,"}/*!sc*/
.fUpWeN{display:inline-block;overflow:hidden;text-overflow:ellipsis;vertical-align:top;white-space:nowrap;max-width:125px;max-width:100%;}/*!sc*/
data-styled.g16[id="Truncate__StyledTruncate-sc-23o1d2-0"]{content:"fUpWeN,"}/*!sc*/
.bPgibo{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;list-style:none;white-space:nowrap;padding-top:0;padding-bottom:0;padding-left:0;padding-right:0;margin:0;margin-bottom:-1px;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;gap:8px;position:relative;}/*!sc*/
data-styled.g103[id="UnderlineNav__NavigationList-sc-1jfr31k-0"]{content:"bPgibo,"}/*!sc*/
              </style>
              <!-- -->
              <!-- -->
              <div class="Box-sc-g0xbh4-0 izjvBm">
               <div class="Box-sc-g0xbh4-0 rPQgy">
                <div class="Box-sc-g0xbh4-0 eUMEDg">
                </div>
               </div>
               <div class="Box-sc-g0xbh4-0 eLcVee">
                <div class="Box-sc-g0xbh4-0 hsfLlq">
                 <div class="Box-sc-g0xbh4-0 gpKoUz">
                  <button aria-haspopup="true" aria-label="master branch" class="types__StyledButton-sc-ws60qy-0 hILMMl overview-ref-selector" data-testid="anchor-button" id="branch-picker-repos-header-ref-selector" tabindex="0" type="button">
                   <span class="Box-sc-g0xbh4-0 kkrdEu" data-component="buttonContent">
                    <span data-component="text">
                     <div class="Box-sc-g0xbh4-0 bKgizp">
                      <div class="Box-sc-g0xbh4-0 iPGYsi">
                       <svg aria-hidden="true" class="octicon octicon-git-branch" fill="currentColor" focusable="false" height="16" role="img" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible" viewbox="0 0 16 16" width="16">
                        <path d="M9.5 3.25a2.25 2.25 0 1 1 3 2.122V6A2.5 2.5 0 0 1 10 8.5H6a1 1 0 0 0-1 1v1.128a2.251 2.251 0 1 1-1.5 0V5.372a2.25 2.25 0 1 1 1.5 0v1.836A2.493 2.493 0 0 1 6 7h4a1 1 0 0 0 1-1v-.628A2.25 2.25 0 0 1 9.5 3.25Zm-6 0a.75.75 0 1 0 1.5 0 .75.75 0 0 0-1.5 0Zm8.25-.75a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5ZM4.25 12a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Z">
                        </path>
                       </svg>
                      </div>
                      <div class="Box-sc-g0xbh4-0 dKmYfk ref-selector-button-text-container">
                       <span class="Text-sc-17v1xeu-0 bOMzPg">
                        <!-- -->
                        master
                       </span>
                      </div>
                     </div>
                    </span>
                    <span class="Box-sc-g0xbh4-0 trpoQ" data-component="trailingVisual">
                     <svg aria-hidden="true" class="octicon octicon-triangle-down" fill="currentColor" focusable="false" height="16" role="img" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible" viewbox="0 0 16 16" width="16">
                      <path d="m4.427 7.427 3.396 3.396a.25.25 0 0 0 .354 0l3.396-3.396A.25.25 0 0 0 11.396 7H4.604a.25.25 0 0 0-.177.427Z">
                      </path>
                     </svg>
                    </span>
                   </span>
                  </button>
                  <button data-hotkey-scope="read-only-cursor-text-area" hidden="">
                  </button>
                 </div>
                 <div class="Box-sc-g0xbh4-0 laYubZ">
                  <a class="types__StyledButton-sc-ws60qy-0 cuOWTR" href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/branches" style="--button-color:fg.muted" type="button">
                   <span class="Box-sc-g0xbh4-0 kkrdEu" data-component="buttonContent">
                    <span class="Box-sc-g0xbh4-0 trpoQ" data-component="leadingVisual">
                     <svg aria-hidden="true" class="octicon octicon-git-branch" fill="currentColor" focusable="false" height="16" role="img" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible" viewbox="0 0 16 16" width="16">
                      <path d="M9.5 3.25a2.25 2.25 0 1 1 3 2.122V6A2.5 2.5 0 0 1 10 8.5H6a1 1 0 0 0-1 1v1.128a2.251 2.251 0 1 1-1.5 0V5.372a2.25 2.25 0 1 1 1.5 0v1.836A2.493 2.493 0 0 1 6 7h4a1 1 0 0 0 1-1v-.628A2.25 2.25 0 0 1 9.5 3.25Zm-6 0a.75.75 0 1 0 1.5 0 .75.75 0 0 0-1.5 0Zm8.25-.75a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5ZM4.25 12a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Z">
                      </path>
                     </svg>
                    </span>
                    <span data-component="text">
                     Branches
                    </span>
                   </span>
                  </a>
                  <a class="types__StyledButton-sc-ws60qy-0 cuOWTR" href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/tags" style="--button-color:fg.muted" type="button">
                   <span class="Box-sc-g0xbh4-0 kkrdEu" data-component="buttonContent">
                    <span class="Box-sc-g0xbh4-0 trpoQ" data-component="leadingVisual">
                     <svg aria-hidden="true" class="octicon octicon-tag" fill="currentColor" focusable="false" height="16" role="img" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible" viewbox="0 0 16 16" width="16">
                      <path d="M1 7.775V2.75C1 1.784 1.784 1 2.75 1h5.025c.464 0 .91.184 1.238.513l6.25 6.25a1.75 1.75 0 0 1 0 2.474l-5.026 5.026a1.75 1.75 0 0 1-2.474 0l-6.25-6.25A1.752 1.752 0 0 1 1 7.775Zm1.5 0c0 .066.026.13.073.177l6.25 6.25a.25.25 0 0 0 .354 0l5.025-5.025a.25.25 0 0 0 0-.354l-6.25-6.25a.25.25 0 0 0-.177-.073H2.75a.25.25 0 0 0-.25.25ZM6 5a1 1 0 1 1 0 2 1 1 0 0 1 0-2Z">
                      </path>
                     </svg>
                    </span>
                    <span data-component="text">
                     Tags
                    </span>
                   </span>
                  </a>
                 </div>
                 <div class="Box-sc-g0xbh4-0 swnaL">
                  <a aria-label="Go to Branches page" class="types__StyledButton-sc-ws60qy-0 tDSzd" data-no-visuals="true" href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/branches" style="--button-color:fg.muted" type="button">
                   <svg aria-hidden="true" class="octicon octicon-git-branch" fill="currentColor" focusable="false" height="16" role="img" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible" viewbox="0 0 16 16" width="16">
                    <path d="M9.5 3.25a2.25 2.25 0 1 1 3 2.122V6A2.5 2.5 0 0 1 10 8.5H6a1 1 0 0 0-1 1v1.128a2.251 2.251 0 1 1-1.5 0V5.372a2.25 2.25 0 1 1 1.5 0v1.836A2.493 2.493 0 0 1 6 7h4a1 1 0 0 0 1-1v-.628A2.25 2.25 0 0 1 9.5 3.25Zm-6 0a.75.75 0 1 0 1.5 0 .75.75 0 0 0-1.5 0Zm8.25-.75a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5ZM4.25 12a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Z">
                    </path>
                   </svg>
                  </a>
                  <a aria-label="Go to Tags page" class="types__StyledButton-sc-ws60qy-0 tDSzd" data-no-visuals="true" href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/tags" style="--button-color:fg.muted" type="button">
                   <svg aria-hidden="true" class="octicon octicon-tag" fill="currentColor" focusable="false" height="16" role="img" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible" viewbox="0 0 16 16" width="16">
                    <path d="M1 7.775V2.75C1 1.784 1.784 1 2.75 1h5.025c.464 0 .91.184 1.238.513l6.25 6.25a1.75 1.75 0 0 1 0 2.474l-5.026 5.026a1.75 1.75 0 0 1-2.474 0l-6.25-6.25A1.752 1.752 0 0 1 1 7.775Zm1.5 0c0 .066.026.13.073.177l6.25 6.25a.25.25 0 0 0 .354 0l5.025-5.025a.25.25 0 0 0 0-.354l-6.25-6.25a.25.25 0 0 0-.177-.073H2.75a.25.25 0 0 0-.25.25ZM6 5a1 1 0 1 1 0 2 1 1 0 0 1 0-2Z">
                    </path>
                   </svg>
                  </a>
                 </div>
                </div>
                <div class="Box-sc-g0xbh4-0 bWpuBf">
                 <div class="Box-sc-g0xbh4-0 grHjNb">
                  <div class="Box-sc-g0xbh4-0 dXTsqj">
                   <!--$!-->
                   <template>
                   </template>
                   <!--/$-->
                  </div>
                  <div class="Box-sc-g0xbh4-0 dCOrmu">
                   <button class="types__StyledButton-sc-ws60qy-0 pyyxt" data-no-visuals="true" type="button">
                    <span class="Box-sc-g0xbh4-0 kkrdEu" data-component="buttonContent">
                     <span data-component="text">
                      Go to file
                     </span>
                    </span>
                   </button>
                  </div>
                  <div class="react-directory-add-file-icon">
                  </div>
                  <div class="react-directory-remove-file-icon">
                  </div>
                 </div>
                 <button aria-haspopup="true" class="types__StyledButton-sc-ws60qy-0 gYvpXq" id=":R55ab:" tabindex="0" type="button">
                  <span class="Box-sc-g0xbh4-0 kkrdEu" data-component="buttonContent">
                   <span class="Box-sc-g0xbh4-0 trpoQ" data-component="leadingVisual">
                    <div class="Box-sc-g0xbh4-0 bVvbgP">
                     <svg aria-hidden="true" class="octicon octicon-code" fill="currentColor" focusable="false" height="16" role="img" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible" viewbox="0 0 16 16" width="16">
                      <path d="m11.28 3.22 4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.275-.326.749.749 0 0 1 .215-.734L13.94 8l-3.72-3.72a.749.749 0 0 1 .326-1.275.749.749 0 0 1 .734.215Zm-6.56 0a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042L2.06 8l3.72 3.72a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L.47 8.53a.75.75 0 0 1 0-1.06Z">
                      </path>
                     </svg>
                    </div>
                   </span>
                   <span data-component="text">
                    Code
                   </span>
                  </span>
                  <span class="Box-sc-g0xbh4-0 trpoQ" data-component="trailingAction">
                   <svg aria-hidden="true" class="octicon octicon-triangle-down" fill="currentColor" focusable="false" height="16" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible" viewbox="0 0 16 16" width="16">
                    <path d="m4.427 7.427 3.396 3.396a.25.25 0 0 0 .354 0l3.396-3.396A.25.25 0 0 0 11.396 7H4.604a.25.25 0 0 0-.177.427Z">
                    </path>
                   </svg>
                  </span>
                 </button>
                 <div class="Box-sc-g0xbh4-0 bNDvfp">
                  <button aria-haspopup="true" aria-label="Open more actions menu" class="types__StyledButton-sc-ws60qy-0 pyyxt" data-component="IconButton" data-no-visuals="true" id=":R75ab:" tabindex="0" type="button">
                   <svg aria-hidden="true" class="octicon octicon-kebab-horizontal" fill="currentColor" focusable="false" height="16" role="img" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible" viewbox="0 0 16 16" width="16">
                    <path d="M8 9a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3ZM1.5 9a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Zm13 0a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Z">
                    </path>
                   </svg>
                  </button>
                 </div>
                </div>
               </div>
               <div aria-live="polite" class="Box-sc-g0xbh4-0 vhgA" data-testid="branch-info-bar">
                <div class="Skeleton Skeleton--text" style="width:40%">
                </div>
                <div class="Skeleton Skeleton--text" style="width:30%">
                </div>
               </div>
               <div class="Box-sc-g0xbh4-0 yfPnm">
                <div class="Box-sc-g0xbh4-0" data-hpc="true">
                 <button data-hotkey="j" data-testid="focus-next-element-button" hidden="">
                 </button>
                 <button data-hotkey="k" data-testid="focus-previous-element-button" hidden="">
                 </button>
                 <h2 class="Heading__StyledHeading-sc-1c1dgg0-0 cgQnMS sr-only" data-testid="screen-reader-heading" id="folders-and-files">
                  Folders and files
                 </h2>
                 <table aria-labelledby="folders-and-files" class="Box-sc-g0xbh4-0 cAQuiW">
                  <thead class="Box-sc-g0xbh4-0 iiUlLN">
                   <tr class="Box-sc-g0xbh4-0 jmggSN">
                    <th class="Box-sc-g0xbh4-0 kvYunM" colspan="2">
                     <span class="Text-sc-17v1xeu-0 eUGNHp">
                      Name
                     </span>
                    </th>
                    <th class="Box-sc-g0xbh4-0 hrLuxA" colspan="1">
                     <span class="Text-sc-17v1xeu-0 eUGNHp">
                      Name
                     </span>
                    </th>
                    <th class="Box-sc-g0xbh4-0 ePjhhA">
                     <div class="Truncate__StyledTruncate-sc-23o1d2-0 fUpWeN" title="Last commit message">
                      <span class="Text-sc-17v1xeu-0 eUGNHp">
                       Last commit message
                      </span>
                     </div>
                    </th>
                    <th class="Box-sc-g0xbh4-0 cuEKae" colspan="1">
                     <div class="Truncate__StyledTruncate-sc-23o1d2-0 fUpWeN" title="Last commit date">
                      <span class="Text-sc-17v1xeu-0 eUGNHp">
                       Last commit date
                      </span>
                     </div>
                    </th>
                   </tr>
                  </thead>
                  <tbody>
                   <tr class="Box-sc-g0xbh4-0 jEbBOT">
                    <td class="Box-sc-g0xbh4-0 bTxCvM" colspan="3">
                     <div class="Box-sc-g0xbh4-0 eYedVD">
                      <h2 class="Heading__StyledHeading-sc-1c1dgg0-0 cgQnMS sr-only" data-testid="screen-reader-heading">
                       Latest commit
                      </h2>
                      <div class="Skeleton Skeleton--text" data-testid="loading" style="width:120px">
                      </div>
                      <div class="Box-sc-g0xbh4-0 lhFvfi" data-testid="latest-commit-details">
                      </div>
                      <div class="Box-sc-g0xbh4-0 jGfYmh">
                       <h2 class="Heading__StyledHeading-sc-1c1dgg0-0 cgQnMS sr-only" data-testid="screen-reader-heading">
                        History
                       </h2>
                       <a class="types__StyledButton-sc-ws60qy-0 fAkXQN react-last-commit-history-group" data-size="small" href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/commits/master/">
                        <span class="Box-sc-g0xbh4-0 kkrdEu" data-component="buttonContent">
                         <span class="Box-sc-g0xbh4-0 trpoQ" data-component="leadingVisual">
                          <svg aria-hidden="true" class="octicon octicon-history" fill="currentColor" focusable="false" height="16" role="img" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible" viewbox="0 0 16 16" width="16">
                           <path d="m.427 1.927 1.215 1.215a8.002 8.002 0 1 1-1.6 5.685.75.75 0 1 1 1.493-.154 6.5 6.5 0 1 0 1.18-4.458l1.358 1.358A.25.25 0 0 1 3.896 6H.25A.25.25 0 0 1 0 5.75V2.104a.25.25 0 0 1 .427-.177ZM7.75 4a.75.75 0 0 1 .75.75v2.992l2.028.812a.75.75 0 0 1-.557 1.392l-2.5-1A.751.751 0 0 1 7 8.25v-3.5A.75.75 0 0 1 7.75 4Z">
                           </path>
                          </svg>
                         </span>
                         <span data-component="text">
                          <span class="Text-sc-17v1xeu-0 dALsKK">
                           40 Commits
                          </span>
                         </span>
                        </span>
                       </a>
                       <div class="Box-sc-g0xbh4-0 bqgLjk">
                       </div>
                       <span aria-label="Commit history" class="Tooltip__TooltipBase-sc-17tf59c-0 gNgnVl tooltipped-n" id=":R92j8pab:" role="tooltip">
                        <a class="types__StyledButton-sc-ws60qy-0 fAkXQN react-last-commit-history-icon" href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/commits/master/">
                         <span class="Box-sc-g0xbh4-0 kkrdEu" data-component="buttonContent">
                          <span class="Box-sc-g0xbh4-0 trpoQ" data-component="leadingVisual">
                           <svg aria-hidden="true" class="octicon octicon-history" fill="currentColor" focusable="false" height="16" role="img" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible" viewbox="0 0 16 16" width="16">
                            <path d="m.427 1.927 1.215 1.215a8.002 8.002 0 1 1-1.6 5.685.75.75 0 1 1 1.493-.154 6.5 6.5 0 1 0 1.18-4.458l1.358 1.358A.25.25 0 0 1 3.896 6H.25A.25.25 0 0 1 0 5.75V2.104a.25.25 0 0 1 .427-.177ZM7.75 4a.75.75 0 0 1 .75.75v2.992l2.028.812a.75.75 0 0 1-.557 1.392l-2.5-1A.751.751 0 0 1 7 8.25v-3.5A.75.75 0 0 1 7.75 4Z">
                            </path>
                           </svg>
                          </span>
                         </span>
                        </a>
                       </span>
                      </div>
                     </div>
                    </td>
                   </tr>
                   <tr class="react-directory-row undefined" id="folder-row-0">
                    <td class="react-directory-row-name-cell-small-screen" colspan="2">
                     <div class="react-directory-filename-column">
                      <svg aria-hidden="true" class="icon-directory" fill="currentColor" focusable="false" height="16" role="img" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible" viewbox="0 0 16 16" width="16">
                       <path d="M1.75 1A1.75 1.75 0 0 0 0 2.75v10.5C0 14.216.784 15 1.75 15h12.5A1.75 1.75 0 0 0 16 13.25v-8.5A1.75 1.75 0 0 0 14.25 3H7.5a.25.25 0 0 1-.2-.1l-.9-1.2C6.07 1.26 5.55 1 5 1H1.75Z">
                       </path>
                      </svg>
                      <div class="overflow-hidden">
                       <h3>
                        <div class="react-directory-truncate">
                         <a aria-label="assets, (Directory)" class="Link--primary" href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/tree/master/assets" title="assets">
                          assets
                         </a>
                        </div>
                       </h3>
                      </div>
                     </div>
                    </td>
                    <td class="react-directory-row-name-cell-large-screen" colspan="1">
                     <div class="react-directory-filename-column">
                      <svg aria-hidden="true" class="icon-directory" fill="currentColor" focusable="false" height="16" role="img" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible" viewbox="0 0 16 16" width="16">
                       <path d="M1.75 1A1.75 1.75 0 0 0 0 2.75v10.5C0 14.216.784 15 1.75 15h12.5A1.75 1.75 0 0 0 16 13.25v-8.5A1.75 1.75 0 0 0 14.25 3H7.5a.25.25 0 0 1-.2-.1l-.9-1.2C6.07 1.26 5.55 1 5 1H1.75Z">
                       </path>
                      </svg>
                      <div class="overflow-hidden">
                       <h3>
                        <div class="react-directory-truncate">
                         <a aria-label="assets, (Directory)" class="Link--primary" href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/tree/master/assets" title="assets">
                          assets
                         </a>
                        </div>
                       </h3>
                      </div>
                     </div>
                    </td>
                    <td class="react-directory-row-commit-cell">
                     <div class="Skeleton Skeleton--text">
                     </div>
                    </td>
                    <td>
                     <div class="Skeleton Skeleton--text">
                     </div>
                    </td>
                   </tr>
                   <tr class="react-directory-row undefined" id="folder-row-1">
                    <td class="react-directory-row-name-cell-small-screen" colspan="2">
                     <div class="react-directory-filename-column">
                      <svg aria-hidden="true" class="icon-directory" fill="currentColor" focusable="false" height="16" role="img" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible" viewbox="0 0 16 16" width="16">
                       <path d="M1.75 1A1.75 1.75 0 0 0 0 2.75v10.5C0 14.216.784 15 1.75 15h12.5A1.75 1.75 0 0 0 16 13.25v-8.5A1.75 1.75 0 0 0 14.25 3H7.5a.25.25 0 0 1-.2-.1l-.9-1.2C6.07 1.26 5.55 1 5 1H1.75Z">
                       </path>
                      </svg>
                      <div class="overflow-hidden">
                       <h3>
                        <div class="react-directory-truncate">
                         <a aria-label="css, (Directory)" class="Link--primary" href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/tree/master/css" title="css">
                          css
                         </a>
                        </div>
                       </h3>
                      </div>
                     </div>
                    </td>
                    <td class="react-directory-row-name-cell-large-screen" colspan="1">
                     <div class="react-directory-filename-column">
                      <svg aria-hidden="true" class="icon-directory" fill="currentColor" focusable="false" height="16" role="img" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible" viewbox="0 0 16 16" width="16">
                       <path d="M1.75 1A1.75 1.75 0 0 0 0 2.75v10.5C0 14.216.784 15 1.75 15h12.5A1.75 1.75 0 0 0 16 13.25v-8.5A1.75 1.75 0 0 0 14.25 3H7.5a.25.25 0 0 1-.2-.1l-.9-1.2C6.07 1.26 5.55 1 5 1H1.75Z">
                       </path>
                      </svg>
                      <div class="overflow-hidden">
                       <h3>
                        <div class="react-directory-truncate">
                         <a aria-label="css, (Directory)" class="Link--primary" href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/tree/master/css" title="css">
                          css
                         </a>
                        </div>
                       </h3>
                      </div>
                     </div>
                    </td>
                    <td class="react-directory-row-commit-cell">
                     <div class="Skeleton Skeleton--text">
                     </div>
                    </td>
                    <td>
                     <div class="Skeleton Skeleton--text">
                     </div>
                    </td>
                   </tr>
                   <tr class="react-directory-row undefined" id="folder-row-2">
                    <td class="react-directory-row-name-cell-small-screen" colspan="2">
                     <div class="react-directory-filename-column">
                      <svg aria-hidden="true" class="color-fg-muted" fill="currentColor" focusable="false" height="16" role="img" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible" viewbox="0 0 16 16" width="16">
                       <path d="M2 1.75C2 .784 2.784 0 3.75 0h6.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v9.586A1.75 1.75 0 0 1 13.25 16h-9.5A1.75 1.75 0 0 1 2 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h9.5a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 9 4.25V1.5Zm6.75.062V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z">
                       </path>
                      </svg>
                      <div class="overflow-hidden">
                       <h3>
                        <div class="react-directory-truncate">
                         <a aria-label="README.html, (File)" class="Link--primary" href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/README.html" title="README.html">
                          README.html
                         </a>
                        </div>
                       </h3>
                      </div>
                     </div>
                    </td>
                    <td class="react-directory-row-name-cell-large-screen" colspan="1">
                     <div class="react-directory-filename-column">
                      <svg aria-hidden="true" class="color-fg-muted" fill="currentColor" focusable="false" height="16" role="img" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible" viewbox="0 0 16 16" width="16">
                       <path d="M2 1.75C2 .784 2.784 0 3.75 0h6.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v9.586A1.75 1.75 0 0 1 13.25 16h-9.5A1.75 1.75 0 0 1 2 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h9.5a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 9 4.25V1.5Zm6.75.062V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z">
                       </path>
                      </svg>
                      <div class="overflow-hidden">
                       <h3>
                        <div class="react-directory-truncate">
                         <a aria-label="README.html, (File)" class="Link--primary" href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/README.html" title="README.html">
                          README.html
                         </a>
                        </div>
                       </h3>
                      </div>
                     </div>
                    </td>
                    <td class="react-directory-row-commit-cell">
                     <div class="Skeleton Skeleton--text">
                     </div>
                    </td>
                    <td>
                     <div class="Skeleton Skeleton--text">
                     </div>
                    </td>
                   </tr>
                   <tr class="react-directory-row undefined" id="folder-row-3">
                    <td class="react-directory-row-name-cell-small-screen" colspan="2">
                     <div class="react-directory-filename-column">
                      <svg aria-hidden="true" class="color-fg-muted" fill="currentColor" focusable="false" height="16" role="img" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible" viewbox="0 0 16 16" width="16">
                       <path d="M2 1.75C2 .784 2.784 0 3.75 0h6.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v9.586A1.75 1.75 0 0 1 13.25 16h-9.5A1.75 1.75 0 0 1 2 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h9.5a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 9 4.25V1.5Zm6.75.062V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z">
                       </path>
                      </svg>
                      <div class="overflow-hidden">
                       <h3>
                        <div class="react-directory-truncate">
                         <a aria-label="README.md, (File)" class="Link--primary" href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/README.md" title="README.md">
                          README.md
                         </a>
                        </div>
                       </h3>
                      </div>
                     </div>
                    </td>
                    <td class="react-directory-row-name-cell-large-screen" colspan="1">
                     <div class="react-directory-filename-column">
                      <svg aria-hidden="true" class="color-fg-muted" fill="currentColor" focusable="false" height="16" role="img" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible" viewbox="0 0 16 16" width="16">
                       <path d="M2 1.75C2 .784 2.784 0 3.75 0h6.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v9.586A1.75 1.75 0 0 1 13.25 16h-9.5A1.75 1.75 0 0 1 2 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h9.5a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 9 4.25V1.5Zm6.75.062V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z">
                       </path>
                      </svg>
                      <div class="overflow-hidden">
                       <h3>
                        <div class="react-directory-truncate">
                         <a aria-label="README.md, (File)" class="Link--primary" href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/README.md" title="README.md">
                          README.md
                         </a>
                        </div>
                       </h3>
                      </div>
                     </div>
                    </td>
                    <td class="react-directory-row-commit-cell">
                     <div class="Skeleton Skeleton--text">
                     </div>
                    </td>
                    <td>
                     <div class="Skeleton Skeleton--text">
                     </div>
                    </td>
                   </tr>
                   <tr class="Box-sc-g0xbh4-0 epsqEd d-none" data-testid="view-all-files-row">
                    <td class="Box-sc-g0xbh4-0 ldpruc" colspan="3">
                     <div>
                      <button class="Link__StyledLink-sc-14289xe-0 dheQRw">
                       View all files
                      </button>
                     </div>
                    </td>
                   </tr>
                  </tbody>
                 </table>
                </div>
                <div class="Box-sc-g0xbh4-0 ehcSsh">
                 <div class="Box-sc-g0xbh4-0 iGmlUb">
                  <div class="Box-sc-g0xbh4-0 iRQGXA">
                   <h2 class="_VisuallyHidden__VisuallyHidden-sc-11jhm7a-0 rTZSs">
                    Repository files navigation
                   </h2>
                   <nav aria-label="Repository files" class="Box-sc-g0xbh4-0 dvTdPK">
                    <ul class="UnderlineNav__NavigationList-sc-1jfr31k-0 bPgibo" role="list">
                     <li class="Box-sc-g0xbh4-0 gwuIGu">
                      <a aria-current="page" class="Link__StyledLink-sc-14289xe-0 vLMkZ" href="#">
                       <span class="Box-sc-g0xbh4-0 kOxwQs" data-component="icon">
                        <svg aria-hidden="true" class="octicon octicon-book" fill="currentColor" focusable="false" height="16" role="img" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible" viewbox="0 0 16 16" width="16">
                         <path d="M0 1.75A.75.75 0 0 1 .75 1h4.253c1.227 0 2.317.59 3 1.501A3.743 3.743 0 0 1 11.006 1h4.245a.75.75 0 0 1 .75.75v10.5a.75.75 0 0 1-.75.75h-4.507a2.25 2.25 0 0 0-1.591.659l-.622.621a.75.75 0 0 1-1.06 0l-.622-.621A2.25 2.25 0 0 0 5.258 13H.75a.75.75 0 0 1-.75-.75Zm7.251 10.324.004-5.073-.002-2.253A2.25 2.25 0 0 0 5.003 2.5H1.5v9h3.757a3.75 3.75 0 0 1 1.994.574ZM8.755 4.75l-.004 7.322a3.752 3.752 0 0 1 1.992-.572H14.5v-9h-3.495a2.25 2.25 0 0 0-2.25 2.25Z">
                         </path>
                        </svg>
                       </span>
                       <span class="Box-sc-g0xbh4-0 kOgeFj" data-component="text" data-content="README">
                        README
                       </span>
                      </a>
                     </li>
                    </ul>
                   </nav>
                   <button aria-haspopup="true" aria-label="Outline" class="types__StyledButton-sc-ws60qy-0 jPraEl" id=":Rr9ab:" style="--button-color:fg.subtle" tabindex="0" type="button">
                    <svg aria-hidden="true" class="octicon octicon-list-unordered" fill="currentColor" focusable="false" height="16" role="img" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible" viewbox="0 0 16 16" width="16">
                     <path d="M5.75 2.5h8.5a.75.75 0 0 1 0 1.5h-8.5a.75.75 0 0 1 0-1.5Zm0 5h8.5a.75.75 0 0 1 0 1.5h-8.5a.75.75 0 0 1 0-1.5Zm0 5h8.5a.75.75 0 0 1 0 1.5h-8.5a.75.75 0 0 1 0-1.5ZM2 14a1 1 0 1 1 0-2 1 1 0 0 1 0 2Zm1-6a1 1 0 1 1-2 0 1 1 0 0 1 2 0ZM2 4a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z">
                     </path>
                    </svg>
                   </button>
                  </div>
                  <div class="Box-sc-g0xbh4-0 bJMeLZ js-snippet-clipboard-copy-unpositioned" data-hpc="true">
                   <article class="markdown-body entry-content container-lg" itemprop="text">
                    <div class="markdown-heading" dir="auto">
                     <h1 class="heading-element" dir="auto" tabindex="-1">
                      Another Awesome Dataset List
                     </h1>
                     <a aria-label="Permalink: Another Awesome Dataset List" class="anchor" href="#another-awesome-dataset-list" id="user-content-another-awesome-dataset-list">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <p dir="auto">
                     <a href="https://github.com/sindresorhus/awesome">
                      <img alt="Awesome" data-canonical-src="https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg" src="https://camo.githubusercontent.com/50cf39121274b3db22bf1bd72cbe25af9078e037441cb5b5bdef1cc9dc5eb2f7/68747470733a2f2f63646e2e7261776769742e636f6d2f73696e647265736f726875732f617765736f6d652f643733303566333864323966656437386661383536353265336136336531353464643865383832392f6d656469612f62616467652e737667" style="max-width: 100%;"/>
                     </a>
                    </p>
                    <p dir="auto">
                     💖 Some Great Tools 💖:
                    </p>
                    <ul dir="auto">
                     <li>
                      ⭐⭐
                      <code>
                       Google Dataset Search
                      </code>
                      :
                      <a href="https://datasetsearch.research.google.com/" rel="nofollow">
                       https://datasetsearch.research.google.com/
                      </a>
                      <ul dir="auto">
                       <li>
                        AI开发者神器! 谷歌重磅推出数据集搜索 Dataset Search:
                        <a href="https://mp.weixin.qq.com/s/ErbwXAz-_AJrmUGMHZIcwg" rel="nofollow">
                         https://mp.weixin.qq.com/s/ErbwXAz-_AJrmUGMHZIcwg
                        </a>
                       </li>
                       <li>
                        Making it easier to discover datasets:
                        <a href="https://www.blog.google/products/search/making-it-easier-discover-datasets/" rel="nofollow">
                         https://www.blog.google/products/search/making-it-easier-discover-datasets/
                        </a>
                       </li>
                      </ul>
                     </li>
                     <li>
                      ⭐⭐⭐
                      <code>
                       Yet Another Computer Vision Index To Datasets (YACVID)
                      </code>
                      :
                      <a href="http://yacvid.hayko.at/" rel="nofollow">
                       http://yacvid.hayko.at/
                      </a>
                     </li>
                    </ul>
                    <blockquote>
                     <p dir="auto">
                      Please
                      <strong>
                       cite related paper
                      </strong>
                      if you
                      <strong>
                       use their dataset
                      </strong>
                      😄
                     </p>
                     <p dir="auto">
                      I list some other datasets in the issue
                      <a class="issue-link js-issue-link" data-error-text="Failed to load title" data-hovercard-type="issue" data-hovercard-url="/lartpang/awesome-segmentation-saliency-dataset/issues/15/hovercard" data-id="484025908" data-permission-text="Title is private" data-url="https://github.com/lartpang/awesome-segmentation-saliency-dataset/issues/15" href="https://github.com/lartpang/awesome-segmentation-saliency-dataset/issues/15">
                       lartpang#15
                      </a>
                      . I hope it works for you.
                     </p>
                    </blockquote>
                    <ul dir="auto">
                     <li>
                      <a href="#another-awesome-dataset-list">
                       Another Awesome Dataset List
                      </a>
                      <ul dir="auto">
                       <li>
                        <a href="#saliency">
                         Saliency
                        </a>
                        <ul dir="auto">
                         <li>
                          <a href="#rgb-saliency-detection">
                           RGB-Saliency Detection
                          </a>
                          <ul dir="auto">
                           <li>
                            <a href="#msramsra10kmsra-b">
                             MSRA(MSRA10K/MSRA-B)
                            </a>
                           </li>
                           <li>
                            <a href="#sed12">
                             SED1/2
                            </a>
                           </li>
                           <li>
                            <a href="#asdmsra1000msra1kneed-some-images">
                             ASD(MSRA1000/MSRA1K)[need some images]
                            </a>
                           </li>
                           <li>
                            <a href="#dut-omron">
                             DUT-OMRON
                            </a>
                           </li>
                           <li>
                            <a href="#duts">
                             DUTS
                            </a>
                           </li>
                           <li>
                            <a href="#hku-isneed-some-iamges">
                             HKU-IS[need some iamges]
                            </a>
                           </li>
                           <li>
                            <a href="#sod">
                             SOD
                            </a>
                           </li>
                           <li>
                            <a href="#icoseg">
                             iCoSeg
                            </a>
                           </li>
                           <li>
                            <a href="#infraredneed-help">
                             Infrared[need help]
                            </a>
                           </li>
                           <li>
                            <a href="#imgsal">
                             ImgSal
                            </a>
                           </li>
                           <li>
                            <a href="#ecssdcssd">
                             ECSSD/CSSD
                            </a>
                           </li>
                           <li>
                            <a href="#thur15k">
                             THUR15K
                            </a>
                           </li>
                           <li>
                            <a href="#bruce-aneed-help">
                             Bruce-A[need help]
                            </a>
                           </li>
                           <li>
                            <a href="#judd-aneed-help">
                             Judd-A[need help]
                            </a>
                           </li>
                           <li>
                            <a href="#pascal-s">
                             PASCAL-S
                            </a>
                           </li>
                           <li>
                            <a href="#ucsbneed-help">
                             UCSB[need help]
                            </a>
                           </li>
                           <li>
                            <a href="#osieneed-help">
                             OSIE[need help]
                            </a>
                           </li>
                           <li>
                            <a href="#acsd">
                             ACSD
                            </a>
                           </li>
                          </ul>
                         </li>
                         <li>
                          <a href="#other-special-sod-datasets">
                           Other Special SOD Datasets
                          </a>
                          <ul dir="auto">
                           <li>
                            <a href="#xpie">
                             XPIE
                            </a>
                           </li>
                           <li>
                            <a href="#soc">
                             SOC
                            </a>
                           </li>
                           <li>
                            <a href="#sosmosneed-some-images">
                             SOS/MOS[need some images]
                            </a>
                           </li>
                           <li>
                            <a href="#ilsoneed-some-images">
                             ILSO[need some images]
                            </a>
                           </li>
                           <li>
                            <a href="#hs-sod">
                             HS-SOD
                            </a>
                           </li>
                          </ul>
                         </li>
                         <li>
                          <a href="#video-saliency-detection">
                           Video Saliency Detection
                          </a>
                          <ul dir="auto">
                           <li>
                            <a href="#rsdpku-rsd">
                             RSD(PKU-RSD)
                            </a>
                           </li>
                           <li>
                            <a href="#stcneed-help">
                             STC[need help]
                            </a>
                           </li>
                          </ul>
                         </li>
                         <li>
                          <a href="#rgbd-saliency-detection">
                           RGBD-Saliency Detection
                          </a>
                          <ul dir="auto">
                           <li>
                            <a href="#sip">
                             SIP
                            </a>
                           </li>
                           <li>
                            <a href="#nlprrgbd1000">
                             NLPR/RGBD1000
                            </a>
                           </li>
                           <li>
                            <a href="#nju4002000">
                             NJU400/2000
                            </a>
                           </li>
                           <li>
                            <a href="#stereossb">
                             STEREO/SSB
                            </a>
                           </li>
                           <li>
                            <a href="#lfsdnead-img">
                             LFSD[nead img]
                            </a>
                           </li>
                           <li>
                            <a href="#rgbd135des">
                             RGBD135/DES
                            </a>
                           </li>
                           <li>
                            <a href="#dut-rgbd">
                             DUT-RGBD
                            </a>
                           </li>
                           <li>
                            <a href="#ssd100">
                             SSD100
                            </a>
                           </li>
                          </ul>
                         </li>
                         <li>
                          <a href="#rgbt-saliency-detection-need-more-information">
                           RGBT-Saliency Detection [need more information...]
                          </a>
                          <ul dir="auto">
                           <li>
                            <a href="#vt1000-dataset">
                             VT1000 Dataset
                            </a>
                           </li>
                           <li>
                            <a href="#vt821-dataset">
                             VT821 Dataset
                            </a>
                           </li>
                          </ul>
                         </li>
                         <li>
                          <a href="#high-resolution-saliency-detection">
                           High-Resolution Saliency Detection
                          </a>
                          <ul dir="auto">
                           <li>
                            <a href="#hrsoddavis-s">
                             HRSOD/DAVIS-S
                            </a>
                           </li>
                          </ul>
                         </li>
                         <li>
                          <a href="#other-saliency-dataset">
                           Other Saliency Dataset
                          </a>
                          <ul dir="auto">
                           <li>
                            <a href="#kaist-salient-pedestrian-dataset">
                             KAIST Salient Pedestrian Dataset
                            </a>
                           </li>
                          </ul>
                         </li>
                        </ul>
                       </li>
                       <li>
                        <a href="#segmentation">
                         Segmentation
                        </a>
                        <ul dir="auto">
                         <li>
                          <a href="#generalneed-help">
                           General[need help]
                          </a>
                          <ul dir="auto">
                           <li>
                            <a href="#davis">
                             DAVIS
                            </a>
                           </li>
                           <li>
                            <a href="#anyu">
                             aNYU
                            </a>
                           </li>
                          </ul>
                         </li>
                         <li>
                          <a href="#about-person">
                           About Person
                          </a>
                          <ul dir="auto">
                           <li>
                            <a href="#supervisely%E4%BA%BA%E5%83%8F%E6%95%B0%E6%8D%AE%E9%9B%86">
                             Supervisely人像数据集
                            </a>
                           </li>
                           <li>
                            <a href="#clothing-parsing">
                             Clothing Parsing
                            </a>
                           </li>
                           <li>
                            <a href="#humanparsing-dataset">
                             HumanParsing-Dataset
                            </a>
                           </li>
                           <li>
                            <a href="#look-into-person-lip">
                             Look into Person (LIP)
                            </a>
                           </li>
                           <li>
                            <a href="#taobao-commodity-dataset">
                             Taobao Commodity Dataset
                            </a>
                           </li>
                           <li>
                            <a href="#object-extraction-dataset">
                             Object Extraction Dataset
                            </a>
                           </li>
                           <li>
                            <a href="#clothing-co-parsing-ccp-dataset">
                             Clothing Co-Parsing (CCP) Dataset
                            </a>
                           </li>
                           <li>
                            <a href="#baidu-people-segmentation-datasetneed-help">
                             Baidu People segmentation dataset[need help]
                            </a>
                           </li>
                          </ul>
                         </li>
                        </ul>
                       </li>
                       <li>
                        <a href="#matting">
                         Matting
                        </a>
                        <ul dir="auto">
                         <li>
                          <a href="#alphamattingcom">
                           alphamatting.com
                          </a>
                         </li>
                         <li>
                          <a href="#composition-1k-deep-image-matting">
                           Composition-1k: Deep Image Matting
                          </a>
                         </li>
                         <li>
                          <a href="#semantic-human-matting">
                           Semantic Human Matting
                          </a>
                         </li>
                         <li>
                          <a href="#matting-human-datasets">
                           Matting-Human-Datasets
                          </a>
                         </li>
                         <li>
                          <a href="#pfcn">
                           PFCN
                          </a>
                         </li>
                         <li>
                          <a href="#deep-automatic-portrait-matting">
                           Deep Automatic Portrait Matting
                          </a>
                         </li>
                        </ul>
                       </li>
                       <li>
                        <a href="#other">
                         Other
                        </a>
                        <ul dir="auto">
                         <li>
                          <a href="#large-scale-fashion-deepfashion-database">
                           Large-scale Fashion (DeepFashion) Database
                          </a>
                         </li>
                         <li>
                          <a href="#ml-image">
                           ML-Image
                          </a>
                         </li>
                        </ul>
                       </li>
                       <li>
                        <a href="#need-your-help">
                         need your help...
                        </a>
                       </li>
                       <li>
                        <a href="#reference">
                         Reference
                        </a>
                        <ul dir="auto">
                         <li>
                          <a href="#salient-object-detection-a-survey">
                           Salient Object Detection: A Survey
                          </a>
                         </li>
                         <li>
                          <a href="#review-of-visual-saliency-detection-with-comprehensive-information">
                           Review of Visual Saliency Detection with Comprehensive Information
                          </a>
                         </li>
                         <li>
                          <a href="#salient-object-detection-in-the-deep-learning-era-an-in-depth-survey">
                           Salient Object Detection in the Deep Learning Era: An In-Depth Survey
                          </a>
                         </li>
                        </ul>
                       </li>
                       <li>
                        <a href="#more">
                         More
                        </a>
                        <ul dir="auto">
                         <li>
                          <a href="#similiar-projects">
                           Similiar Projects
                          </a>
                         </li>
                         <li>
                          <a href="#research-institutes">
                           Research Institutes
                          </a>
                         </li>
                         <li>
                          <a href="#resource-websites">
                           Resource Websites
                          </a>
                         </li>
                        </ul>
                       </li>
                       <li>
                        <a href="#about">
                         About
                        </a>
                       </li>
                      </ul>
                     </li>
                    </ul>
                    <div class="markdown-heading" dir="auto">
                     <h2 class="heading-element" dir="auto" tabindex="-1">
                      Saliency
                     </h2>
                     <a aria-label="Permalink: Saliency" class="anchor" href="#saliency" id="user-content-saliency">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <div class="markdown-heading" dir="auto">
                     <h3 class="heading-element" dir="auto" tabindex="-1">
                      RGB-Saliency Detection
                     </h3>
                     <a aria-label="Permalink: RGB-Saliency Detection" class="anchor" href="#rgb-saliency-detection" id="user-content-rgb-saliency-detection">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <div class="markdown-heading" dir="auto">
                     <h4 class="heading-element" dir="auto" tabindex="-1">
                      MSRA(MSRA10K/MSRA-B)
                     </h4>
                     <a aria-label="Permalink: MSRA(MSRA10K/MSRA-B)" class="anchor" href="#msramsra10kmsra-b" id="user-content-msramsra10kmsra-b">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <p dir="auto">
                     <a href="https://camo.githubusercontent.com/3250531d515ac0cc26ef6edbe4cca3aa7338f54720a59159147d1594919fc8da/68747470733a2f2f6d6d6368656e672e6e65742f77702d636f6e74656e742f75706c6f6164732f323031342f30372f4d53524131304b2e6a7067" rel="noopener noreferrer nofollow" target="_blank">
                      <img alt="img" data-canonical-src="https://mmcheng.net/wp-content/uploads/2014/07/MSRA10K.jpg" src="https://camo.githubusercontent.com/3250531d515ac0cc26ef6edbe4cca3aa7338f54720a59159147d1594919fc8da/68747470733a2f2f6d6d6368656e672e6e65742f77702d636f6e74656e742f75706c6f6164732f323031342f30372f4d53524131304b2e6a7067" style="max-width: 100%;"/>
                     </a>
                    </p>
                    <ul dir="auto">
                     <li>
                      论文:
                      <a href="http://mmlab.ie.cuhk.edu.hk/2007/CVPR07_detect.pdf" rel="nofollow">
                       T. Liu, J. Sun, N. Zheng, X. Tang, and H.-Y. Shum, "Learningto detect a salient object, " inCVPR, 2007, pp.1–8
                      </a>
                     </li>
                     <li>
                      主页: 南开大学媒体计算实验室:
                      <a href="https://mmcheng.net/zh/msra10k/" rel="nofollow">
                       https://mmcheng.net/zh/msra10k/
                      </a>
                     </li>
                     <li>
                      下载:
                      <ul dir="auto">
                       <li>
                        MSRA10K(formally named as THUS10000;
                        <a href="http://mftp.mmcheng.net/Data/MSRA10K_Imgs_GT.zip" rel="nofollow">
                         195MB
                        </a>
                        : images + binary masks):
                        <ul dir="auto">
                         <li>
                          Pixel accurate salient object labeling for
                          <strong>
                           10000 images
                          </strong>
                          from MSRA dataset.
                         </li>
                         <li>
                          Please cite our paper [
                          <a href="https://mmcheng.net/SalObj/" rel="nofollow">
                           https://mmcheng.net/SalObj/
                          </a>
                          ] if you use it.
                         </li>
                         <li>
                          Saliency maps and salient object region segmentation for other 20+ alternative methods are also available (
                          <a href="http://pan.baidu.com/s/1dEaQqlF#path=%252FShare%252FSalObjRes" rel="nofollow">
                           百度网盘
                          </a>
                          ).
                         </li>
                        </ul>
                       </li>
                       <li>
                        MSRA-B (
                        <a href="http://mftp.mmcheng.net/Data/MSRA-B.zip" rel="nofollow">
                         111MB
                        </a>
                        : images + binary masks):
                        <ul dir="auto">
                         <li>
                          Pixel accurate salient object labeling for
                          <strong>
                           5000 images
                          </strong>
                          from MSRA-B dataset.
                         </li>
                         <li>
                          Please cite the corresponding paper [
                          <a href="https://mmcheng.net/drfi/" rel="nofollow">
                           https://mmcheng.net/drfi/
                          </a>
                          ] if you use it.
                         </li>
                        </ul>
                       </li>
                      </ul>
                     </li>
                    </ul>
                    <blockquote>
                     <p dir="auto">
                      我们通过检测输入图像中的显着对象来研究视觉注意力. 我们将显着对象检测表示为图像分割问题, 我们将显着对象与图像背景分开. 我们提出了一系列新颖的特征, 包括多尺度对比度, 中心环绕直方图和颜色空间分布, 以在本地, 区域和全局描述显着对象. 学习条件随机场以有效地组合这些特征以用于显着对象检测. 我们还构建了一个
                      <strong>
                       包含由多个用户标记的数以万计的完全标记图像的图像数据库
                      </strong>
                      . 据我们所知, 它是第一个用于视觉注意算法定量评估的大型图像数据库. 我们在此图像数据库上验证了我们的方法, 该数据库在本文中是公开的.
                     </p>
                     <p dir="auto">
                      人们可能对图像中的显着对象有不同的看法. 为了解决"给定图像中可能是什么样的显着对象"的问题, 我们通过在多个用户的图像中标记"基础事实"显着对象来进行投票策略. 在本文中, 我们关注图像中单个显着对象的情况.
                     </p>
                     <p dir="auto">
                      显著性对象表示. 通常, 我们
                      <strong>
                       将给定对象表示为给定image I中的二元mask
                      </strong>
                      <math-renderer class="js-inline-math" data-run-id="41dafde2f9a470d0c64156772edde64a" data-static-url="https://github.githubassets.com/static" style="display: inline">
                       $A={a_x}$
                      </math-renderer>
                      . 对于每个像素x,
                      <math-renderer class="js-inline-math" data-run-id="41dafde2f9a470d0c64156772edde64a" data-static-url="https://github.githubassets.com/static" style="display: inline">
                       $a_x∈{1, 0}$
                      </math-renderer>
                      是二进制标签, 以指示像素是否属于显着对象.
                      <strong>
                       为了标记和评估, 我们要求用户绘制一个矩形来指定一个显着对象. 我们的检测算法也输出一个矩形.
                      </strong>
                     </p>
                     <p dir="auto">
                      图像来源. 我们收集了一个非常大的图像数据库, 其中130, 099个来自各种来源的高质量图像, 主要来自图像论坛和图像搜索引擎. 然后我们手动选择60, 000多个图像, 每个图像包含一个显着对象或一个独特的前景对象. 我们进一步选择了20, 840张图片进行标记. 在选择过程中, 我们
                      <strong>
                       排除了包含非常大的显着对象的任何图像
                      </strong>
                      , 从而可以更准确地评估检测的性能.
                     </p>
                     <p dir="auto">
                      标记一致性. 对于每个要标记的图像, 我们请用户绘制一个矩形, 该矩形包围图像中最大的对象根据他/她自己的理解. 由不同用户标记的矩形通常不相同. 为了减少标签的不一致性, 我们从多个用户绘制的矩形中选择一个"真实"标签.
                     </p>
                    </blockquote>
                    <div class="markdown-heading" dir="auto">
                     <h4 class="heading-element" dir="auto" tabindex="-1">
                      SED1/2
                     </h4>
                     <a aria-label="Permalink: SED1/2" class="anchor" href="#sed12" id="user-content-sed12">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <ul dir="auto">
                     <li>
                      单目标
                     </li>
                    </ul>
                    <p dir="auto">
                     <a href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/2018-12-29-18-38-59.png" rel="noopener noreferrer" target="_blank">
                      <img alt="img" src="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/2018-12-29-18-38-59.png" style="max-width: 100%;"/>
                     </a>
                    </p>
                    <ul dir="auto">
                     <li>
                      双目标
                     </li>
                    </ul>
                    <p dir="auto">
                     <a href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/2018-12-29-18-39-30.png" rel="noopener noreferrer" target="_blank">
                      <img alt="img" src="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/2018-12-29-18-39-30.png" style="max-width: 100%;"/>
                     </a>
                    </p>
                    <ul dir="auto">
                     <li>
                      真值
                     </li>
                    </ul>
                    <p dir="auto">
                     给出的是每个图像由三个不同的人类对象分割的结果.
                    </p>
                    <p dir="auto">
                     <a href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/2018-12-29-18-40-17.png" rel="noopener noreferrer" target="_blank">
                      <img alt="img" src="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/2018-12-29-18-40-17.png" style="max-width: 100%;"/>
                     </a>
                    </p>
                    <ul dir="auto">
                     <li>
                      <a href="https://arxiv.org/abs/1501.02741" rel="nofollow">
                       A. Borji, M.-M. Cheng, H. Jiang, and J. Li, "Salient objectdetection: A benchmark, "IEEE TIP, vol.24, no.12, pp.5706–5722, 2015.
                      </a>
                     </li>
                     <li>
                      <a href="http://www.wisdom.weizmann.ac.il/~meirav/Segmentation_Alpert_Galun_Brandt_Basri.pdf" rel="nofollow">
                       Image Segmentation by Probabilistic Bottom-Up Aggregation and Cue Integration
                      </a>
                     </li>
                     <li>
                      项目:
                      <a href="http://www.wisdom.weizmann.ac.il/~vision/Seg_Evaluation_DB/index.html" rel="nofollow">
                       http://www.wisdom.weizmann.ac.il/~vision/Seg_Evaluation_DB/index.html
                      </a>
                     </li>
                     <li>
                      下载:
                      <a href="http://www.wisdom.weizmann.ac.il/~vision/Seg_Evaluation_DB/dl.html" rel="nofollow">
                       http://www.wisdom.weizmann.ac.il/~vision/Seg_Evaluation_DB/dl.html
                      </a>
                     </li>
                    </ul>
                    <blockquote>
                     <p dir="auto">
                      这项工作的目的是为图像分割研究提供经验和科学依据. 评估分割算法产生的结果具有挑战性, 因为很难提出提供基础真实分割的规范测试集. 这部分是因为在日常复杂图像中手动描绘片段可能是费力的. 此外, 人们往往倾向于将语义考虑纳入其分段中, 这超出了数据驱动的分割算法的范围. 因此, 许多现有算法仅显示很少的分割结果. 为了评估由不同算法产生的分割, 我们编制了一个数据库, 目前
                      <strong>
                       包含200个灰度图像以及真实标注分割
                      </strong>
                      . 该数据库专门设计用于避免潜在的模糊, 仅通过仅通过强度, 纹理或其他低水平线索合并清晰描绘前景中与其周围环境不同的一个或两个物体的图像. 通过要求人类对象手动地将灰度图像(还提供颜色源)分成两个或三个类别来获得地面真实分割, 其中
                      <strong>
                       每个图像由三个不同的人类对象分割
                      </strong>
                      . 通过评估其与真实分割的一致性及其碎片量来评估分割. 与此数据库评估一起, 我们提供了用于评估给定分割算法的代码. 这样, 不同的分割算法可能具有可比较的结果以获得更多细节, 请参阅"评估测试"部分.
                     </p>
                    </blockquote>
                    <div class="markdown-heading" dir="auto">
                     <h4 class="heading-element" dir="auto" tabindex="-1">
                      ASD(MSRA1000/MSRA1K)[need some images]
                     </h4>
                     <a aria-label="Permalink: ASD(MSRA1000/MSRA1K)[need some images]" class="anchor" href="#asdmsra1000msra1kneed-some-images" id="user-content-asdmsra1000msra1kneed-some-images">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <ul dir="auto">
                     <li>
                      论文:
                      <a href="https://www.researchgate.net/publication/224312323_A_two-stage_approach_to_saliency_detection_in_images" rel="nofollow">
                       A two-stage approach to saliency detection inimages
                      </a>
                     </li>
                     <li>
                      相关:
                      <ul dir="auto">
                       <li>
                        T. Liu, J. Sun, N.-N. Zheng, X. Tang, and H.-Y. Shum, "
                        <a href="http://research.microsoft.com/en-us/um/people/jiansun/salientobject/salient_object.htm" rel="nofollow">
                         Learning to detect a salient object
                        </a>
                        , " in
                        <em>
                         Proc. IEEE Conf. Comput. Vis. Pattern Recognit.
                        </em>
                        , 2007, pp.1–8.
                       </li>
                       <li>
                        R. Achanta, S. Hemami, F. Estrada, and S. Süsstrunk, "
                        <a href="http://ivrlwww.epfl.ch/supplementary_material/RK_CVPR09/" rel="nofollow">
                         Frequency-tuned salient region detection
                        </a>
                        , " in
                        <em>
                         Proc. IEEE Conf. Comput. Vis. Pattern Recognit.
                        </em>
                        , 2009, pp.1597–1604.
                       </li>
                      </ul>
                     </li>
                     <li>
                      下载:
                      <a href="http://download.csdn.net/detail/wanyq07/9839322" rel="nofollow">
                       http://download.csdn.net/detail/wanyq07/9839322
                      </a>
                      <ul dir="auto">
                       <li>
                        关于下载的说明: 因为基于MSRA的图片数据集, 在孙剑走了之后, MARA上就没了他的页面, 相关的资源也就找不到了. CSDN一篇博客有分享. 原图下载地址:
                        <a href="http://download.csdn.net/detail/tuconghuan/8357509" rel="nofollow">
                         MSRA图像数据集(1000幅含真实标注)
                        </a>
                        . 上面下载到的标注图尺寸被统一改为512*512, 所以这里在给个地址:
                        <a href="http://download.csdn.net/detail/zzb4702/9559378" rel="nofollow">
                         ASD尺寸一致
                        </a>
                       </li>
                      </ul>
                     </li>
                    </ul>
                    <blockquote>
                     <p dir="auto">
                      ASD contains 1, 000 images with pixel-wise ground-truths. The images are selected from the MSRA-A dataset, where only the bounding boxes around salient regions are provided. The accurate salient masks in ASD are created based on object contours.
                     </p>
                     <p dir="auto">
                      这个数据集包含有1000张图(MSRA1000)这个数据库来自于 该数据库的说明以及一些算法(IT, MZ, GB, SR, AC, IG ) 的结果可以在
                      <a href="http://ivrlwww.epfl.ch/supplementary_material/RK_CVPR09/index.html" rel="nofollow">
                       Frequency-tuned Salient Region Detection
                      </a>
                      (FT算法 =&gt; 这里改进的数据集叫做ACSD, 相关可见
                      <a href="#ACSD">
                       ACSD
                      </a>
                      部分)下载, 此外其中还包含了这1000张测试图的真值图.
                     </p>
                    </blockquote>
                    <div class="markdown-heading" dir="auto">
                     <h4 class="heading-element" dir="auto" tabindex="-1">
                      DUT-OMRON
                     </h4>
                     <a aria-label="Permalink: DUT-OMRON" class="anchor" href="#dut-omron" id="user-content-dut-omron">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <p dir="auto">
                     <a href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/2019-03-22-18-45-56.png" rel="noopener noreferrer" target="_blank">
                      <img alt="img" src="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/2019-03-22-18-45-56.png" style="max-width: 100%;"/>
                     </a>
                    </p>
                    <ul dir="auto">
                     <li>
                      论文: C. Yang, L. Zhang, H. Lu, X. Ruan, and M.-H. Yang, "
                      <a href="http://saliencydetection.net/dut-omron/" rel="nofollow">
                       Saliency detection via graph-based manifold ranking
                      </a>
                      , " in
                      <em>
                       Proc. IEEE Conf. Comput. Vis. Pattern Recognit.
                      </em>
                      , 2013, pp.3166–3173.
                     </li>
                     <li>
                      项目:
                      <a href="http://saliencydetection.net/dut-omron/#outline-container-org0e04792" rel="nofollow">
                       http://saliencydetection.net/dut-omron/#outline-container-org0e04792
                      </a>
                     </li>
                     <li>
                      下载:
                      <a href="http://saliencydetection.net/dut-omron/download/DUT-OMRON-image.zip" rel="nofollow">
                       http://saliencydetection.net/dut-omron/download/DUT-OMRON-image.zip
                      </a>
                     </li>
                    </ul>
                    <blockquote>
                     <p dir="auto">
                      数据库包括从超过140, 000张图像中手动选择的5, 168个高质量图像. 我们将图像的大小调整为宽为400或高为400像素, 其中另一条边小于400. 我们数据库的图像具有一个或多个显着对象和相对复杂的背景. 我们共有25名参与者, 用于汇总真值, 每个图像有五个参与者标签. 他们都有正常或矫正到正常的视力并且意识到我们实验的目标. 我们为提出的数据库构建像素方面的真实标注, 边界框, 和眼睛固定标注真值.
                     </p>
                     <p dir="auto">
                      我们的数据集是唯一一个具有眼睛固定, 边界框和像素方面的大规模真实标注的数据集. 与ASD和MSRA数据集以及其他一些眼睛固定数据集(即MIT和NUSEF数据集)相比, 数据集中的图像更加困难, 因此更具挑战性, 并为相关的显着性研究提供了更多的改进空间.
                     </p>
                    </blockquote>
                    <div class="markdown-heading" dir="auto">
                     <h4 class="heading-element" dir="auto" tabindex="-1">
                      DUTS
                     </h4>
                     <a aria-label="Permalink: DUTS" class="anchor" href="#duts" id="user-content-duts">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <ul dir="auto">
                     <li>
                      项目:
                      <a href="http://saliencydetection.net/duts/" rel="nofollow">
                       http://saliencydetection.net/duts/
                      </a>
                     </li>
                    </ul>
                    <blockquote>
                     <p dir="auto">
                      ...we contribute a large scale data set named DUTS,
                      <strong>
                       containing 10, 553 training images and 5, 019 test images
                      </strong>
                      . All training images are collected from the ImageNet DET training/val sets, while test images are collected from the ImageNet DET test set and the SUN data set.
                     </p>
                     <p dir="auto">
                      Both the training and test set contain very challenging scenarios for saliency detection. Accurate pixel-level ground truths are manually annotated by 50 subjects.
                     </p>
                     <p dir="auto">
                      To our knowledge, DUTS is currently
                      <strong>
                       the largest saliency detection benchmark
                      </strong>
                      with the explicit training/test evaluation protocol.
                     </p>
                     <p dir="auto">
                      For fair comparison in the future research, the training set of DUTS serves as a good candidate for learning DNNs, while the test set and other public data sets can be used for evaluation.
                     </p>
                    </blockquote>
                    <div class="markdown-heading" dir="auto">
                     <h4 class="heading-element" dir="auto" tabindex="-1">
                      HKU-IS[need some iamges]
                     </h4>
                     <a aria-label="Permalink: HKU-IS[need some iamges]" class="anchor" href="#hku-isneed-some-iamges" id="user-content-hku-isneed-some-iamges">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <ul dir="auto">
                     <li>
                      项目:
                      <a href="https://i.cs.hku.hk/~gbli/deep_saliency.html" rel="nofollow">
                       https://i.cs.hku.hk/~gbli/deep_saliency.html
                      </a>
                     </li>
                     <li>
                      论文:
                      <a href="http://i.cs.hku.hk/~yzyu/publication/mdfsaliency-cvpr15.pdf" rel="nofollow">
                       Visual Saliency Based on Multiscale Deep Features
                      </a>
                     </li>
                     <li>
                      下载:
                      <ul dir="auto">
                       <li>
                        <a href="https://drive.google.com/open?id=0BxNhBO0S5JCRQ1N6V25VeVh6cHc&amp;authuser=0" rel="nofollow">
                         Google Drive
                        </a>
                       </li>
                       <li>
                        <a href="http://pan.baidu.com/s/1c0EpNfM" rel="nofollow">
                         Baidu Yun
                        </a>
                       </li>
                      </ul>
                     </li>
                    </ul>
                    <blockquote>
                     <p dir="auto">
                      数据集包含4447个具有显着对象的像素注释的图像
                     </p>
                     <p dir="auto">
                      视觉显着性是包括计算机视觉在内的认知和计算科学中的一个基本问题. 在本文中, 我们发现可以从使用深度卷积神经网络(CNN)提取的多尺度特征中学习高质量的视觉显着性模型. 视觉识别任务的成功. 为了学习这样的显着性模型, 我们引入了一种神经网络结构, 它在CNN顶部具有完全连接的层, 负责三个不同尺度的特征提取. 然后, 我们提出一种改进方法来增强我们的显着性结果的空间一致性. 最后, 针对不同级别的图像分割计算的聚合多个显着性图可以进一步提高性能, 从而产生比由单个分割产生的显着性图更好的显着性图. 为了促进对视觉显着性模型的进一步研究和评估,
                      <strong>
                       我们还构建了一个新的大型数据库, 包括4447个具有挑战性的图像及其像素显着性注释
                      </strong>
                      .
                     </p>
                    </blockquote>
                    <div class="markdown-heading" dir="auto">
                     <h4 class="heading-element" dir="auto" tabindex="-1">
                      SOD
                     </h4>
                     <a aria-label="Permalink: SOD" class="anchor" href="#sod" id="user-content-sod">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <p dir="auto">
                     <a href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/2019-03-22-18-46-40.png" rel="noopener noreferrer" target="_blank">
                      <img alt="img" src="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/2019-03-22-18-46-40.png" style="max-width: 100%;"/>
                     </a>
                    </p>
                    <ul dir="auto">
                     <li>
                      项目:
                      <a href="http://elderlab.yorku.ca/SOD/" rel="nofollow">
                       http://elderlab.yorku.ca/SOD/
                      </a>
                     </li>
                     <li>
                      下载
                      <ul dir="auto">
                       <li>
                        官方:
                        <a href="http://elderlab.yorku.ca/SOD/SOD.zip" rel="nofollow">
                         http://elderlab.yorku.ca/SOD/SOD.zip
                        </a>
                       </li>
                       <li>
                        百度云:
                        <a href="https://pan.baidu.com/s/1IMElTPwD4yTo2TMSRU-keQ" rel="nofollow">
                         https://pan.baidu.com/s/1IMElTPwD4yTo2TMSRU-keQ
                        </a>
                       </li>
                      </ul>
                     </li>
                    </ul>
                    <blockquote>
                     <p dir="auto">
                      此数据集是基于Berkeley Segmentation Dataset(BSD)的显着对象边界的集合. 要求七个对象选择BSD中使用的每个图像中的显着对象. 每个主题随机显示伯克利分割数据集的子集, 作为在相应图像上重叠的边界. 然后, 可以通过单击选择哪些区域或区段对应于显着对象.
                     </p>
                     <p dir="auto">
                      对于BSD中使用的300个图像的每个图像, 都有一个.mat文件可以由Matlab打开. 加载每个mat文件会将一个名为"SES"的结构读入内存, 该结构是从SOD中每个主题的会话中收集的数据数组.
                     </p>
                     <p dir="auto">
                      💝 that the original images are available from the Berkely Segmentation Dataset at:
                      <a href="http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/segbench/" rel="nofollow">
                       http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/segbench/
                      </a>
                     </p>
                    </blockquote>
                    <div class="markdown-heading" dir="auto">
                     <h4 class="heading-element" dir="auto" tabindex="-1">
                      iCoSeg
                     </h4>
                     <a aria-label="Permalink: iCoSeg" class="anchor" href="#icoseg" id="user-content-icoseg">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <p dir="auto">
                     <a href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/1546085516505.png" rel="noopener noreferrer" target="_blank">
                      <img alt="1546085516505" src="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/1546085516505.png" style="max-width: 100%;"/>
                     </a>
                    </p>
                    <ul dir="auto">
                     <li>
                      论文:
                      <a href="http://chenlab.ece.cornell.edu/projects/touch-coseg/iCoseg_dataset.pdf" rel="nofollow">
                       http://chenlab.ece.cornell.edu/projects/touch-coseg/iCoseg_dataset.pdf
                      </a>
                     </li>
                     <li>
                      项目:
                      <a href="http://chenlab.ece.cornell.edu/projects/touch-coseg/" rel="nofollow">
                       http://chenlab.ece.cornell.edu/projects/touch-coseg/
                      </a>
                     </li>
                     <li>
                      下载:
                      <a href="http://chenlab.ece.cornell.edu/projects/touch-coseg/CMU_Cornell_iCoseg_dataset.zip" rel="nofollow">
                       http://chenlab.ece.cornell.edu/projects/touch-coseg/CMU_Cornell_iCoseg_dataset.zip
                      </a>
                     </li>
                    </ul>
                    <blockquote>
                     <p dir="auto">
                      我们引入了38组(643幅图像)中最大的公开可用的 co-segmentation, 以及像素标注真值.
                     </p>
                    </blockquote>
                    <div class="markdown-heading" dir="auto">
                     <h4 class="heading-element" dir="auto" tabindex="-1">
                      Infrared[need help]
                     </h4>
                     <a aria-label="Permalink: Infrared[need help]" class="anchor" href="#infraredneed-help" id="user-content-infraredneed-help">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <ul dir="auto">
                     <li>
                      项目:
                      <a href="https://ivrl.epfl.ch/research-2/research-downloads/supplementary_material-cvpr11-index-html/" rel="nofollow">
                       https://ivrl.epfl.ch/research-2/research-downloads/supplementary_material-cvpr11-index-html/
                      </a>
                     </li>
                     <li>
                      论文:
                      <a href="http://infoscience.epfl.ch/record/167478" rel="nofollow">
                       http://infoscience.epfl.ch/record/167478
                      </a>
                     </li>
                     <li>
                      下载:
                      <a href="http://ivrgwww.epfl.ch/supplementary_material/cvpr11/nirscene1.zip" rel="nofollow">
                       http://ivrgwww.epfl.ch/supplementary_material/cvpr11/nirscene1.zip
                      </a>
                     </li>
                    </ul>
                    <blockquote>
                     <p dir="auto">
                      我们使用对传统SLR相机的简单修改来捕获数百个彩色(RGB)和近红外(NIR)场景的图像. 我们表明, 近红外信息的添加导致场景识别任务中的性能显着提高, 并且当使用适当的4维颜色表示时, 改进仍然更大. 特别地, 我们提出了MSIFT-一种多光谱SIFT描述符, 当与基于内核的分类器结合时, 超过了现有技术的场景识别技术(例如GIST)及其多光谱扩展的性能. 我们使用数百个RGB-NIR场景图像的新数据集对我们的算法进行了广泛的测试, 并对Torralba的场景分类数据集进行了基准测试.
                     </p>
                    </blockquote>
                    <div class="markdown-heading" dir="auto">
                     <h4 class="heading-element" dir="auto" tabindex="-1">
                      ImgSal
                     </h4>
                     <a aria-label="Permalink: ImgSal" class="anchor" href="#imgsal" id="user-content-imgsal">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <p dir="auto">
                     <a href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/1546087781641.png" rel="noopener noreferrer" target="_blank">
                      <img alt="1546087781641" src="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/1546087781641.png" style="max-width: 100%;"/>
                     </a>
                    </p>
                    <ul dir="auto">
                     <li>
                      项目:
                      <a href="https://sites.google.com/site/jianlinudt/saliency-database" rel="nofollow">
                       https://sites.google.com/site/jianlinudt/saliency-database
                      </a>
                     </li>
                     <li>
                      作者主页:
                      <a href="http://www.escience.cn/people/jianli/DataBase.html" rel="nofollow">
                       http://www.escience.cn/people/jianli/DataBase.html
                      </a>
                     </li>
                    </ul>
                    <blockquote>
                     <p dir="auto">
                      数据库的特点
                     </p>
                     <p dir="auto">
                      1.235个彩色图像的集合, 分为六个不同的类别;
2. 提供人类固定记录(扫视数据)和人类标记结果;
3. 易于使用.
                     </p>
                     <p dir="auto">
                      我们将同时考虑不同大小的显着区域的检测. 实际上, 可接受的显着性检测器应该检测大的和小的显着区域. 此外, 显着性检测还应该定位杂乱背景中的显着区域和具有重复干扰物的区域. 我们还注意到, 对于任何显着性检测器, 不同的图像呈现不同的难度. 但是, 现有的显着性基准(例如Bruce的数据集, Hou'dataset, Harel的数据集等)是图像集合, 没有尝试对所需分析的难度进行分类. 因此, 我们为显着性模型验证创建了一个新的显着性基准. 该数据库提供REGION基础事实(人类标记)和FIXATION基础事实(通过眼动仪).
                     </p>
                     <p dir="auto">
                      图像集使用Google以及参考最近的文献收集了包含235张图像的数据库. 此数据库中的图像为480 x 640像素, 分为6类:1)50个具有大显着区域的图像; 2)具有中间显着区域的80幅图像; 3)具有小显着区域的60幅图像; 4)背景杂乱的15幅图像; 5)带有重复干扰物的15张图像; 6)具有大和小显着区域的15个图像.
                     </p>
                    </blockquote>
                    <div class="markdown-heading" dir="auto">
                     <h4 class="heading-element" dir="auto" tabindex="-1">
                      ECSSD/CSSD
                     </h4>
                     <a aria-label="Permalink: ECSSD/CSSD" class="anchor" href="#ecssdcssd" id="user-content-ecssdcssd">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <p dir="auto">
                     <a href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/2019-03-22-18-47-32.png" rel="noopener noreferrer" target="_blank">
                      <img alt="img" src="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/2019-03-22-18-47-32.png" style="max-width: 100%;"/>
                     </a>
                    </p>
                    <ul dir="auto">
                     <li>
                      下载:
                      <a href="http://www.cse.cuhk.edu.hk/leojia/projects/hsaliency/dataset.html" rel="nofollow">
                       http://www.cse.cuhk.edu.hk/leojia/projects/hsaliency/dataset.html
                      </a>
                      <ul dir="auto">
                       <li>
                        ECSSD (1000 images)
                        <ul dir="auto">
                         <li>
                          <a href="http://www.cse.cuhk.edu.hk/leojia/projects/hsaliency/data/ECSSD/images.zip" rel="nofollow">
                           ECSSD images (64.6MB)
                          </a>
                         </li>
                         <li>
                          <a href="http://www.cse.cuhk.edu.hk/leojia/projects/hsaliency/data/ECSSD/ground_truth_mask.zip" rel="nofollow">
                           ECSSD ground truth masks (1.78MB)
                          </a>
                          (Updated on 9 April, 2015)
                         </li>
                        </ul>
                       </li>
                       <li>
                        CSSD (200 images)
                        <ul dir="auto">
                         <li>
                          <a href="http://www.cse.cuhk.edu.hk/leojia/projects/hsaliency/data/CSSD/images.zip" rel="nofollow">
                           CSSD images (18.7MB)
                          </a>
                         </li>
                         <li>
                          <a href="http://www.cse.cuhk.edu.hk/leojia/projects/hsaliency/data/CSSD/ground_truth_mask.zip" rel="nofollow">
                           CSSD groud truth masks (0.75MB)
                          </a>
                         </li>
                        </ul>
                       </li>
                      </ul>
                     </li>
                    </ul>
                    <p dir="auto">
                     其中CSSD包含了200张图, 而ECSSD是前者的扩展集包含有1000张图
                    </p>
                    <blockquote>
                     <p dir="auto">
                      虽然MSRA-1000的图像内容种类繁多, 但背景结构主要是简单而流畅. 为了表示自然图像通常落入的情况, 我们将[1]中的复杂场景显着性数据集(CSSD)扩展到包含1000个图像的更大数据集(ECSSD)[2], 其中包含许多语义上有意义但结构复杂的图像用于评估. 这些图像是从互联网上获取的, 并要求5名助手制作地面真相面具. 上面显示了几个带有相应掩模的例子.
                     </p>
                    </blockquote>
                    <div class="markdown-heading" dir="auto">
                     <h4 class="heading-element" dir="auto" tabindex="-1">
                      THUR15K
                     </h4>
                     <a aria-label="Permalink: THUR15K" class="anchor" href="#thur15k" id="user-content-thur15k">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <p dir="auto">
                     <a href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/1546088375285.png" rel="noopener noreferrer" target="_blank">
                      <img alt="1546088375285" src="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/1546088375285.png" style="max-width: 100%;"/>
                     </a>
                    </p>
                    <ul dir="auto">
                     <li>
                      论文:
                      <a href="https://mmcheng.net/zh/gsal/" rel="nofollow">
                       https://mmcheng.net/zh/gsal/
                      </a>
                     </li>
                     <li>
                      下载:
                      <a href="https://mmcheng.net/mftp/Data/THUR15000.zip" rel="nofollow">
                       https://mmcheng.net/mftp/Data/THUR15000.zip
                      </a>
                      <ul dir="auto">
                       <li>
                        百度云:
                        <a href="https://pan.baidu.com/s/1u-E-8ujnxBz0mdmXsJglvg" rel="nofollow">
                         https://pan.baidu.com/s/1u-E-8ujnxBz0mdmXsJglvg
                        </a>
                       </li>
                      </ul>
                     </li>
                    </ul>
                    <blockquote>
                     <p dir="auto">
                      有效识别大型图像集中的显着对象对于许多应用是必不可少的, 包括图像检索, 监视, 图像注释和对象识别. 我们提出了一种简单, 快速, 有效的算法, 通过分析图像集合来定位和分割显着对象. 作为一个关键的新颖性, 我们通过提取最大化图像间相似性和图像内清晰度的显着对象(在预过滤图像的集合中)来引入群体显着性以实现优越的无监督显着对象分割. 为了评估我们的方法, 我们构建了一个大型基准数据集,
                      <strong>
                       该数据集包含多个类别的15K图像, 适用于显着对象区域的6000多个像素精确的地面实况注释
                      </strong>
                      . 在我们的所有测试中, group saliency 始终优于最先进的单图像显着性算法, 从而实现更高的精度和更好的回忆. 我们的算法成功处理了比任何现有基准数据集更大的订单的图像集合, 包括来自各种网络间源的各种异构图像.
                     </p>
                     <p dir="auto">
                      我们引入了分类图像的标记数据集, 用于评估基于草图的图像检索. 我们为5个关键字中的每一个下载了大约3000张图像:"蝴蝶", "咖啡杯", "狗跳", "长颈鹿"和"平面", 一起包括大约15000张图像.
                      <strong>
                       对于每个图像, 如果存在具有与查询关键字匹配的正确内容的非模糊对象并且对象的大部分可见, 则我们标记这样的对象区域. 与MSRA10K类似, 显着区域以像素级别标记. 我们只标记几乎完全可见的对象的显着对象区域, 因为部分遮挡的对象对形状匹配不太有用. 与MSRA10K不同, THUR15K数据集不包含为数据集中的每个图像标记的显着区域, 即, 一些图像可能没有任何显着区域. 该数据集用于评估基于形状的图像检索性能.
                      </strong>
                     </p>
                    </blockquote>
                    <div class="markdown-heading" dir="auto">
                     <h4 class="heading-element" dir="auto" tabindex="-1">
                      Bruce-A[need help]
                     </h4>
                     <a aria-label="Permalink: Bruce-A[need help]" class="anchor" href="#bruce-aneed-help" id="user-content-bruce-aneed-help">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <ul dir="auto">
                     <li>
                      论文:
                      <a href="https://papers.nips.cc/paper/2830-saliency-based-on-information-maximization.pdf" rel="nofollow">
                       https://papers.nips.cc/paper/2830-saliency-based-on-information-maximization.pdf
                      </a>
                     </li>
                    </ul>
                    <div class="markdown-heading" dir="auto">
                     <h4 class="heading-element" dir="auto" tabindex="-1">
                      Judd-A[need help]
                     </h4>
                     <a aria-label="Permalink: Judd-A[need help]" class="anchor" href="#judd-aneed-help" id="user-content-judd-aneed-help">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <ul dir="auto">
                     <li>
                      论文:
                      <a href="http://people.csail.mit.edu/torralba/publications/wherepeoplelook.pdf" rel="nofollow">
                       http://people.csail.mit.edu/torralba/publications/wherepeoplelook.pdf
                      </a>
                     </li>
                    </ul>
                    <div class="markdown-heading" dir="auto">
                     <h4 class="heading-element" dir="auto" tabindex="-1">
                      PASCAL-S
                     </h4>
                     <a aria-label="Permalink: PASCAL-S" class="anchor" href="#pascal-s" id="user-content-pascal-s">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <p dir="auto">
                     <a href="https://camo.githubusercontent.com/98830481bf1fc0d5bfca896627d7b09ae46f09de45bfadcfe78d68eb2a1d9b72/68747470733a2f2f6363766c2e6a68752e6564752f64617461736574732f6173736574732f70617363616c5f73616c69656e745f6f626a6563742e6a7067" rel="noopener noreferrer nofollow" target="_blank">
                      <img alt="img" data-canonical-src="https://ccvl.jhu.edu/datasets/assets/pascal_salient_object.jpg" src="https://camo.githubusercontent.com/98830481bf1fc0d5bfca896627d7b09ae46f09de45bfadcfe78d68eb2a1d9b72/68747470733a2f2f6363766c2e6a68752e6564752f64617461736574732f6173736574732f70617363616c5f73616c69656e745f6f626a6563742e6a7067" style="max-width: 100%;"/>
                     </a>
                    </p>
                    <ul dir="auto">
                     <li>
                      项目:
                      <ul dir="auto">
                       <li>
                        <a href="https://ccvl.jhu.edu/datasets/" rel="nofollow">
                         https://ccvl.jhu.edu/datasets/
                        </a>
                       </li>
                       <li>
                        <a href="http://www.cbi.gatech.edu/salobj/" rel="nofollow">
                         http://www.cbi.gatech.edu/salobj/
                        </a>
                       </li>
                      </ul>
                     </li>
                     <li>
                      下载:
                      <ul dir="auto">
                       <li>
                        百度云盘:
                        <a href="https://pan.baidu.com/s/1DZcfwCYdeMW4EGawhXQyig" rel="nofollow">
                         https://pan.baidu.com/s/1DZcfwCYdeMW4EGawhXQyig
                        </a>
                       </li>
                       <li>
                        页面:
                        <a href="http://academictorrents.com/details/6c49defd6f0e417c039637475cde638d1363037e" rel="nofollow">
                         http://academictorrents.com/details/6c49defd6f0e417c039637475cde638d1363037e
                        </a>
                       </li>
                      </ul>
                     </li>
                    </ul>
                    <blockquote>
                     <p dir="auto">
                      对来自PASCAL VOC的850张图像子集的自由修复. 收集8个主题, 3s观看时间, Eyelink II眼动仪. 大多数算法的性能表明PASCAL-S比大多数显着性数据集偏差更小.
                     </p>
                     <p dir="auto">
                      💔 由于其标注的真值有多个值, 常见的做法是使用
                      <code>
                       255/2
                      </code>
                      值作为阈值进行处理后, 再使用该数据集
                     </p>
                    </blockquote>
                    <div class="markdown-heading" dir="auto">
                     <h4 class="heading-element" dir="auto" tabindex="-1">
                      UCSB[need help]
                     </h4>
                     <a aria-label="Permalink: UCSB[need help]" class="anchor" href="#ucsbneed-help" id="user-content-ucsbneed-help">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <ul dir="auto">
                     <li>
                      论文:
                      <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3954044/" rel="nofollow">
                       https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3954044/
                      </a>
                     </li>
                     <li>
                      下载:
                      <a href="https://labs.psych.ucsb.edu/eckstein/miguel/research_pages/saliencydata.html" rel="nofollow">
                       https://labs.psych.ucsb.edu/eckstein/miguel/research_pages/saliencydata.html
                      </a>
                     </li>
                    </ul>
                    <div class="markdown-heading" dir="auto">
                     <h4 class="heading-element" dir="auto" tabindex="-1">
                      OSIE[need help]
                     </h4>
                     <a aria-label="Permalink: OSIE[need help]" class="anchor" href="#osieneed-help" id="user-content-osieneed-help">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <ul dir="auto">
                     <li>
                      论文:
                      <a href="https://jov.arvojournals.org/article.aspx?articleid=2193943" rel="nofollow">
                       https://jov.arvojournals.org/article.aspx?articleid=2193943
                      </a>
                     </li>
                    </ul>
                    <blockquote>
                     <p dir="auto">
                      大量先前的模型用于预测人们在自然场景中的外观, 侧重于像素级图像属性. 为了弥合计算显着性模型的预测能力与人类行为之间的语义差距, 我们提出了一种新的显着性体系结构, 它将信息分为三个层次: 像素级图像属性, 对象级属性和语义级属性. 通常忽略对象和语义级别信息, 或者仅讨论少数样本对象类别, 其中缩放到大量对象类别是不可行的, 也不是神经合理的. 为了解决这个问题, 这项工作构建了一个基本属性的原则词汇表来描述对象和语义级信息, 从而不限制有限数量的对象类别. 我们
                      <strong>
                       建立了一个包含500个图像的新数据集, 其中包含15个观察者的眼动追踪数据和5, 551个具有精细轮廓和12个语义属性的分段对象的注释数据
                      </strong>
                      (可在论文中公开获得). 实验结果证明了对象和语义级信息在预测视觉注意力方面的重要性.
                     </p>
                    </blockquote>
                    <div class="markdown-heading" dir="auto">
                     <h4 class="heading-element" dir="auto" tabindex="-1">
                      ACSD
                     </h4>
                     <a aria-label="Permalink: ACSD" class="anchor" href="#acsd" id="user-content-acsd">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <p dir="auto">
                     <a href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/1546135560011.png" rel="noopener noreferrer" target="_blank">
                      <img alt="1546135560011" src="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/1546135560011.png" style="max-width: 100%;"/>
                     </a>
                    </p>
                    <ul dir="auto">
                     <li>
                      论文:
                      <a href="https://infoscience.epfl.ch/record/135217/files/1708.pdf" rel="nofollow">
                       R. Achanta, S. Hemami, F. Estrada, and S. Ssstrunk, "Frequency-tuned salient region detection, " in CVPR, 2009, pp.1597–1604
                      </a>
                     </li>
                     <li>
                      项目:
                      <a href="https://ivrl.epfl.ch/research-2/research-current/research-saliency/supplementary_material-rk_cvpr09-index-html/" rel="nofollow">
                       https://ivrl.epfl.ch/research-2/research-current/research-saliency/supplementary_material-rk_cvpr09-index-html/
                      </a>
                     </li>
                     <li>
                      下载: 官网只提供了
                      <a href="https://ivrl.epfl.ch/wp-content/uploads/2018/08/binarymasks.zip" rel="nofollow">
                       真值标注的下载
                      </a>
                      .
                     </li>
                    </ul>
                    <p dir="auto">
                     基于[ASD数据集(MSRA1K)]制作.
                    </p>
                    <blockquote>
                     <p dir="auto">
                      我们从[Z. Wang and B. Li. A two-stage approach to saliency detection in images. ICASSP 2008.]中提出的1000个图像中获得了一个真实数据库.[Z. Wang and B. Li. A two-stage approach to saliency detection in images. ICASSP 2008.]中的基本事实是在显着区域周围的用户绘制的矩形. 这是不准确的, 并将多个对象合二为一. 我们手动分割用户绘制的矩形内的显着对象以获得二进制掩码, 如下所示. 这样的掩膜既准确又允许我们清楚地处理多个显着对象.
                     </p>
                    </blockquote>
                    <div class="markdown-heading" dir="auto">
                     <h3 class="heading-element" dir="auto" tabindex="-1">
                      Other Special SOD Datasets
                     </h3>
                     <a aria-label="Permalink: Other Special SOD Datasets" class="anchor" href="#other-special-sod-datasets" id="user-content-other-special-sod-datasets">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <div class="markdown-heading" dir="auto">
                     <h4 class="heading-element" dir="auto" tabindex="-1">
                      XPIE
                     </h4>
                     <a aria-label="Permalink: XPIE" class="anchor" href="#xpie" id="user-content-xpie">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <p dir="auto">
                     <a href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/1546137404871.png" rel="noopener noreferrer" target="_blank">
                      <img alt="1546137404871" src="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/1546137404871.png" style="max-width: 100%;"/>
                     </a>
                    </p>
                    <ul dir="auto">
                     <li>
                      链接:
                      <a href="https://www.researchgate.net/publication/320971838_What_is_and_What_is_Not_a_Salient_Object_Learning_Salient_Object_Detector_by_Ensembling_Linear_Exemplar_Regressors" rel="nofollow">
                       C. Xia, J. Li, X. Chen, A. Zheng, and Y. Zhang, "What  is  and  what is not a salient object? Learning salient object detector by ensembling linear exemplar regressors, " in CVPR , 2017, pp.4142–4150.
                      </a>
                     </li>
                     <li>
                      团队: cvteam:
                      <a href="http://cvteam.net/" rel="nofollow">
                       http://cvteam.net/
                      </a>
                     </li>
                     <li>
                      项目:
                      <a href="http://cvteam.net/projects/CVPR17-ELE/ELE.html" rel="nofollow">
                       http://cvteam.net/projects/CVPR17-ELE/ELE.html
                      </a>
                     </li>
                     <li>
                      下载:
                      <a href="http://cvteam.net/projects/CVPR17-ELE/XPIE.tar.gz" rel="nofollow">
                       http://cvteam.net/projects/CVPR17-ELE/XPIE.tar.gz
                      </a>
                     </li>
                    </ul>
                    <blockquote>
                     <p dir="auto">
                      找出什么是什么和什么不是显着对象可以有助于在显着对象检测(SOD)中开发更好的特征和模型. 在本文中, 我们研究了在构建新的SOD数据集时选择和丢弃的图像, 发现许多相似的候选者, 复杂形状和低对象性是很多非显着对象的三个主要属性. 此外, 对象可能具有使其显着的不同属性.
                     </p>
                     <p dir="auto">
                      为了全面解释什么是什么和什么不是显着对象, 一个可行的解决方案是通过观察包含在数据集中或从数据集中丢弃的图像中的对象的主要特征来研究构建新SOD数据集的整个过程. 从这些观察中, 我们可以推断显着和非显着对象的关键属性以及基于图像的SOD数据集中可能存在的主观偏差. 为此, 我们构建了一个大的SOD数据集(称为XPIE)并记录构建过程中的所有细节.
                     </p>
                     <ol dir="auto">
                      <li>
                       我们首先从三个来源收集三种图像, 包括Panoramio, ImageNet和两个fixation数据集. 这些操作是全自动的, 以避免引入太多的主观偏见.
                      </li>
                      <li>
                       之后, 我们调整每个图像的大小, 使其最大边长为300像素, 并丢弃所有最小边长小于128像素的灰度或彩色图像.
                      </li>
                      <li>
                       最后, 我们在三个图像子集中获得29, 600个彩色图像. 分别表示为Set-P, Set-I, Set-E.
                      </li>
                     </ol>
                     <p dir="auto">
                      <strong>
                       Set-P 包含8, 800具有地理信息的感兴趣地点的图像(例如, GPS和标签), 具有对象标签的Set-I包含19, 600图像, 以及Set-E包含1, 200个human fixations图像
                      </strong>
                      .
                     </p>
                     <p dir="auto">
                      对于这些图像, 我们要求两位工程师通过两个阶段对其进行注释. 在第一阶段, 图像被分配一个二进制标记:'是'用于包含非明确对象, 否则为'否'. 在第一阶段之后, 我们将21, 002张图片标记为"是", 并且8, 598图像标记为"否". 在第二阶段, 这两位工程师进一步要求手动标记标记为"是"的10, 000张图像中的显着对象的准确边界. 注意我们有10名志愿者参与整个过程, 以检查注释的质量.
                      <strong>
                       最后, 我们获得了10, 000张图像的二进制掩码
                      </strong>
                      .
                     </p>
                     <p dir="auto">
                      可见论文内容第2节.
                     </p>
                    </blockquote>
                    <div class="markdown-heading" dir="auto">
                     <h4 class="heading-element" dir="auto" tabindex="-1">
                      SOC
                     </h4>
                     <a aria-label="Permalink: SOC" class="anchor" href="#soc" id="user-content-soc">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <p dir="auto">
                     <a href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/1546081178458.png" rel="noopener noreferrer" target="_blank">
                      <img alt="1546081178458" src="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/1546081178458.png" style="max-width: 100%;"/>
                     </a>
                    </p>
                    <p dir="auto">
                     <a href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/1546081446332.png" rel="noopener noreferrer" target="_blank">
                      <img alt="1546081446332" src="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/1546081446332.png" style="max-width: 100%;"/>
                     </a>
                    </p>
                    <ul dir="auto">
                     <li>
                      项目:
                      <a href="http://dpfan.net/SOCBenchmark/" rel="nofollow">
                       http://dpfan.net/SOCBenchmark/
                      </a>
                     </li>
                     <li>
                      论文:
                      <a href="http://dpfan.net/wp-content/uploads/2018/04/SOCBenchmark.pdf" rel="nofollow">
                       Salient Objects in Clutter: Bringing Salient Object Detection to the Foreground
                      </a>
                      <ul dir="auto">
                       <li>
                        中文:
                        <a href="http://dpfan.net/wp-content/uploads/SOCBenchmarkCN.pdf" rel="nofollow">
                         http://dpfan.net/wp-content/uploads/SOCBenchmarkCN.pdf
                        </a>
                       </li>
                      </ul>
                     </li>
                     <li>
                      下载:
                      <ul dir="auto">
                       <li>
                        Overall 6K SOC Dataset  (730.2MB)
                        <a href="https://pan.baidu.com/s/1J8_CF7zE1zApqgAR9eS1Dw" rel="nofollow">
                         Baidu
                        </a>
                        <a href="https://drive.google.com/file/d/1hfo33A7diED2dikTpN9o4KnZTxizGdLr/view?usp=sharing" rel="nofollow">
                         Google
                        </a>
                       </li>
                       <li>
                        3.6K SOC Training Set (441.32MB)
                        <a href="http://dpfan.net/wp-content/uploads/TrainSet.zip" rel="nofollow">
                         Here
                        </a>
                        <a href="https://pan.baidu.com/s/1Mao0piUuqVXAzmJoNtrtAw" rel="nofollow">
                         Baidu
                        </a>
                        <a href="https://drive.google.com/open?id=16jlzeJJ1tawyBLBN5fRiWbh2y_F0iSyP" rel="nofollow">
                         Google
                        </a>
                       </li>
                       <li>
                        1.2K  SOC Validation Set (146.56MB)
                        <a href="http://dpfan.net/wp-content/uploads/ValSet.zip" rel="nofollow">
                         Here
                        </a>
                        <a href="https://pan.baidu.com/s/1mOmiezCpkr5NCQk8ecvGiQ" rel="nofollow">
                         Baidu
                        </a>
                        <a href="https://drive.google.com/open?id=1vAfP8fCAo2a2KwgsmYLn8r8Rk4Lk7Urr" rel="nofollow">
                         Google
                        </a>
                       </li>
                       <li>
                        1.2K  SOC Test Set (141.86MB)
                        <a href="http://dpfan.net/wp-content/uploads/TestSet.zip" rel="nofollow">
                         Here
                        </a>
                        <a href="https://pan.baidu.com/s/10y-dx9HCPQm9fnp-Brswgw" rel="nofollow">
                         Baidu
                        </a>
                        <a href="https://drive.google.com/open?id=1ZdKrsk-S4J6KQyjx-cPeL0HoKXy7CCxG" rel="nofollow">
                         Google
                        </a>
                       </li>
                      </ul>
                     </li>
                    </ul>
                    <blockquote>
                     <p dir="auto">
                      在本文中, 我们提供了显着对象检测(SOD)模型的综合评估. 我们的分析确定了现有SOD数据集的严重设计偏差, 假设每个图像在低杂波中包含至少一个明显突出的显着对象. 这是一个不切实际的假设. 在现有数据集上进行评估时, 设计偏差导致了最先进的SOD模型的饱和高性能. 然而, 当应用于现实世界的日常场景时, 这些模型仍然远远不能令人满意. 根据我们的分析, 我们首先确定了全面和平衡的数据集应该实现的7个关键方面. 然后, 我们提出一个新的高质量数据集并更新以前的显着性基准.
                     </p>
                     <p dir="auto">
                      具体来说, 我们的数据集称为SOC, Salient Objects in Clutter,
                      <strong>
                       包括来自日常对象类别的显着和非显着对象的图像
                      </strong>
                      . 除了对象类别注释之外, 每个突出图像都伴随着反映现实世界场景中常见挑战的属性(例如, 外观变化, 杂乱), 并且可以帮助 1)更深入地了解SOD问题, 2)调查专业人员和SOD模型的缺点, 3)从不同的角度客观地评估模型. 最后, 我们在SOC数据集上报告基于属性的性能评估. 我们相信, 我们的数据集和结果将为未来的显着物体检测研究开辟新的方向.
                     </p>
                     <p dir="auto">
                      SOC has 6, 000 images with 80 common categories. Half of the images contain salient objects and the others contain none.
                      <strong>
                       Each salient-object-contained image is annotated with instance-level SOD ground-truth, object category (e.g., dog, book), and challenging factors
                      </strong>
                      (e.g., big/small object).
                      <strong>
                       The non-salient object subset has 783 texture images and 2, 217 real-scene images
                      </strong>
                      (e.g., aurora, sky).
                     </p>
                    </blockquote>
                    <div class="markdown-heading" dir="auto">
                     <h4 class="heading-element" dir="auto" tabindex="-1">
                      SOS/MOS[need some images]
                     </h4>
                     <a aria-label="Permalink: SOS/MOS[need some images]" class="anchor" href="#sosmosneed-some-images" id="user-content-sosmosneed-some-images">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <ul dir="auto">
                     <li>
                      项目:
                      <a href="http://cs-people.bu.edu/jmzhang/sos.html" rel="nofollow">
                       http://cs-people.bu.edu/jmzhang/sos.html
                      </a>
                     </li>
                     <li>
                      论文:
                      <ul dir="auto">
                       <li>
                        SOS: J. Zhang, S. Ma, M. Sameki, S. Sclaroff, M. Betke, Z. Lin, X. Shen, B. Price, and R. Mech, "Salient object subitizing, " in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2015, pp.4045–4054.
                       </li>
                       <li>
                        MOS: J. Zhang, S. Ma, M. Sameki, S. Sclaroff, M. Betke, Z. Lin, X. Shen, B. Price, and R. Mech, "Salient object subitizing, " in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2015, pp.4045–4054.
                       </li>
                      </ul>
                     </li>
                    </ul>
                    <blockquote>
                     <p dir="auto">
                      SOS 10 is created for SOD subitizing [115], i.e., to predict the number of salient objects without an expensive detection process. It contains 6, 900 images selected from:
                     </p>
                     <ol dir="auto">
                      <li>
                       A large-scale hierarchical image database
                      </li>
                      <li>
                       Sun database: Large-scale scene recognition from abbey to zoo
                      </li>
                      <li>
                       Microsoft coco: Common objects in context
                      </li>
                      <li>
                       The pascal visual object classes (voc) challenge results
                      </li>
                     </ol>
                     <p dir="auto">
                      Each image is labeled as containing 0, 1, 2, 3 or 4+ salient objects. SOS is randomly split into a training (5, 520 images) and a test set (1, 380 images).
                     </p>
                     <p dir="auto">
                      <strong>
                       MSO is a subset of the test set of SOS and contains 1, 224 images
                      </strong>
                      . It has a more balanced distribution regarding the number of salient objects, and each object is annotated with a bounding box.
                     </p>
                    </blockquote>
                    <div class="markdown-heading" dir="auto">
                     <h4 class="heading-element" dir="auto" tabindex="-1">
                      ILSO[need some images]
                     </h4>
                     <a aria-label="Permalink: ILSO[need some images]" class="anchor" href="#ilsoneed-some-images" id="user-content-ilsoneed-some-images">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <ul dir="auto">
                     <li>
                      项目:
                      <a href="http://www.sysu-hcp.net/instance-level-salient-object-segmentation/" rel="nofollow">
                       http://www.sysu-hcp.net/instance-level-salient-object-segmentation/
                      </a>
                     </li>
                     <li>
                      论文: G. Li, Y. Xie, L. Lin, and Y. Yu, "Instance-level salient object segmentation, " in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2017, pp.247–256.
                     </li>
                    </ul>
                    <blockquote>
                     <p dir="auto">
                      ILSO has 1, 000 images with pixel-wise instancelevel saliency annotations and coarse contour labeling, where the benchmark results are generated using MSRNet [Instance-level salient object segmentation]. Most of the images in ILSO are selected from the following datasets to reduce ambiguity over the salient object regions.
                     </p>
                     <ol dir="auto">
                      <li>
                       Visual saliency based on multiscale deep features
                      </li>
                      <li>
                       Hierarchical saliency detection
                      </li>
                      <li>
                       Saliency detection via graph-based manifold ranking
                      </li>
                      <li>
                       Salient object subitizing
                      </li>
                     </ol>
                    </blockquote>
                    <div class="markdown-heading" dir="auto">
                     <h4 class="heading-element" dir="auto" tabindex="-1">
                      HS-SOD
                     </h4>
                     <a aria-label="Permalink: HS-SOD" class="anchor" href="#hs-sod" id="user-content-hs-sod">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <p dir="auto">
                     <a href="https://github.com/gistairc/HS-SOD/raw/master/images/poster-QoMEX2018.png" rel="noopener noreferrer" target="_blank">
                      <img alt="img" src="https://github.com/gistairc/HS-SOD/raw/master/images/poster-QoMEX2018.png" style="max-width: 100%;"/>
                     </a>
                    </p>
                    <p dir="auto">
                     <a href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/2018-12-28-22-16-20.png" rel="noopener noreferrer" target="_blank">
                      <img alt="eva" src="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/2018-12-28-22-16-20.png" style="max-width: 100%;"/>
                     </a>
                    </p>
                    <ul dir="auto">
                     <li>
                      项目:
                      <a href="https://github.com/gistairc/HS-SOD">
                       https://github.com/gistairc/HS-SOD
                      </a>
                     </li>
                     <li>
                      下载:
                      <a href="http://data.airc.aist.go.jp/HS-SOD/HS-SOD.zip" rel="nofollow">
                       http://data.airc.aist.go.jp/HS-SOD/HS-SOD.zip
                      </a>
                      5.6G
                     </li>
                     <li>
                      论文:
                      <a href="https://arxiv.org/abs/1806.11314" rel="nofollow">
                       Hyperspectral Image Dataset for Benchmarking on Salient Object Detection
                      </a>
                     </li>
                    </ul>
                    <blockquote>
                     <p dir="auto">
                      使用有监督或无监督的方法对着色对象进行了显着的物体检测. 最近, 一些研究表明, 通过在来自天然景观的高光谱图像的可见光谱中使用光谱特征, 也可以实现有效的显着对象检测. 然而, 这些关于高光谱显着物体检测的模型使用从各种在线公共数据集中选择的极少数数据进行测试, 这些数据不是为了物体检测目的而特别创建的. 因此, 在这里, 我们的目标是通过发布
                      <strong>
                       具有60个高光谱图像的集合的高光谱显着物体检测数据集
                      </strong>
                      以及
                      <strong>
                       相应的地面实况二值图像
                      </strong>
                      和**代表性的彩色图像(sRGB)**来指导该领域. 我们在数据收集过程中考虑了几个方面, 例如对象大小的变化, 对象的数量, 前景-背景对比度, 图像上的对象位置等. 然后, 我们为每个高光谱数据准备了真值二进制图像, 其中显著性目标被标记为图像. 最后, 我们使用曲线下面积(AUC)度量对文献中一些现有的高光谱显着性检测模型进行了性能评估.
                     </p>
                     <p dir="auto">
                      这些数据是在东京港口码头公司的许可下, 在日本东京台场的东京海滨城市公园收集的. 我们在2017年8月至9月期间的几天内收集了数据, 当时天气晴朗或部分多云. 在每个数据收集日, 使用三脚架固定相机以最小化图像上的运动失真. 我们尝试根据日光条件尽可能地保持相机设置的曝光时间和增益, 同时保持像素值饱和度或图像可见性. 作为数据集用户的参考, 我们提供相机设置, 例如文本文件中每个图像的曝光时间和增益值以及相应的数据. 我们也没有对捕获的波段应用标准化. 它可以提高前景和背景区域之间色彩对比度更高的高光谱图像的质量; 但是, 它也可能降低数据集在显着对象检测任务上进行基准测试的难度.
                     </p>
                     <p dir="auto">
                      在获得各种高光谱图像后, 我们从大约50个不同的场景中选择了60个图像, 条件是:i)我们去除了由于场景中的运动引起的失真图像(取决于曝光时间, 一个图像可能需要几秒钟才能用于相机), ii)我们考虑了几个方面, 如显着物体大小的变化, 图像上物体的空间位置, 显着物体的数量, 前景 * 背景对比度, iii)一些图像具有相同的场景但物体位置, 物距或数量对象各不相同.
                     </p>
                     <p dir="auto">
                      为了便于显着物体检测任务, 我们在可见光谱周围裁剪光谱带, 并在传感器暗噪声校正后以".mat"文件格式保存每个场景的超立方体. 如[21]中所定义, 可见光谱具有380-780nm的良好可接受范围, 但也可以使用[3, 4]中的400-700nm范围. 为了保持范围广泛和灵活性, 想要使用数据集的人, 我们在[21]中为我们的数据集选择了380 * 780 nm的定义范围, 尽管在人类视觉系统的这些范围的边界处视觉刺激可能较弱. 然后, 我们使用高光谱图像渲染sRGB彩色图像, 通过标记显着对象的边界来创建地面真实显着对象二进制图像.
                     </p>
                    </blockquote>
                    <p dir="auto">
                     HS-SOD.zip file contains three folders:
                    </p>
                    <p dir="auto">
                     1.hyperspectral: containing 60 hyperspectral images with #spatial rows:768 #spatial columns:1024 #spectral channels:81 (data only within visible spectrum: 380 nm -720 nm)
2.color: 60 color images of hyperspectral dataset rendered in sRGB for visualization
3.ground-truth: 60 ground-truth binary images for salient objects
                    </p>
                    <div class="markdown-heading" dir="auto">
                     <h3 class="heading-element" dir="auto" tabindex="-1">
                      Video Saliency Detection
                     </h3>
                     <a aria-label="Permalink: Video Saliency Detection" class="anchor" href="#video-saliency-detection" id="user-content-video-saliency-detection">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <div class="markdown-heading" dir="auto">
                     <h4 class="heading-element" dir="auto" tabindex="-1">
                      RSD(PKU-RSD)
                     </h4>
                     <a aria-label="Permalink: RSD(PKU-RSD)" class="anchor" href="#rsdpku-rsd" id="user-content-rsdpku-rsd">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <p dir="auto">
                     <a href="https://camo.githubusercontent.com/bad3b4eb081112c2e37582e1ab22e3e7a3281b61724a455b6dd615a93a79bf21/68747470733a2f2f706b756d6c2e6f72672f77702d636f6e74656e742f75706c6f6164732f323031342f31322f73616d706c65732d6f662d5253442d31303234783237312e706e67" rel="noopener noreferrer nofollow" target="_blank">
                      <img alt="samples of RSD" data-canonical-src="https://pkuml.org/wp-content/uploads/2014/12/samples-of-RSD-1024x271.png" src="https://camo.githubusercontent.com/bad3b4eb081112c2e37582e1ab22e3e7a3281b61724a455b6dd615a93a79bf21/68747470733a2f2f706b756d6c2e6f72672f77702d636f6e74656e742f75706c6f6164732f323031342f31322f73616d706c65732d6f662d5253442d31303234783237312e706e67" style="max-width: 100%;"/>
                     </a>
                    </p>
                    <ul dir="auto">
                     <li>
                      论文:
                      <a href="https://ieeexplore.ieee.org/document/5202529" rel="nofollow">
                       J. Li, Y. Tian, T. Huang, and W. Gao, "A dataset and evaluation methodology for visual saliency in video, " in IEEE ICME, 2009, pp.442–445
                      </a>
                     </li>
                     <li>
                      项目:
                      <a href="https://pkuml.org/resources/dataset.html" rel="nofollow">
                       https://pkuml.org/resources/dataset.html
                      </a>
                     </li>
                     <li>
                      下载:
                      <a href="https://pkuml.org/resources/pku-rsd.html" rel="nofollow">
                       https://pkuml.org/resources/pku-rsd.html
                      </a>
                     </li>
                    </ul>
                    <blockquote>
                     <p dir="auto">
                      我们构建了这个PKU-RSD(区域显着性数据集)数据集, 可以捕获时空视觉显着性, 用于评估不同的视频显着性模型. 该数据集包含431个短视频, 其涵盖各种场景(监视, 广告, 新闻, 卡通, 电影等)以及由23个主题手动标记的采样关键帧中的显着对象的相应注释结果.
                     </p>
                    </blockquote>
                    <div class="markdown-heading" dir="auto">
                     <h4 class="heading-element" dir="auto" tabindex="-1">
                      STC[need help]
                     </h4>
                     <a aria-label="Permalink: STC[need help]" class="anchor" href="#stcneed-help" id="user-content-stcneed-help">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <ul dir="auto">
                     <li>
                      论文:
                      <a href="https://pdfs.semanticscholar.org/3347/c330ac5586020ebea60823b1fd4e8d68e936.pdf?_ga=2.181072804.269179473.1546092428-61549168.1544104573" rel="nofollow">
                       Y. Wu, N. Zheng, Z. Yuan, H. Jiang, and T. Liu, "Detection of salient objects with focused attention based on spatial and temporal coherence, " Chinese Science Bulletin, vol.56, pp.1055–1062, 2011.
                      </a>
                     </li>
                     <li>
                      下载: This dataset is freely available from the author
                     </li>
                    </ul>
                    <blockquote>
                     <p dir="auto">
                      对视频内容的理解和分析对于众多应用程序来说至关重要, 包括视频摘要, 检索, 导航和编辑. 此过程的一个重要部分是检测视频片段中的显着(通常意味着重要和有趣)对象. 与现有方法不同, 我们提出了一种将显着性测量与空间和时间相干性相结合的方法. 空间和时间一致性的整合受到人类视觉中关注焦点的启发. 在所提出的方法中, 低级视觉分组线索的空间相干性(例如外观和运动)有助于每帧对象背景分离, 而对象属性的时间一致性(例如形状和外观)确保一致物体随时间定位, 因此该方法对于意外的环境变化和相机振动是鲁棒的. 在
                      <strong>
                       开发了基于粗到细多尺度动态规划的有效优化策略之后, 我们使用可与本文一起免费获得的具有挑战性的数据集来评估我们的方法
                      </strong>
                      . 我们展示了两种类型的一致性的有效性和互补性, 并证明它们可以显着提高视频中显着对象检测的性能.
                     </p>
                    </blockquote>
                    <div class="markdown-heading" dir="auto">
                     <h3 class="heading-element" dir="auto" tabindex="-1">
                      RGBD-Saliency Detection
                     </h3>
                     <a aria-label="Permalink: RGBD-Saliency Detection" class="anchor" href="#rgbd-saliency-detection" id="user-content-rgbd-saliency-detection">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <blockquote>
                     <p dir="auto">
                      致谢:
                     </p>
                     <ul dir="auto">
                      <li>
                       @JXingZhao, 在他的工作中整理并公开了多个数据集:
                       <a href="https://github.com/JXingZhao/ContrastPrior">
                        https://github.com/JXingZhao/ContrastPrior
                       </a>
                      </li>
                      <li>
                       @jiwei0921, 在他的工作中整理并公开了多个数据集:
                       <a href="https://github.com/jiwei0921/RGBD-SOD-datasets">
                        https://github.com/jiwei0921/RGBD-SOD-datasets
                       </a>
                      </li>
                      <li>
                       <strong>
                        更全面的内容
                       </strong>
                       可见
                       <a href="http://dpfan.net/d3netbenchmark/" rel="nofollow">
                        http://dpfan.net/d3netbenchmark/
                       </a>
                      </li>
                     </ul>
                    </blockquote>
                    <div class="markdown-heading" dir="auto">
                     <h4 class="heading-element" dir="auto" tabindex="-1">
                      SIP
                     </h4>
                     <a aria-label="Permalink: SIP" class="anchor" href="#sip" id="user-content-sip">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <p dir="auto">
                     <a href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/2019-09-15-16-26-26.png" rel="noopener noreferrer" target="_blank">
                      <img alt="" src="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/2019-09-15-16-26-26.png" style="max-width: 100%;"/>
                     </a>
                    </p>
                    <ul dir="auto">
                     <li>
                      论文: Rethinking RGB-D Salient Object Detection: Models, Datasets, and Large-Scale Benchmarks:
                      <a href="https://arxiv.org/pdf/1907.06781.pdf" rel="nofollow">
                       https://arxiv.org/pdf/1907.06781.pdf
                      </a>
                     </li>
                     <li>
                      项目:
                      <a href="http://dpfan.net/d3netbenchmark/" rel="nofollow">
                       http://dpfan.net/d3netbenchmark/
                      </a>
                     </li>
                     <li>
                      下载: 请见项目主页
                     </li>
                    </ul>
                    <blockquote>
                     <p dir="auto">
                      we carefully collect a new salient person (SIP) dataset, which consists of 1K high-resolution images that cover diverse real-world scenes from various viewpoints, poses, occlusion, illumination, and background.
                     </p>
                    </blockquote>
                    <div class="markdown-heading" dir="auto">
                     <h4 class="heading-element" dir="auto" tabindex="-1">
                      NLPR/RGBD1000
                     </h4>
                     <a aria-label="Permalink: NLPR/RGBD1000" class="anchor" href="#nlprrgbd1000" id="user-content-nlprrgbd1000">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <p dir="auto">
                     <a href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/1546138815074.png" rel="noopener noreferrer" target="_blank">
                      <img alt="1546138815074" src="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/1546138815074.png" style="max-width: 100%;"/>
                     </a>
                    </p>
                    <ul dir="auto">
                     <li>
                      论文:
                      <a href="https://docs.google.com/uc?authuser=0&amp;id=0B1wzzt1_uP1rb250d0t6dVFXWG8&amp;export=download" rel="nofollow">
                       Rgbd salient object detection: a benchmark and algorithms
                      </a>
                     </li>
                     <li>
                      项目:
                      <a href="https://sites.google.com/site/rgbdsaliency/home" rel="nofollow">
                       https://sites.google.com/site/rgbdsaliency/home
                      </a>
                     </li>
                     <li>
                      下载:
                      <a href="https://sites.google.com/site/rgbdsaliency/dataset" rel="nofollow">
                       https://sites.google.com/site/rgbdsaliency/dataset
                      </a>
                     </li>
                    </ul>
                    <blockquote>
                     <p dir="auto">
                      NLPR is also called RGBD1000 dataset which including 1, 000 images. There may exist multiple salient objects in each image. The structured light depth images are obtained by the Microsoft Kinect under different illumination conditions.
                     </p>
                     <p dir="auto">
                      虽然深度信息在人类视觉系统中起着重要作用, 但在现有的视觉显着性计算模型中尚未得到很好的探索. 在这项工作中,
                      <strong>
                       我们首先引入了一个大规模的RGBD图像数据集, 以解决目前RGBD显着目标检测研究中数据不足的问题
                      </strong>
                      . 为了确保大多数现有的RGB显着模型在RGBD场景中仍然足够, 我们继续提供一个简单的融合框架, 将现有的RGB产生的显着性与新的深度诱导显着性相结合, 前者是从现有的RGB模型中估算的, 而前者是后者基于提出的多上下文对比模型. 此外, 还提出了一种专门的多阶段RGBD模型, 其考虑了来自低级特征对比度, 中级区域分组和高级先验增强的深度和外观线索. 大量实验表明, 我们的模型能够准确定位RGBD图像中的显着对象, 并为目标对象分配一致的显着性值.
                     </p>
                    </blockquote>
                    <div class="markdown-heading" dir="auto">
                     <h4 class="heading-element" dir="auto" tabindex="-1">
                      NJU400/2000
                     </h4>
                     <a aria-label="Permalink: NJU400/2000" class="anchor" href="#nju4002000" id="user-content-nju4002000">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <p dir="auto">
                     <a href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/1546139249376.png" rel="noopener noreferrer" target="_blank">
                      <img alt="1546139249376" src="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/1546139249376.png" style="max-width: 100%;"/>
                     </a>
                    </p>
                    <ul dir="auto">
                     <li>
                      论文:
                      <ul dir="auto">
                       <li>
                        <a href="http://mcg.nju.edu.cn/publication/2014/icip14-jur.pdf" rel="nofollow">
                         NJU400: Depth saliency based on anisotropic center-surround difference
                        </a>
                       </li>
                       <li>
                        <a href="http://mcg.nju.edu.cn/publication/2015/spic15-jur.pdf" rel="nofollow">
                         NJU2000: Depth-aware salient object detection using anisotropic center-surround difference
                        </a>
                       </li>
                      </ul>
                     </li>
                     <li>
                      团队:
                      <a href="http://mcg.nju.edu.cn/index.html" rel="nofollow">
                       MGG
                      </a>
                     </li>
                     <li>
                      项目:
                      <a href="http://mcg.nju.edu.cn/publication/2014/icip14-jur/index.html" rel="nofollow">
                       http://mcg.nju.edu.cn/publication/2014/icip14-jur/index.html
                      </a>
                     </li>
                     <li>
                      下载:
                      <ul dir="auto">
                       <li>
                        <a href="http://mcg.nju.edu.cn/resource.html" rel="nofollow">
                         http://mcg.nju.edu.cn/resource.html
                        </a>
                       </li>
                       <li>
                        <a href="http://mcg.nju.edu.cn/dataset/nju400.zip" rel="nofollow">
                         http://mcg.nju.edu.cn/dataset/nju400.zip
                        </a>
                       </li>
                       <li>
                        <a href="http://mcg.nju.edu.cn/dataset/nju2000.zip" rel="nofollow">
                         http://mcg.nju.edu.cn/dataset/nju2000.zip
                        </a>
                       </li>
                      </ul>
                     </li>
                    </ul>
                    <blockquote>
                     <p dir="auto">
                      NJU2000 contains 2003 stereo image pairs with diverse objects and complex, challenging scenarios, along with ground-truth map. The stereo images are gathered from 3D movies, the Internet, and photographs taken by a Fuji W3 stereo camera.
                     </p>
                    </blockquote>
                    <div class="markdown-heading" dir="auto">
                     <h4 class="heading-element" dir="auto" tabindex="-1">
                      STEREO/SSB
                     </h4>
                     <a aria-label="Permalink: STEREO/SSB" class="anchor" href="#stereossb" id="user-content-stereossb">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <p dir="auto">
                     <a href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/2019-05-13-19-48-20.png" rel="noopener noreferrer" target="_blank">
                      <img alt="" src="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/2019-05-13-19-48-20.png" style="max-width: 100%;"/>
                     </a>
                    </p>
                    <ul dir="auto">
                     <li>
                      论文:
                      <a href="http://web.cecs.pdx.edu/~fliu/papers/cvpr2012.pdf" rel="nofollow">
                       Leveraging stereopsis for saliency analysis
                      </a>
                     </li>
                     <li>
                      项目:
                      <a href="http://web.cecs.pdx.edu/~fliu/" rel="nofollow">
                       http://web.cecs.pdx.edu/~fliu/
                      </a>
                     </li>
                     <li>
                      下载: 请到主页寻找, 需要联系作者.
                     </li>
                    </ul>
                    <blockquote>
                     <p dir="auto">
                      SSB is also called STEREO dataset, which consists of 1000 pairs of binocular images.
                     </p>
                    </blockquote>
                    <div class="markdown-heading" dir="auto">
                     <h4 class="heading-element" dir="auto" tabindex="-1">
                      LFSD[nead img]
                     </h4>
                     <a aria-label="Permalink: LFSD[nead img]" class="anchor" href="#lfsdnead-img" id="user-content-lfsdnead-img">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <ul dir="auto">
                     <li>
                      论文: Saliency detection on light field
                     </li>
                     <li>
                      项目:
                      <a href="https://sites.duke.edu/nianyi/publication/saliency-detection-on-light-field/" rel="nofollow">
                       https://sites.duke.edu/nianyi/publication/saliency-detection-on-light-field/
                      </a>
                     </li>
                     <li>
                      下载: 请到主页寻找, 需要联系作者.
                     </li>
                    </ul>
                    <blockquote>
                     <p dir="auto">
                      LFSD is a small dataset which contains 100 images with depth information and human labeled ground truths. The depth information was obtained via the Lytro light field camera.
                     </p>
                    </blockquote>
                    <div class="markdown-heading" dir="auto">
                     <h4 class="heading-element" dir="auto" tabindex="-1">
                      RGBD135/DES
                     </h4>
                     <a aria-label="Permalink: RGBD135/DES" class="anchor" href="#rgbd135des" id="user-content-rgbd135des">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <p dir="auto">
                     <a href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/2019-05-23-10-44-38.png" rel="noopener noreferrer" target="_blank">
                      <img alt="image" src="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/2019-05-23-10-44-38.png" style="max-width: 100%;"/>
                     </a>
                    </p>
                    <p dir="auto">
                     <a href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/2019-05-23-10-44-15.png" rel="noopener noreferrer" target="_blank">
                      <img alt="depth" src="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/2019-05-23-10-44-15.png" style="max-width: 100%;"/>
                     </a>
                    </p>
                    <p dir="auto">
                     <a href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/2019-05-23-10-44-59.png" rel="noopener noreferrer" target="_blank">
                      <img alt="mask" src="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/2019-05-23-10-44-59.png" style="max-width: 100%;"/>
                     </a>
                    </p>
                    <ul dir="auto">
                     <li>
                      论文:
                      <a href="http://delivery.acm.org/10.1145/2640000/2632866/p23-cheng.pdf?ip=202.118.97.210&amp;id=2632866&amp;acc=ACTIVE%20SERVICE&amp;key=BF85BBA5741FDC6E%2E5FC7500D8F9CB386%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1557798709_a26e3faff3faccad6d62e02d79d1921a" rel="nofollow">
                       Depth enhanced saliency detection method
                      </a>
                     </li>
                     <li>
                      项目:
                      <a href="https://github.com/HzFu/DES_code">
                       https://github.com/HzFu/DES_code
                      </a>
                     </li>
                     <li>
                      下载: 项目主页提供了下面的下载链接诶:
                      <ul dir="auto">
                       <li>
                        <a href="https://onedrive.live.com/redir?resid=F3A8A31ABFAC51B0!256&amp;authkey=!AC4-yOEjn0bgrCQ&amp;ithint=file%2crar" rel="nofollow">
                         https://onedrive.live.com/redir?resid=F3A8A31ABFAC51B0!256&amp;authkey=!AC4-yOEjn0bgrCQ&amp;ithint=file%2crar
                        </a>
                       </li>
                       <li>
                        <a href="https://pan.baidu.com/s/1pLv2B8n" rel="nofollow">
                         https://pan.baidu.com/s/1pLv2B8n
                        </a>
                       </li>
                      </ul>
                     </li>
                    </ul>
                    <blockquote>
                     <p dir="auto">
                      RGBD135 is also named DES which consists of seven indoor scenes and contains 135 indoor images collected by Microsoft Kinect.
                     </p>
                    </blockquote>
                    <div class="markdown-heading" dir="auto">
                     <h4 class="heading-element" dir="auto" tabindex="-1">
                      DUT-RGBD
                     </h4>
                     <a aria-label="Permalink: DUT-RGBD" class="anchor" href="#dut-rgbd" id="user-content-dut-rgbd">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <ul dir="auto">
                     <li>
                      论文: Depth-induced Multi-scale Recurrent Attention Network for Saliency Detection
                     </li>
                     <li>
                      项目:
                      <a href="https://github.com/jiwei0921/DMRA_RGBD-SOD">
                       https://github.com/jiwei0921/DMRA_RGBD-SOD
                      </a>
                     </li>
                     <li>
                      下载: 请见@jiwei0921的RGBD-SOD-datasets仓库
                     </li>
                    </ul>
                    <div class="markdown-heading" dir="auto">
                     <h4 class="heading-element" dir="auto" tabindex="-1">
                      SSD100
                     </h4>
                     <a aria-label="Permalink: SSD100" class="anchor" href="#ssd100" id="user-content-ssd100">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <p dir="auto">
                     <a href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/2019-09-15-16-17-12.png" rel="noopener noreferrer" target="_blank">
                      <img alt="" src="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/2019-09-15-16-17-12.png" style="max-width: 100%;"/>
                     </a>
                    </p>
                    <ul dir="auto">
                     <li>
                      论文: A Three-pathway Psychobiological Framework of Salient Object Detection Using Stereoscopic Technology:
                      <a href="http://dpfan.net/wp-content/uploads/SSD_dataset_ICCVW17.pdf" rel="nofollow">
                       http://dpfan.net/wp-content/uploads/SSD_dataset_ICCVW17.pdf
                      </a>
                     </li>
                     <li>
                      下载: 请见@jiwei0921的RGBD-SOD-datasets仓库
                     </li>
                    </ul>
                    <blockquote>
                     <p dir="auto">
                      Our SSD100 dataset is built on three stereo movies. The movies contain both the indoors and outdoors scenes. We pick up one stereo image pair at each hundred frames. It totally has tens of thousands of stereo image pairs. We make the image acquisition and image annotation independent to each other, we can avoid dataset design bias, namely a specific type of bias that is caused by experimenters unnatural selection of dataset images. The chosen stereo image pairs are based on one principle: choose the one which the computer detect the salient objects within the complex scenes where even the human cannot tell the salient objects at once. After picking up the stereo image pairs, we divide the image pairs into left images and right images both in 960x1080 size. When we build the ground truth of salient objects, we adhere to the following rules: 1) we mark the salient objects, taking the advice of most people; 2) disconnected regions of the same object are labeled separately; 3) we use solid regions to approximate hollow objects, such as bike wheels. Besides, we will expand this dataset continually in future.
                     </p>
                    </blockquote>
                    <div class="markdown-heading" dir="auto">
                     <h3 class="heading-element" dir="auto" tabindex="-1">
                      RGBT-Saliency Detection [need more information...]
                     </h3>
                     <a aria-label="Permalink: RGBT-Saliency Detection [need more information...]" class="anchor" href="#rgbt-saliency-detection-need-more-information" id="user-content-rgbt-saliency-detection-need-more-information">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <div class="markdown-heading" dir="auto">
                     <h4 class="heading-element" dir="auto" tabindex="-1">
                      VT1000 Dataset
                     </h4>
                     <a aria-label="Permalink: VT1000 Dataset" class="anchor" href="#vt1000-dataset" id="user-content-vt1000-dataset">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <p dir="auto">
                     <a href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/2019-05-23-10-49-47.png" rel="noopener noreferrer" target="_blank">
                      <img alt="image" src="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/2019-05-23-10-49-47.png" style="max-width: 100%;"/>
                     </a>
                    </p>
                    <p dir="auto">
                     <a href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/2019-05-23-10-50-23.png" rel="noopener noreferrer" target="_blank">
                      <img alt="thermal" src="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/2019-05-23-10-50-23.png" style="max-width: 100%;"/>
                     </a>
                    </p>
                    <p dir="auto">
                     <a href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/2019-05-23-10-50-02.png" rel="noopener noreferrer" target="_blank">
                      <img alt="mask" src="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/2019-05-23-10-50-02.png" style="max-width: 100%;"/>
                     </a>
                    </p>
                    <ul dir="auto">
                     <li>
                      论文: RGB-T Image Saliency Detection via Collaborative Graph Learning
                     </li>
                     <li>
                      项目:
                      <a href="http://chenglongli.cn/people/lcl/dataset-code.html" rel="nofollow">
                       http://chenglongli.cn/people/lcl/dataset-code.html
                      </a>
                     </li>
                     <li>
                      下载: 具体信息可见项目主页
                      <ul dir="auto">
                       <li>
                        <a href="https://drive.google.com/file/d/1NCPFNeiy1n6uY74L0FDInN27p6N_VCSd/view?usp=sharing" rel="nofollow">
                         https://drive.google.com/file/d/1NCPFNeiy1n6uY74L0FDInN27p6N_VCSd/view?usp=sharing
                        </a>
                       </li>
                       <li>
                        <a href="https://pan.baidu.com/s/1eGQJhvnKnqV1KJ1GY_63NA" rel="nofollow">
                         https://pan.baidu.com/s/1eGQJhvnKnqV1KJ1GY_63NA
                        </a>
                       </li>
                      </ul>
                     </li>
                    </ul>
                    <div class="markdown-heading" dir="auto">
                     <h4 class="heading-element" dir="auto" tabindex="-1">
                      VT821 Dataset
                     </h4>
                     <a aria-label="Permalink: VT821 Dataset" class="anchor" href="#vt821-dataset" id="user-content-vt821-dataset">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <p dir="auto">
                     <a href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/2019-05-23-10-50-42.png" rel="noopener noreferrer" target="_blank">
                      <img alt="image" src="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/2019-05-23-10-50-42.png" style="max-width: 100%;"/>
                     </a>
                    </p>
                    <p dir="auto">
                     <a href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/2019-05-23-10-51-00.png" rel="noopener noreferrer" target="_blank">
                      <img alt="mask" src="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/2019-05-23-10-51-00.png" style="max-width: 100%;"/>
                     </a>
                    </p>
                    <ul dir="auto">
                     <li>
                      论文: A Unified RGB-T Saliency Detection Benchmark: Dataset, Baselines, Analysis and A Novel Approach
                     </li>
                     <li>
                      项目:
                      <a href="http://chenglongli.cn/people/lcl/dataset-code.html" rel="nofollow">
                       http://chenglongli.cn/people/lcl/dataset-code.html
                      </a>
                     </li>
                     <li>
                      下载: 具体信息可见项目主页
                      <ul dir="auto">
                       <li>
                        <a href="https://drive.google.com/file/d/0B4fH4G1f-jjNR3NtQUkwWjFFREk/view?usp=sharing" rel="nofollow">
                         https://drive.google.com/file/d/0B4fH4G1f-jjNR3NtQUkwWjFFREk/view?usp=sharing
                        </a>
                       </li>
                       <li>
                        <a href="http://pan.baidu.com/s/1bpEaeQV" rel="nofollow">
                         http://pan.baidu.com/s/1bpEaeQV
                        </a>
                       </li>
                      </ul>
                     </li>
                    </ul>
                    <div class="markdown-heading" dir="auto">
                     <h3 class="heading-element" dir="auto" tabindex="-1">
                      High-Resolution Saliency Detection
                     </h3>
                     <a aria-label="Permalink: High-Resolution Saliency Detection" class="anchor" href="#high-resolution-saliency-detection" id="user-content-high-resolution-saliency-detection">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <div class="markdown-heading" dir="auto">
                     <h4 class="heading-element" dir="auto" tabindex="-1">
                      HRSOD/DAVIS-S
                     </h4>
                     <a aria-label="Permalink: HRSOD/DAVIS-S" class="anchor" href="#hrsoddavis-s" id="user-content-hrsoddavis-s">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <p dir="auto">
                     <a href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/2019-09-15-15-54-12.png" rel="noopener noreferrer" target="_blank">
                      <img alt="" src="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/2019-09-15-15-54-12.png" style="max-width: 100%;"/>
                     </a>
                    </p>
                    <ul dir="auto">
                     <li>
                      论文: Towards High-Resolution Salient Object Detection:
                      <a href="https://arxiv.org/pdf/1908.07274.pdf" rel="nofollow">
                       https://arxiv.org/pdf/1908.07274.pdf
                      </a>
                     </li>
                     <li>
                      项目:
                      <a href="https://github.com/yi94code/HRSOD">
                       https://github.com/yi94code/HRSOD
                      </a>
                     </li>
                     <li>
                      下载: 可见项目主页
                      <ul dir="auto">
                       <li>
                        HRSOD:
                        <a href="https://drive.google.com/open?id=1bmDGlkzqHoduNigi_GO4Qy9sA9sIaZcY" rel="nofollow">
                         https://drive.google.com/open?id=1bmDGlkzqHoduNigi_GO4Qy9sA9sIaZcY
                        </a>
                       </li>
                       <li>
                        DAVIS-S:
                        <a href="https://drive.google.com/open?id=1q1H7yoITLS6i2n-PhgYMIxLdjyhge5AR" rel="nofollow">
                         https://drive.google.com/open?id=1q1H7yoITLS6i2n-PhgYMIxLdjyhge5AR
                        </a>
                       </li>
                      </ul>
                     </li>
                    </ul>
                    <blockquote>
                     <p dir="auto">
                      ...we contribute a High-Resolution Salient Object Detection (HRSOD) dataset, containing 1610 training images and 400 test images. The total 2010 images are collected from the website of Flickr with the license of all creative commons. Pixel-level ground truths are manually annotated by 40 subjects. The shortest edge of each image in our HRSOD is more than 1200 pixels.
                     </p>
                    </blockquote>
                    <div class="markdown-heading" dir="auto">
                     <h3 class="heading-element" dir="auto" tabindex="-1">
                      Other Saliency Dataset
                     </h3>
                     <a aria-label="Permalink: Other Saliency Dataset" class="anchor" href="#other-saliency-dataset" id="user-content-other-saliency-dataset">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <div class="markdown-heading" dir="auto">
                     <h4 class="heading-element" dir="auto" tabindex="-1">
                      KAIST Salient Pedestrian Dataset
                     </h4>
                     <a aria-label="Permalink: KAIST Salient Pedestrian Dataset" class="anchor" href="#kaist-salient-pedestrian-dataset" id="user-content-kaist-salient-pedestrian-dataset">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <p dir="auto">
                     <a href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/2019-05-23-10-53-57.png" rel="noopener noreferrer" target="_blank">
                      <img alt="" src="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/2019-05-23-10-53-57.png" style="max-width: 100%;"/>
                     </a>
                    </p>
                    <ul dir="auto">
                     <li>
                      论文: Pedestrian Detection from Thermal Images using Saliency Maps
                     </li>
                     <li>
                      项目:
                      <a href="https://github.com/Information-Fusion-Lab-Umass/Salient-Pedestrian-Detection">
                       https://github.com/Information-Fusion-Lab-Umass/Salient-Pedestrian-Detection
                      </a>
                     </li>
                     <li>
                      下载: 具体详见项目页面
                     </li>
                    </ul>
                    <blockquote>
                     <p dir="auto">
                      We select 1702 images from the training set of the KAIST Multispectral Pedestrian dataset, by sampling every 15th image from all the images captured during the day and every 10thimage from all the images captured during the night, which contain pedestrians. These images were selected in order to have approximately the same number of images captured on both times of the day (913 day images and 789 night images), containing 4170 instances of pedestrians. We manually annotate these images using the VGG Image Annotator tool to generate the ground truth saliency masks based on the location of the bounding boxes on pedestrians in the original dataset. Additionally, we create a set of 362 images with similar annotations from the test set to validate our deep saliency detection networks, with 193 day images and 169 night images, containing 1029 instances of pedestrians.
                     </p>
                    </blockquote>
                    <div class="markdown-heading" dir="auto">
                     <h2 class="heading-element" dir="auto" tabindex="-1">
                      Segmentation
                     </h2>
                     <a aria-label="Permalink: Segmentation" class="anchor" href="#segmentation" id="user-content-segmentation">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <div class="markdown-heading" dir="auto">
                     <h3 class="heading-element" dir="auto" tabindex="-1">
                      General[need help]
                     </h3>
                     <a aria-label="Permalink: General[need help]" class="anchor" href="#generalneed-help" id="user-content-generalneed-help">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <div class="markdown-heading" dir="auto">
                     <h4 class="heading-element" dir="auto" tabindex="-1">
                      DAVIS
                     </h4>
                     <a aria-label="Permalink: DAVIS" class="anchor" href="#davis" id="user-content-davis">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <p dir="auto">
                     <a href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/2019-03-13-11-01-47.png" rel="noopener noreferrer" target="_blank">
                      <img alt="img" src="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/2019-03-13-11-01-47.png" style="max-width: 100%;"/>
                     </a>
                    </p>
                    <ul dir="auto">
                     <li>
                      项目:
                      <ul dir="auto">
                       <li>
                        竞赛主页:
                        <a href="https://davischallenge.org/index.html" rel="nofollow">
                         https://davischallenge.org/index.html
                        </a>
                       </li>
                      </ul>
                     </li>
                     <li>
                      论文:
                      <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Perazzi_A_Benchmark_Dataset_CVPR_2016_paper.pdf" rel="nofollow">
                       A Benchmark Dataset and Evaluation Methodology for Video Object Segmentation
                      </a>
                     </li>
                     <li>
                      下载:
                      <ul dir="auto">
                       <li>
                        <a href="https://davischallenge.org/davis2016/code.html" rel="nofollow">
                         DAVIS 2016
                        </a>
                        In each video sequence a single instance is annotated.
                       </li>
                       <li>
                        <a href="https://davischallenge.org/davis2017/code.html" rel="nofollow">
                         DAVIS 2017
                        </a>
                        In each video sequence multiple instances are annotated.
                       </li>
                      </ul>
                     </li>
                    </ul>
                    <div class="markdown-heading" dir="auto">
                     <h4 class="heading-element" dir="auto" tabindex="-1">
                      aNYU
                     </h4>
                     <a aria-label="Permalink: aNYU" class="anchor" href="#anyu" id="user-content-anyu">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <p dir="auto">
                     <a href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/1546153000959.png" rel="noopener noreferrer" target="_blank">
                      <img alt="img" src="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/1546153000959.png" style="max-width: 100%;"/>
                     </a>
                    </p>
                    <ul dir="auto">
                     <li>
                      项目:
                      <a href="https://kylezheng.org/research-projects/densesegattobj/" rel="nofollow">
                       https://kylezheng.org/research-projects/densesegattobj/
                      </a>
                     </li>
                     <li>
                      论文:
                      <a href="http://kylezheng.org/densesegattobjdataset/denseseg4objatt_CVPR2014_Kyle.pdf" rel="nofollow">
                       Dense Semantic Image Segmentation with Objects and Attributes
                      </a>
                     </li>
                     <li>
                      下载:
                      <ul dir="auto">
                       <li>
                        NYU:
                        <a href="http://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html" rel="nofollow">
                         http://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html
                        </a>
                       </li>
                       <li>
                        aNYU:
                        <a href="http://www.robots.ox.ac.uk/~szheng/aNYU/aNYU.tar.gz" rel="nofollow">
                         http://www.robots.ox.ac.uk/~szheng/aNYU/aNYU.tar.gz
                        </a>
                       </li>
                      </ul>
                     </li>
                    </ul>
                    <blockquote>
                     <p dir="auto">
                      我们的第一组实验是关于来自NYU V2 dataset的RGB图像. 如图3所示, 我们添加了8个附加属性标签, 即木制, 彩绘, 棉花, 玻璃, 光面, 塑料, 闪亮和纹理. 我们要求3个注释者在每个分割地面真实区域上分配材料, 表面属性属性. Wethen将3名工作者的多数票作为我们的8个附加属性标签. 我们将此扩展数据集称为attribute NYU(aNYU)数据集.
                      <strong>
                       该数据集从28个不同的室内场景中收集了1449个图像.
                      </strong>
                      在我们的实验中, 我们选择了具有足够数量实例的15个对象类和8个属性来训练unary potential. 此外,
                      <strong>
                       我们随机地将数据集分成训练集的725个图像, 验证集的100个, 以及测试集的624个.
                      </strong>
                     </p>
                    </blockquote>
                    <div class="markdown-heading" dir="auto">
                     <h3 class="heading-element" dir="auto" tabindex="-1">
                      About Person
                     </h3>
                     <a aria-label="Permalink: About Person" class="anchor" href="#about-person" id="user-content-about-person">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <div class="markdown-heading" dir="auto">
                     <h4 class="heading-element" dir="auto" tabindex="-1">
                      Supervisely人像数据集
                     </h4>
                     <a aria-label="Permalink: Supervisely人像数据集" class="anchor" href="#supervisely人像数据集" id="user-content-supervisely人像数据集">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <p dir="auto">
                     <a href="https://camo.githubusercontent.com/26157d92880274fae7580e4fe1b936d25710e5a6d650d3d235d7ee2260ad6d62/68747470733a2f2f7374617469632e6c656970686f6e652e636f6d2f75706c6f6164732f6e65772f61727469636c652f3734305f3734302f3230313830342f356163623137313961363235322e706e673f696d6167654d6f6772322f666f726d61742f6a70672f7175616c6974792f3930" rel="noopener noreferrer nofollow" target="_blank">
                      <img alt="img" data-canonical-src="https://static.leiphone.com/uploads/new/article/740_740/201804/5acb1719a6252.png?imageMogr2/format/jpg/quality/90" src="https://camo.githubusercontent.com/26157d92880274fae7580e4fe1b936d25710e5a6d650d3d235d7ee2260ad6d62/68747470733a2f2f7374617469632e6c656970686f6e652e636f6d2f75706c6f6164732f6e65772f61727469636c652f3734305f3734302f3230313830342f356163623137313961363235322e706e673f696d6167654d6f6772322f666f726d61742f6a70672f7175616c6974792f3930" style="max-width: 100%;"/>
                     </a>
                    </p>
                    <ul dir="auto">
                     <li>
                      项目:
                      <a href="https://supervise.ly/" rel="nofollow">
                       https://supervise.ly/
                      </a>
                     </li>
                    </ul>
                    <blockquote>
                     <ul dir="auto">
                      <li>
                       数据集
                       <strong>
                        由5711张图片组成, 有6884个高质量的标注的人体实例
                       </strong>
                       .
                      </li>
                      <li>
                       下面的所有步骤在Supervisely内部完成的, 没有任何编码.
                      </li>
                      <li>
                       更重要的是, 这些步骤是被我内部的注释器执行的, 没有任何机器学习专业知识. 数据科学家仅仅只是控制和管理这过程.
                      </li>
                      <li>
                       注释组由两名成员组成并且这整个过程只花了4天.
                      </li>
                     </ul>
                    </blockquote>
                    <div class="markdown-heading" dir="auto">
                     <h4 class="heading-element" dir="auto" tabindex="-1">
                      Clothing Parsing
                     </h4>
                     <a aria-label="Permalink: Clothing Parsing" class="anchor" href="#clothing-parsing" id="user-content-clothing-parsing">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <p dir="auto">
                     <a href="https://camo.githubusercontent.com/4b0564db9b324a86d7964d205d1c5b3a4506a405e46a417a89fd83d3d34b53d3/687474703a2f2f766973696f6e2e69732e746f686f6b752e61632e6a702f7e6b79616d6167752f72657365617263682f636c6f7468696e675f70617273696e672f636c6f7468696e672e706e67" rel="noopener noreferrer nofollow" target="_blank">
                      <img alt="img" data-canonical-src="http://vision.is.tohoku.ac.jp/~kyamagu/research/clothing_parsing/clothing.png" src="https://camo.githubusercontent.com/4b0564db9b324a86d7964d205d1c5b3a4506a405e46a417a89fd83d3d34b53d3/687474703a2f2f766973696f6e2e69732e746f686f6b752e61632e6a702f7e6b79616d6167752f72657365617263682f636c6f7468696e675f70617273696e672f636c6f7468696e672e706e67" style="max-width: 100%;"/>
                     </a>
                    </p>
                    <ul dir="auto">
                     <li>
                      项目 :
                      <ul dir="auto">
                       <li>
                        <a href="http://vision.is.tohoku.ac.jp/~kyamagu/research/clothing_parsing/" rel="nofollow">
                         http://vision.is.tohoku.ac.jp/~kyamagu/research/clothing_parsing/
                        </a>
                       </li>
                       <li>
                        <a href="http://vision.is.tohoku.ac.jp/~kyamagu/research/paperdoll/" rel="nofollow">
                         http://vision.is.tohoku.ac.jp/~kyamagu/research/paperdoll/
                        </a>
                       </li>
                      </ul>
                     </li>
                    </ul>
                    <blockquote>
                     <p dir="auto">
                      In this paper we demonstrate an effective method for parsing clothing in fashion photographs, an extremely challenging problem due to the large number of possible garment items, variations in configuration, garment appearance, layering, and occlusion. In addition, we provide a large novel dataset and tools for labeling garment items, to enable future research on clothing estimation. Finally, we present intriguing initial results on using clothing estimates to improve pose identification, and demonstrate a prototype application for pose-independent visual garment retrieval.
                     </p>
                    </blockquote>
                    <div class="markdown-heading" dir="auto">
                     <h4 class="heading-element" dir="auto" tabindex="-1">
                      HumanParsing-Dataset
                     </h4>
                     <a aria-label="Permalink: HumanParsing-Dataset" class="anchor" href="#humanparsing-dataset" id="user-content-humanparsing-dataset">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <p dir="auto">
                     <a href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/2019-03-22-19-14-03.png" rel="noopener noreferrer" target="_blank">
                      <img alt="img" src="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/2019-03-22-19-14-03.png" style="max-width: 100%;"/>
                     </a>
                    </p>
                    <ul dir="auto">
                     <li>
                      项目:
                      <ul dir="auto">
                       <li>
                        <a href="https://github.com/lemondan/HumanParsing-Dataset">
                         https://github.com/lemondan/HumanParsing-Dataset
                        </a>
                       </li>
                       <li>
                        <a href="http://www.sysu-hcp.net/deep-human-parsing/" rel="nofollow">
                         http://www.sysu-hcp.net/deep-human-parsing/
                        </a>
                       </li>
                       <li>
                        <a href="https://vuhcs.github.io/" rel="nofollow">
                         https://vuhcs.github.io/
                        </a>
                       </li>
                      </ul>
                     </li>
                     <li>
                      组织:
                      <a href="http://sysu-hcp.net/" rel="nofollow">
                       http://sysu-hcp.net/
                      </a>
                     </li>
                     <li>
                      下载:
                      <a href="http://pan.baidu.com/s/1qY8bToS" rel="nofollow">
                       http://pan.baidu.com/s/1qY8bToS
                      </a>
                      (kjgk)
                     </li>
                    </ul>
                    <blockquote>
                     <p dir="auto">
                      This human parsing dataset includes the detailed pixel-wise annotations for fashion images, which is proposed in our TPAMI paper "Deep Human Parsing with Active Template Regression", and ICCV 2015 paper "Human Parsing with Contextualized Convolutional Neural Network". This dataset contains 7700 images. We use 6000 images for training, 1000 for testing and 700 as the validation set.
                     </p>
                    </blockquote>
                    <div class="markdown-heading" dir="auto">
                     <h4 class="heading-element" dir="auto" tabindex="-1">
                      Look into Person (LIP)
                     </h4>
                     <a aria-label="Permalink: Look into Person (LIP)" class="anchor" href="#look-into-person-lip" id="user-content-look-into-person-lip">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <p dir="auto">
                     <a href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/2019-03-12-11-18-29.png" rel="noopener noreferrer" target="_blank">
                      <img alt="img" src="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/2019-03-12-11-18-29.png" style="max-width: 100%;"/>
                     </a>
                    </p>
                    <ul dir="auto">
                     <li>
                      项目:
                      <a href="http://sysu-hcp.net/lip/overview.php" rel="nofollow">
                       http://sysu-hcp.net/lip/overview.php
                      </a>
                     </li>
                     <li>
                      下载: 不同任务有不同部分, 具体可见
                      <a href="http://sysu-hcp.net/lip/overview.php" rel="nofollow">
                       Dataset
                      </a>
                      页面
                     </li>
                    </ul>
                    <blockquote>
                     <p dir="auto">
                      Look into Person (LIP) is a new large-scale dataset, focus on semantic understanding of person. Following are the detailed descriptions.
                     </p>
                     <p dir="auto">
                      The dataset contains 50, 000 images with elaborated pixel-wise annotations with 19 semantic human part labels and 2D human poses with 16 key points.
                     </p>
                     <p dir="auto">
                      The annotated 50, 000 images are cropped person instances from COCO dataset with size larger than 50 * 50. The images collected from the real-world scenarios contain human appearing with challenging poses and views, heavily occlusions, various appearances and low-resolutions. We are working on collecting and annotating more images to increase diversity.
                     </p>
                    </blockquote>
                    <div class="markdown-heading" dir="auto">
                     <h4 class="heading-element" dir="auto" tabindex="-1">
                      Taobao Commodity Dataset
                     </h4>
                     <a aria-label="Permalink: Taobao Commodity Dataset" class="anchor" href="#taobao-commodity-dataset" id="user-content-taobao-commodity-dataset">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <p dir="auto">
                     <a href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/2019-03-22-19-09-55.png" rel="noopener noreferrer" target="_blank">
                      <img alt="img" src="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/2019-03-22-19-09-55.png" style="max-width: 100%;"/>
                     </a>
                    </p>
                    <ul dir="auto">
                     <li>
                      项目:
                      <a href="http://www.sysu-hcp.net/taobao-commodity-dataset/" rel="nofollow">
                       http://www.sysu-hcp.net/taobao-commodity-dataset/
                      </a>
                     </li>
                     <li>
                      下载:
                      <ul dir="auto">
                       <li>
                        <a href="http://www.sysu-hcp.net/wp-content/uploads/2016/03/Imgs_TCD.zip" rel="nofollow">
                         http://www.sysu-hcp.net/wp-content/uploads/2016/03/Imgs_TCD.zip
                        </a>
                       </li>
                       <li>
                        <a href="http://www.sysu-hcp.net/wp-content/uploads/2016/03/Mask_TCD.zip" rel="nofollow">
                         http://www.sysu-hcp.net/wp-content/uploads/2016/03/Mask_TCD.zip
                        </a>
                       </li>
                      </ul>
                     </li>
                    </ul>
                    <blockquote>
                     <p dir="auto">
                      TCD contains 800 commodity images (dresses, jeans, T-shirts, shoes and hats) from the shops on the Taobao website. The ground truth masks of the TCD dataset are obtained by inviting common sellers of Taobao website to annotate their commodities, i.e., masking salient objects that they want to show from their exhibition. These images include all kind   s of commodity with and without human models, thus having complex backgrounds and scenes with highly complex foregrounds. Pixel-accurate ground truth masks are given. These images including all kinds of commodities with and without human models have complex backgrounds and scenes with large foregrounds for evaluation. Figure 1 illustrates some of them.
                     </p>
                    </blockquote>
                    <div class="markdown-heading" dir="auto">
                     <h4 class="heading-element" dir="auto" tabindex="-1">
                      Object Extraction Dataset
                     </h4>
                     <a aria-label="Permalink: Object Extraction Dataset" class="anchor" href="#object-extraction-dataset" id="user-content-object-extraction-dataset">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <p dir="auto">
                     <a href="https://camo.githubusercontent.com/829ac09125e91b21f8e3562b06d148472843f3b1209e45bf3c6061d8b54010fa/68747470733a2f2f6f626a65637465787472616374696f6e2e6769746875622e696f2f696d67732f696d616765735f6d61736b732e706e67" rel="noopener noreferrer nofollow" target="_blank">
                      <img alt="img" data-canonical-src="https://objectextraction.github.io/imgs/images_masks.png" src="https://camo.githubusercontent.com/829ac09125e91b21f8e3562b06d148472843f3b1209e45bf3c6061d8b54010fa/68747470733a2f2f6f626a65637465787472616374696f6e2e6769746875622e696f2f696d67732f696d616765735f6d61736b732e706e67" style="max-width: 100%;"/>
                     </a>
                    </p>
                    <ul dir="auto">
                     <li>
                      项目:
                      <a href="https://objectextraction.github.io/" rel="nofollow">
                       https://objectextraction.github.io/
                      </a>
                     </li>
                    </ul>
                    <blockquote>
                     <p dir="auto">
                      This Object Extraction newly collected by us contains 10183 images with groundtruth segmentation masks. We selected the images from the PASCAL, iCoseg, Internet dataset as well as other data (most of them are about people and clothes) from the web. We randomly split the dataset with 8230 images for training and 1953 images for testing.
                     </p>
                    </blockquote>
                    <div class="markdown-heading" dir="auto">
                     <h4 class="heading-element" dir="auto" tabindex="-1">
                      Clothing Co-Parsing (CCP) Dataset
                     </h4>
                     <a aria-label="Permalink: Clothing Co-Parsing (CCP) Dataset" class="anchor" href="#clothing-co-parsing-ccp-dataset" id="user-content-clothing-co-parsing-ccp-dataset">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <p dir="auto">
                     <a href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/2019-03-12-11-12-28.png" rel="noopener noreferrer" target="_blank">
                      <img alt="img" src="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/2019-03-12-11-12-28.png" style="max-width: 100%;"/>
                     </a>
                    </p>
                    <ul dir="auto">
                     <li>
                      项目:
                      <a href="https://github.com/bearpaw/clothing-co-parsing">
                       https://github.com/bearpaw/clothing-co-parsing
                      </a>
                     </li>
                    </ul>
                    <blockquote>
                     <p dir="auto">
                      Clothing Co-Parsing (CCP) dataset is a new clothing database including elaborately annotated clothing items.
2, 098 high-resolution street fashion photos with totally 59 tags
Wide range of styles, accessaries, garments, and pose
All images are with image-level annotations
1000+ images are with pixel-level annotations
                     </p>
                    </blockquote>
                    <div class="markdown-heading" dir="auto">
                     <h4 class="heading-element" dir="auto" tabindex="-1">
                      Baidu People segmentation dataset[need help]
                     </h4>
                     <a aria-label="Permalink: Baidu People segmentation dataset[need help]" class="anchor" href="#baidu-people-segmentation-datasetneed-help" id="user-content-baidu-people-segmentation-datasetneed-help">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <ul dir="auto">
                     <li>
                      下载:
                      <a href="http://www.cbsr.ia.ac.cn/users/ynyu/dataset/" rel="nofollow">
                       http://www.cbsr.ia.ac.cn/users/ynyu/dataset/
                      </a>
                     </li>
                    </ul>
                    <blockquote>
                     <p dir="auto">
                      这个数据集主要是用于人体整体分割. 它由5387张训练图片组成, 但是测试图片没有公布. 因此训练时可以从5387中随机挑选500张作为验证集, 然后4887张作为训练集. 参考论文《Early Hierarchical Contexts Learned by CNN for image segmentation》.
                     </p>
                     <p dir="auto">
                      原文:
                      <a href="https://blog.csdn.net/mou_it/article/details/82225505" rel="nofollow">
                       https://blog.csdn.net/mou_it/article/details/82225505
                      </a>
                     </p>
                    </blockquote>
                    <div class="markdown-heading" dir="auto">
                     <h2 class="heading-element" dir="auto" tabindex="-1">
                      Matting
                     </h2>
                     <a aria-label="Permalink: Matting" class="anchor" href="#matting" id="user-content-matting">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <div class="markdown-heading" dir="auto">
                     <h3 class="heading-element" dir="auto" tabindex="-1">
                      alphamatting.com
                     </h3>
                     <a aria-label="Permalink: alphamatting.com" class="anchor" href="#alphamattingcom" id="user-content-alphamattingcom">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <p dir="auto">
                     <a href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/1546154705536.png" rel="noopener noreferrer" target="_blank">
                      <img alt="1546154705536" src="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/1546154705536.png" style="max-width: 100%;"/>
                     </a>
                    </p>
                    <ul dir="auto">
                     <li>
                      项目:
                      <a href="http://alphamatting.com/datasets.php" rel="nofollow">
                       http://alphamatting.com/datasets.php
                      </a>
                     </li>
                     <li>
                      下载:
                      <ul dir="auto">
                       <li>
                        <a href="http://alphamatting.com/datasets/zip/input_training_lowres.zip" rel="nofollow">
                         input_training_lowres.zip
                        </a>
                       </li>
                       <li>
                        <a href="http://alphamatting.com/datasets/zip/input_training_highres.zip" rel="nofollow">
                         input_training_highres.zip
                        </a>
                       </li>
                       <li>
                        <a href="http://alphamatting.com/datasets/zip/trimap_training_lowres.zip" rel="nofollow">
                         trimap_training_lowres.zip
                        </a>
                       </li>
                       <li>
                        <a href="http://alphamatting.com/datasets/zip/trimap_training_highres.zip" rel="nofollow">
                         trimap_training_highres.zip
                        </a>
                       </li>
                       <li>
                        <a href="http://alphamatting.com/datasets/zip/gt_training_lowres.zip" rel="nofollow">
                         gt_training_lowres.zip
                        </a>
                       </li>
                       <li>
                        <a href="http://alphamatting.com/datasets/zip/gt_training_highres.zip" rel="nofollow">
                         gt_training_highres.zip
                        </a>
                       </li>
                      </ul>
                     </li>
                    </ul>
                    <blockquote>
                     <p dir="auto">
                      这是图像matting方法的现有基准. 它
                      <strong>
                       包括8个测试图像, 每个图像有3个不同的三维图形
                      </strong>
                      , 即"small", "large"和"user"
                     </p>
                    </blockquote>
                    <div class="markdown-heading" dir="auto">
                     <h3 class="heading-element" dir="auto" tabindex="-1">
                      Composition-1k: Deep Image Matting
                     </h3>
                     <a aria-label="Permalink: Composition-1k: Deep Image Matting" class="anchor" href="#composition-1k-deep-image-matting" id="user-content-composition-1k-deep-image-matting">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <p dir="auto">
                     <a href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/1546154519720.png" rel="noopener noreferrer" target="_blank">
                      <img alt="1546154519720" src="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/1546154519720.png" style="max-width: 100%;"/>
                     </a>
                    </p>
                    <ul dir="auto">
                     <li>
                      项目:
                      <a href="https://sites.google.com/view/deepimagematting" rel="nofollow">
                       https://sites.google.com/view/deepimagematting
                      </a>
                     </li>
                     <li>
                      论文:
                      <a href="https://arxiv.org/abs/1703.03872" rel="nofollow">
                       https://arxiv.org/abs/1703.03872
                      </a>
                     </li>
                     <li>
                      下载: Please contact Brian Price (
                      <a href="mailto:bprice@adobe.com">
                       bprice@adobe.com
                      </a>
                      ) for the dataset.
                     </li>
                    </ul>
                    <blockquote>
                     <p dir="auto">
                      抠图是一个基本的计算机视觉问题, 有许多应用. 当图像具有相似的前景色和背景色或复杂的纹理时, 先前的算法具有差的性能. 主要原因是先前的方法 1)仅使用低级功能和2)缺乏高级上下文. 在本文中, 我们提出了一种新的基于深度学习的算法, 可以解决这两个问题. 我们的深层模型有两个部分. 第一部分是深度卷积编码器 * 解码器网络, 它将图像和相应的trimap作为输入并预测图像的alpha遮罩. 第二部分是一个小的卷积网络, 它改进了第一个网络的alpha遮罩预测, 以获得更准确的alpha值和更清晰的边缘. 此外,
                      <strong>
                       我们还创建了一个大型图像抠图数据集, 包括49300个训练图像和1000个测试图像
                      </strong>
                      . 我们在抠图基准, 我们的测试集和各种真实图像上评估我们的算法. 实验结果清楚地证明了我们的算法优于以前的方法.
                     </p>
                     <p dir="auto">
                      我们使用合成创建一个大规模的matting数据集. 仔细提取具有简单背景上的对象的图像并将其合成到新的背景图像上以创建具有49300(45500)个训练图像和1000个测试图像的数据集.
                     </p>
                     <p dir="auto">
                      ......
                     </p>
                     <p dir="auto">
                      我们将评估3个数据集上的方法.1)我们评估alphamatting.com数据集, 这是图像matting方法的现有基准. 它
                      <strong>
                       包括8个测试图像, 每个图像有3个不同的三维图形
                      </strong>
                      , 即"小", "大"和"用户".2)由于alphamatting.com数据集中对象的大小和范围有限, **我们提出了Composition-1k测试集. 我们基于作品的数据集包括1000个图像和50个独特的前景. 此数据集具有更广泛的对象类型和背景场景.**3)为了测量我们在自然图像上的表现, 我们还收集了包括31个自然图像的第三个数据集.
                     </p>
                    </blockquote>
                    <div class="markdown-heading" dir="auto">
                     <h3 class="heading-element" dir="auto" tabindex="-1">
                      Semantic Human Matting
                     </h3>
                     <a aria-label="Permalink: Semantic Human Matting" class="anchor" href="#semantic-human-matting" id="user-content-semantic-human-matting">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <p dir="auto">
                     <a href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/1546156688347.png" rel="noopener noreferrer" target="_blank">
                      <img alt="1546156688347" src="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/1546156688347.png" style="max-width: 100%;"/>
                     </a>
                    </p>
                    <p dir="auto">
                     <a href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/2018-12-27-11-47-26.png" rel="noopener noreferrer" target="_blank">
                      <img alt="dataset" src="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/2018-12-27-11-47-26.png" style="max-width: 100%;"/>
                     </a>
                    </p>
                    <ul dir="auto">
                     <li>
                      论文:
                      <a href="https://arxiv.org/abs/1809.01354" rel="nofollow">
                       https://arxiv.org/abs/1809.01354
                      </a>
                     </li>
                    </ul>
                    <blockquote>
                     <ul dir="auto">
                      <li>
                       alpha matting 的資料庫樣本過少, 對於深度學習來說首要條件就是資料樣本要多
                      </li>
                      <li>
                       Shen et al. 此資料庫是透過 CF 以及 KNN 的方式所製造的, 因此有可能該資料庫有bias, 不採用.(這部分可搜尋幾個關鍵字: deep learning dataset bias).
                      </li>
                      <li>
                       DIM 的資料庫雖然有 493 個物件, 但是物件中包含人物的只有 202 個.
                      </li>
                      <li>
                       Our dataset 從電子商務網站中搜集圖片, 將35, 513個人物透過人工標注他的Annotation, 此資料集有遵循DIM的方法收集.
                      </li>
                     </ul>
                     <p dir="auto">
                      <a href="https://medium.com/@xiaosean5408/%E6%B7%98%E5%AF%B6%E7%B6%B2%E7%9A%84%E4%BA%BA%E7%89%A9%E6%8F%90%E5%8F%96%E8%AB%96%E6%96%87%E7%B0%A1%E4%BB%8B-semantic-human-matting-52591c3f8e0c" rel="nofollow">
                       https://medium.com/@xiaosean5408/%E6%B7%98%E5%AF%B6%E7%B6%B2%E7%9A%84%E4%BA%BA%E7%89%A9%E6%8F%90%E5%8F%96%E8%AB%96%E6%96%87%E7%B0%A1%E4%BB%8B-semantic-human-matting-52591c3f8e0c
                      </a>
                     </p>
                    </blockquote>
                    <div class="markdown-heading" dir="auto">
                     <h3 class="heading-element" dir="auto" tabindex="-1">
                      Matting-Human-Datasets
                     </h3>
                     <a aria-label="Permalink: Matting-Human-Datasets" class="anchor" href="#matting-human-datasets" id="user-content-matting-human-datasets">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <p dir="auto">
                     <a href="https://github.com/aisegmentcn/matting_human_datasets/raw/master/1.png" rel="noopener noreferrer" target="_blank">
                      <img alt="" src="https://github.com/aisegmentcn/matting_human_datasets/raw/master/1.png" style="max-width: 100%;"/>
                     </a>
                    </p>
                    <p dir="auto">
                     <a href="https://github.com/aisegmentcn/matting_human_datasets/raw/master/2.png" rel="noopener noreferrer" target="_blank">
                      <img alt="" src="https://github.com/aisegmentcn/matting_human_datasets/raw/master/2.png" style="max-width: 100%;"/>
                     </a>
                    </p>
                    <ul dir="auto">
                     <li>
                      项目:
                      <a href="https://github.com/aisegmentcn/matting_human_datasets">
                       https://github.com/aisegmentcn/matting_human_datasets
                      </a>
                     </li>
                     <li>
                      下载:
                      <ul dir="auto">
                       <li>
                        百度云盘:
                        <a href="https://pan.baidu.com/s/1R9PJJRT-KjSxh-2-3wCGxQ" rel="nofollow">
                         https://pan.baidu.com/s/1R9PJJRT-KjSxh-2-3wCGxQ
                        </a>
                        提取码:dzsn
                       </li>
                       <li>
                        mega:
                        <a href="https://mega.nz/#F!Gh8CFAyb!e2ppUh-copP76GbE8IWAEQ" rel="nofollow">
                         https://mega.nz/#F!Gh8CFAyb!e2ppUh-copP76GbE8IWAEQ
                        </a>
                       </li>
                       <li>
                        kaggle:
                        <a href="https://www.kaggle.com/laurentmih/aisegmentcom-matting-human-datasets/" rel="nofollow">
                         https://www.kaggle.com/laurentmih/aisegmentcom-matting-human-datasets/
                        </a>
                       </li>
                      </ul>
                     </li>
                    </ul>
                    <blockquote>
                     <p dir="auto">
                      本数据集为目前已知最大的人像matting数据集, 包含34427张图像和对应的matting结果图. 数据集由北京玩星汇聚科技有限公司高质量标注, 使用该数据集所训练的人像软分割模型已商用.
                     </p>
                     <p dir="auto">
                      数据集中的原始图片来源于Flickr, 百度, 淘宝. 经过人脸检测和区域裁剪后生成了600*800的半身人像.
                     </p>
                     <ul dir="auto">
                      <li>
                       clip_img目录为半身人像图像, 格式为jpg;
                      </li>
                      <li>
                       matting目录为对应的matting文件(方便确认matting质量), 格式为png, 您训练前应该先从png图像提取alpha图. 例如使用opencv可以这样获得alpha图:
                      </li>
                     </ul>
                    </blockquote>
                    <div class="highlight highlight-source-python notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="in_image = cv2.imread('png图像文件路径', cv2.IMREAD_UNCHANGED)
alpha = in_image[:,:,3]" dir="auto">
                     <pre><span class="pl-s1">in_image</span> <span class="pl-c1">=</span> <span class="pl-s1">cv2</span>.<span class="pl-en">imread</span>(<span class="pl-s">'png图像文件路径'</span>, <span class="pl-s1">cv2</span>.<span class="pl-v">IMREAD_UNCHANGED</span>)
<span class="pl-s1">alpha</span> <span class="pl-c1">=</span> <span class="pl-s1">in_image</span>[:,:,<span class="pl-c1">3</span>]</pre>
                    </div>
                    <div class="markdown-heading" dir="auto">
                     <h3 class="heading-element" dir="auto" tabindex="-1">
                      PFCN
                     </h3>
                     <a aria-label="Permalink: PFCN" class="anchor" href="#pfcn" id="user-content-pfcn">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <p dir="auto">
                     <a href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/1546173669466.png" rel="noopener noreferrer" target="_blank">
                      <img alt="1546173669466" src="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/1546173669466.png" style="max-width: 100%;"/>
                     </a>
                    </p>
                    <ul dir="auto">
                     <li>
                      项目:
                      <a href="http://xiaoyongshen.me/webpage_portrait/index.html" rel="nofollow">
                       http://xiaoyongshen.me/webpage_portrait/index.html
                      </a>
                     </li>
                     <li>
                      论文:
                      <a href="http://xiaoyongshen.me/webpage_portrait/papers/portrait_eg16.pdf" rel="nofollow">
                       Automatic Portrait Segmentation for Image Stylization
                      </a>
                     </li>
                     <li>
                      下载: Please download from
                      <a href="https://1drv.ms/u/s!ApwdOxIIFBH19TzDv7nRfH5ZsMNL" rel="nofollow">
                       OneDrive
                      </a>
                      or
                      <a href="http://pan.baidu.com/s/1bQ4yHC" rel="nofollow">
                       Baiduyun
                      </a>
                      .
                     </li>
                    </ul>
                    <blockquote>
                     <p dir="auto">
                      肖像画是摄影和绘画的主要艺术形式. 在大多数情况下, 艺术家试图使主体从周围突出, 例如, 使其更亮或更锐利. 在数字世界中, 通过使用适合于图像语义的照相或绘画滤镜处理肖像图像, 可以实现类似的效果. 虽然存在许多成功的用户指导方法来描绘该主题, 但缺乏全自动技术并且产生不令人满意的结果. 我们的论文首先通过引入专用于肖像的新自动分割算法来解决这个问题. 然后, 我们在此结果的基础上, 描述了几个利用我们的自动分割算法生成高质量肖像的肖像滤镜.
                     </p>
                    </blockquote>
                    <div class="markdown-heading" dir="auto">
                     <h3 class="heading-element" dir="auto" tabindex="-1">
                      Deep Automatic Portrait Matting
                     </h3>
                     <a aria-label="Permalink: Deep Automatic Portrait Matting" class="anchor" href="#deep-automatic-portrait-matting" id="user-content-deep-automatic-portrait-matting">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <p dir="auto">
                     <a href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/2019-01-01-19-31-55.png" rel="noopener noreferrer" target="_blank">
                      <img alt="img" src="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/2019-01-01-19-31-55.png" style="max-width: 100%;"/>
                     </a>
                    </p>
                    <ul dir="auto">
                     <li>
                      论文:
                      <a href="http://www.cse.cuhk.edu.hk/~leojia/projects/automatting/papers/deepmatting.pdf" rel="nofollow">
                       http://www.cse.cuhk.edu.hk/~leojia/projects/automatting/papers/deepmatting.pdf
                      </a>
                     </li>
                     <li>
                      项目:
                      <ul dir="auto">
                       <li>
                        <a href="http://www.cse.cuhk.edu.hk/~leojia/projects/automatting/" rel="nofollow">
                         http://www.cse.cuhk.edu.hk/~leojia/projects/automatting/
                        </a>
                       </li>
                       <li>
                        <a href="http://xiaoyongshen.me/webpages/webpage_automatting/" rel="nofollow">
                         http://xiaoyongshen.me/webpages/webpage_automatting/
                        </a>
                       </li>
                      </ul>
                     </li>
                     <li>
                      下载:
                      <ul dir="auto">
                       <li>
                        [Data(zip, 1.15GB)] Please send Email to
                        <a href="mailto:goodshenxy@gmail.com">
                         goodshenxy@gmail.com
                        </a>
                        to request it.
                       </li>
                       <li>
                        作者自己公开了:
                        <a href="https://1drv.ms/u/s!ApwdOxIIFBH19Ts5EuFd9gVJrKTo" rel="nofollow">
                         https://1drv.ms/u/s!ApwdOxIIFBH19Ts5EuFd9gVJrKTo
                        </a>
                       </li>
                      </ul>
                     </li>
                    </ul>
                    <blockquote>
                     <p dir="auto">
                      我们提出了一种用于性状图像的自动图像matting方法. 该方法不需要用户交互, 这在大多数先前的方法中是必不可少的. 为了实现这一目标, 提出了一种新的端到端卷积神经网络(CNN)框架, 其采用肖像图像的输入. 它输出matting的结果. 我们的方法不仅考虑图像语义预测, 还考虑像素级图像matte优化. 一个新的肖像image dataset与我们标记的matting基础事实构成. 我们的自动方法通过最先进的方法获得了可比较的结果, 该方法需要指定的前景和背景区域或像素.
                     </p>
                     <p dir="auto">
                      我们从Flickr收集了肖像图像. 然后选择它们以确保肖像具有各种年龄, 颜色, 衣服, 配饰, 发型, 头部位置, 背景场景等.matting区域主要是由于景深引起的头发和柔软边缘. 裁剪所有图像, 使得面部矩形具有相似的尺寸. 通过选定的肖像图像, 我们创建了具有密集用户交互的alpha matte, 以确保它们具有高质量.
                     </p>
                     <p dir="auto">
                      首先, 我们标记每个图像放大到局部区域的三元组.
                     </p>
                     <p dir="auto">
                      然后我们计算mattes, 使用闭式matting[1]和KNN matting[2].
                     </p>
                     <p dir="auto">
                      每个图像的两个计算遮罩覆盖背景图像以手动检查质量. 我们为数据集选择更好的一个. 如果两个mattes都不符合我们的高标准, 结果将被丢弃. 必要时, 小错误可以通过Photoshop[31]来解决. 在此标签处理后, 我们收集了2, 000张高质量遮罩图像. 这些图像被随机分成训练和测试集, 分别具有1, 700和300个图像.
                     </p>
                    </blockquote>
                    <div class="markdown-heading" dir="auto">
                     <h2 class="heading-element" dir="auto" tabindex="-1">
                      Other
                     </h2>
                     <a aria-label="Permalink: Other" class="anchor" href="#other" id="user-content-other">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <div class="markdown-heading" dir="auto">
                     <h3 class="heading-element" dir="auto" tabindex="-1">
                      Large-scale Fashion (DeepFashion) Database
                     </h3>
                     <a aria-label="Permalink: Large-scale Fashion (DeepFashion) Database" class="anchor" href="#large-scale-fashion-deepfashion-database" id="user-content-large-scale-fashion-deepfashion-database">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <p dir="auto">
                     <a href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/2019-03-22-18-57-36.png" rel="noopener noreferrer" target="_blank">
                      <img alt="img" src="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/2019-03-22-18-57-36.png" style="max-width: 100%;"/>
                     </a>
                    </p>
                    <ul dir="auto">
                     <li>
                      项目:
                      <a href="http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html" rel="nofollow">
                       http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html
                      </a>
                     </li>
                     <li>
                      组织:
                      <ul dir="auto">
                       <li>
                        <a href="http://mmlab.ie.cuhk.edu.hk/" rel="nofollow">
                         Multimedia Laboratory
                        </a>
                       </li>
                       <li>
                        <a href="http://www.cuhk.edu.hk/english/index.html" rel="nofollow">
                         The Chinese University of Hong Kong
                        </a>
                       </li>
                      </ul>
                     </li>
                     <li>
                      下载:
                      <a href="http://pan.baidu.com/s/1i43pnZR" rel="nofollow">
                       http://pan.baidu.com/s/1i43pnZR
                      </a>
                      (更多细节请见项目主页)
                     </li>
                    </ul>
                    <blockquote>
                     <p dir="auto">
                      我们提供DeepFashion数据库, 这是一个大型服装数据库, 它有几个吸引人的特性:
                     </p>
                     <ul dir="auto">
                      <li>
                       首先, DeepFashion包含超过800, 000种不同的时尚图像, 从精美的商店图像到无约束的消费者照片.
                      </li>
                      <li>
                       其次, DeepFashion注释了丰富的服装商品信息. 此数据集中的每个图像都标有50个类别, 1, 000个描述性属性, 边界框和服装标记.
                      </li>
                      <li>
                       第三, DeepFashion包含超过300, 000个交叉姿势/跨域图像对. 使用DeepFashion数据库开发了四个基准, 包括属性预测, 消费者到商店的衣服检索, 店内衣服检索和地标检测.
                      </li>
                     </ul>
                     <p dir="auto">
                      这些基准的数据和注释也可以用作以下计算机视觉任务的训练和测试集, 例如衣服检测, 衣服识别和图像检索. 请阅读"下载说明"以访问数据集.
                     </p>
                    </blockquote>
                    <div class="markdown-heading" dir="auto">
                     <h3 class="heading-element" dir="auto" tabindex="-1">
                      ML-Image
                     </h3>
                     <a aria-label="Permalink: ML-Image" class="anchor" href="#ml-image" id="user-content-ml-image">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <ul dir="auto">
                     <li>
                      项目:
                      <a href="https://github.com/Tencent/tencent-ml-images#download-images-from-open-images">
                       https://github.com/Tencent/tencent-ml-images#download-images-from-open-images
                      </a>
                     </li>
                    </ul>
                    <blockquote>
                     <p dir="auto">
                      ML-Images: the largest open-source multi-label image database, including 17, 609, 752 training and 88, 739 validation image URLs, which are annotated with up to 11, 166 categories
                     </p>
                    </blockquote>
                    <div class="markdown-heading" dir="auto">
                     <h2 class="heading-element" dir="auto" tabindex="-1">
                      need your help...
                     </h2>
                     <a aria-label="Permalink: need your help..." class="anchor" href="#need-your-help" id="user-content-need-your-help">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <blockquote>
                     <p dir="auto">
                      有些数据集已经忘记了出处, 大家有见过的, 希望可以补充下.
                     </p>
                    </blockquote>
                    <ul dir="auto">
                     <li>
                      Image Pair
                     </li>
                     <li>
                      Cosal2015
                     </li>
                     <li>
                      INCT2016
                     </li>
                     <li>
                      RGBDCoseg183
                     </li>
                     <li>
                      06RGBDCosal150
                     </li>
                     <li>
                      SegTrackV1/V2
                     </li>
                     <li>
                      ViSal
                     </li>
                     <li>
                      MCL
                     </li>
                     <li>
                      UVSD
                     </li>
                     <li>
                      VOS
                     </li>
                    </ul>
                    <div class="markdown-heading" dir="auto">
                     <h2 class="heading-element" dir="auto" tabindex="-1">
                      Reference
                     </h2>
                     <a aria-label="Permalink: Reference" class="anchor" href="#reference" id="user-content-reference">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <div class="markdown-heading" dir="auto">
                     <h3 class="heading-element" dir="auto" tabindex="-1">
                      <a href="https://arxiv.org/abs/1411.5878" rel="nofollow">
                       Salient Object Detection: A Survey
                      </a>
                     </h3>
                     <a aria-label="Permalink: Salient Object Detection: A Survey" class="anchor" href="#salient-object-detection-a-survey" id="user-content-salient-object-detection-a-survey">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <p dir="auto">
                     <a href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/2018-12-29-17-06-42.png" rel="noopener noreferrer" target="_blank">
                      <img alt="img" src="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/2018-12-29-17-06-42.png" style="max-width: 100%;"/>
                     </a>
                    </p>
                    <p dir="auto">
                     详细评估:
                     <a href="https://mmcheng.net/zh/salobjbenchmark/" rel="nofollow">
                      https://mmcheng.net/zh/salobjbenchmark/
                     </a>
                     (这里展示了{THUR15K, JuddDB, DUT-OMRON, SED2, MSRA10K, ECSSD}六种数据集的一个榜单).
                    </p>
                    <div class="markdown-heading" dir="auto">
                     <h3 class="heading-element" dir="auto" tabindex="-1">
                      <a href="https://arxiv.org/abs/1803.03391" rel="nofollow">
                       Review of Visual Saliency Detection with Comprehensive Information
                      </a>
                     </h3>
                     <a aria-label="Permalink: Review of Visual Saliency Detection with Comprehensive Information" class="anchor" href="#review-of-visual-saliency-detection-with-comprehensive-information" id="user-content-review-of-visual-saliency-detection-with-comprehensive-information">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <p dir="auto">
                     <a href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/blob/master/assets/2018-12-27-11-05-49.png" rel="noopener noreferrer" target="_blank">
                      <img alt="dataset" src="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/raw/master/assets/2018-12-27-11-05-49.png" style="max-width: 100%;"/>
                     </a>
                    </p>
                    <div class="markdown-heading" dir="auto">
                     <h3 class="heading-element" dir="auto" tabindex="-1">
                      <a href="https://www.researchgate.net/publication/332553805_Salient_Object_Detection_in_the_Deep_Learning_Era_An_In-Depth_Survey" rel="nofollow">
                       Salient Object Detection in the Deep Learning Era: An In-Depth Survey
                      </a>
                     </h3>
                     <a aria-label="Permalink: Salient Object Detection in the Deep Learning Era: An In-Depth Survey" class="anchor" href="#salient-object-detection-in-the-deep-learning-era-an-in-depth-survey" id="user-content-salient-object-detection-in-the-deep-learning-era-an-in-depth-survey">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <ul dir="auto">
                     <li>
                      项目:
                      <a href="https://github.com/wenguanwang/SODsurvey">
                       https://github.com/wenguanwang/SODsurvey
                      </a>
                     </li>
                     <li>
                      说明: 本文档于2019年07月07日修改的内容主要参考自该综述论文, 感谢作者的工作, 总结的非常详细!
                     </li>
                    </ul>
                    <div class="markdown-heading" dir="auto">
                     <h2 class="heading-element" dir="auto" tabindex="-1">
                      More
                     </h2>
                     <a aria-label="Permalink: More" class="anchor" href="#more" id="user-content-more">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <div class="markdown-heading" dir="auto">
                     <h3 class="heading-element" dir="auto" tabindex="-1">
                      Similiar Projects
                     </h3>
                     <a aria-label="Permalink: Similiar Projects" class="anchor" href="#similiar-projects" id="user-content-similiar-projects">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <ul dir="auto">
                     <li>
                      <a href="https://github.com/mrgloom/awesome-semantic-segmentation">
                       awesome-semantic-segmentation
                      </a>
                     </li>
                    </ul>
                    <div class="markdown-heading" dir="auto">
                     <h3 class="heading-element" dir="auto" tabindex="-1">
                      Research Institutes
                     </h3>
                     <a aria-label="Permalink: Research Institutes" class="anchor" href="#research-institutes" id="user-content-research-institutes">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <ul dir="auto">
                     <li>
                      百度研究院:
                      <a href="https://ai.baidu.com/broad/introduction" rel="nofollow">
                       https://ai.baidu.com/broad/introduction
                      </a>
                     </li>
                     <li>
                      中山大学人机物智能融合实验室:
                      <a href="http://www.sysu-hcp.net/resources/" rel="nofollow">
                       http://www.sysu-hcp.net/resources/
                      </a>
                     </li>
                     <li>
                      大连理工大学IIAU-LAB:
                      <a href="http://ice.dlut.edu.cn/lu/publications.html" rel="nofollow">
                       http://ice.dlut.edu.cn/lu/publications.html
                      </a>
                     </li>
                     <li>
                      CUHK Multimedia Laboratory:
                      <a href="http://mmlab.ie.cuhk.edu.hk/datasets.html" rel="nofollow">
                       http://mmlab.ie.cuhk.edu.hk/datasets.html
                      </a>
                     </li>
                    </ul>
                    <div class="markdown-heading" dir="auto">
                     <h3 class="heading-element" dir="auto" tabindex="-1">
                      Resource Websites
                     </h3>
                     <a aria-label="Permalink: Resource Websites" class="anchor" href="#resource-websites" id="user-content-resource-websites">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <ul dir="auto">
                     <li>
                      TC-11 Online Resources:
                      <a href="http://tc11.cvc.uab.es/datasets/type/" rel="nofollow">
                       http://tc11.cvc.uab.es/datasets/type/
                      </a>
                     </li>
                     <li>
                      CVonline: Image Databases:
                      <a href="http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm" rel="nofollow">
                       http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm
                      </a>
                      <ul dir="auto">
                       <li>
                        中文:
                        <a href="https://blog.csdn.net/zhaoliang027/article/details/83376167" rel="nofollow">
                         https://blog.csdn.net/zhaoliang027/article/details/83376167
                        </a>
                       </li>
                      </ul>
                     </li>
                     <li>
                      MediaEval Benchmark:
                      <a href="http://www.multimediaeval.org/datasets/" rel="nofollow">
                       http://www.multimediaeval.org/datasets/
                      </a>
                     </li>
                     <li>
                      Mit Saliency Benchmark:
                      <a href="http://saliency.mit.edu/datasets.html" rel="nofollow">
                       http://saliency.mit.edu/datasets.html
                      </a>
                     </li>
                     <li>
                      Datasets for machine learning:
                      <a href="https://www.datasetlist.com/" rel="nofollow">
                       https://www.datasetlist.com/
                      </a>
                     </li>
                     <li>
                      UCI machine learning repository:
                      <a href="https://archive.ics.uci.edu/ml/datasets.html" rel="nofollow">
                       https://archive.ics.uci.edu/ml/datasets.html
                      </a>
                     </li>
                     <li>
                      Kaggle datasets:
                      <a href="https://www.kaggle.com/datasets" rel="nofollow">
                       https://www.kaggle.com/datasets
                      </a>
                     </li>
                     <li>
                      Google
                      <ul dir="auto">
                       <li>
                        Dataset Seaerch:
                        <a href="https://toolbox.google.com/datasetsearch" rel="nofollow">
                         https://toolbox.google.com/datasetsearch
                        </a>
                       </li>
                       <li>
                        <a href="https://ai.google/tools/datasets/" rel="nofollow">
                         https://ai.google/tools/datasets/
                        </a>
                       </li>
                      </ul>
                     </li>
                     <li>
                      Yet Another Computer Vision Index To Datasets (YACVID): This website provides a list of frequently used computer vision datasets. Wait, there is more! There is also a description containing common problems, pitfalls and characteristics and now a searchable TAG cloud.:
                      <a href="http://yacvid.hayko.at/" rel="nofollow">
                       http://yacvid.hayko.at/
                      </a>
                     </li>
                    </ul>
                    <div class="markdown-heading" dir="auto">
                     <h2 class="heading-element" dir="auto" tabindex="-1">
                      About
                     </h2>
                     <a aria-label="Permalink: About" class="anchor" href="#about" id="user-content-about">
                      <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                       <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                       </path>
                      </svg>
                     </a>
                    </div>
                    <ul dir="auto">
                     <li>
                      Edited by Lart Pang
                     </li>
                     <li>
                      Tools: VSCode
                     </li>
                     <li>
                      Plugins:
                      <ul dir="auto">
                       <li>
                        Markdown All in One
                       </li>
                       <li>
                        markdown-formatter(随着不断地提了一些issue(
                        <a data-hovercard-type="issue" data-hovercard-url="/sumnow/markdown-formatter/issues/5/hovercard" href="https://github.com/sumnow/markdown-formatter/issues/5">
                         #5
                        </a>
                        ,
                        <a data-hovercard-type="issue" data-hovercard-url="/sumnow/markdown-formatter/issues/6/hovercard" href="https://github.com/sumnow/markdown-formatter/issues/6">
                         #6
                        </a>
                        ,
                        <a data-hovercard-type="issue" data-hovercard-url="/sumnow/markdown-formatter/issues/7/hovercard" href="https://github.com/sumnow/markdown-formatter/issues/7">
                         #7
                        </a>
                        ,
                        <a data-hovercard-type="issue" data-hovercard-url="/sumnow/markdown-formatter/issues/8/hovercard" href="https://github.com/sumnow/markdown-formatter/issues/8">
                         #8
                        </a>
                        ,
                        <a data-hovercard-type="issue" data-hovercard-url="/sumnow/markdown-formatter/issues/9/hovercard" href="https://github.com/sumnow/markdown-formatter/issues/9">
                         #9
                        </a>
                        ), 越来越好用了, 强烈推荐)
                       </li>
                       <li>
                        Paste Image
                       </li>
                      </ul>
                     </li>
                    </ul>
                   </article>
                  </div>
                 </div>
                </div>
               </div>
              </div>
              <!-- -->
              <!-- -->
              <script id="__PRIMER_DATA_:R0:__" type="application/json">
               {"resolvedServerColorMode":"day"}
              </script>
             </div>
            </react-partial>
            <input data-csrf="true" type="hidden" value="kbzXRrjebvkWBB4E6cJb7UX3EkoYiYlXctzviVJndytfBUfx+eQDIMlwLJT/Kx0DZHWyqpHvujKgTO04HuZ03A==">
            </input>
           </div>
           <div class="Layout-sidebar" data-view-component="true">
            <div class="BorderGrid about-margin" data-pjax="">
             <div class="BorderGrid-row">
              <div class="BorderGrid-cell">
               <div class="hide-sm hide-md">
                <h2 class="mb-3 h4">
                 About
                </h2>
                <p class="f4 my-3">
                 A collection of some datasets for segmentation / saliency detection. Welcome to PR...:smile:
                </p>
                <div class="my-3 d-flex flex-items-center">
                 <svg aria-hidden="true" class="octicon octicon-link flex-shrink-0 mr-2" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                  <path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">
                  </path>
                 </svg>
                 <span class="flex-auto min-width-0 css-truncate css-truncate-target width-fit">
                  <a class="text-bold" href="https://lartpang.github.io/awesome-segmentation-saliency-dataset/README.html" rel="noopener noreferrer nofollow" role="link" target="_blank" title="https://lartpang.github.io/awesome-segmentation-saliency-dataset/README.html">
                   lartpang.github.io/awesome-segmentation-saliency-dataset/README.html
                  </a>
                 </span>
                </div>
                <h3 class="sr-only">
                 Resources
                </h3>
                <div class="mt-2">
                 <a class="Link--muted" data-analytics-event='{"category":"Repository Overview","action":"click","label":"location:sidebar;file:readme"}' href="#readme-ov-file">
                  <svg aria-hidden="true" class="octicon octicon-book mr-2" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                   <path d="M0 1.75A.75.75 0 0 1 .75 1h4.253c1.227 0 2.317.59 3 1.501A3.743 3.743 0 0 1 11.006 1h4.245a.75.75 0 0 1 .75.75v10.5a.75.75 0 0 1-.75.75h-4.507a2.25 2.25 0 0 0-1.591.659l-.622.621a.75.75 0 0 1-1.06 0l-.622-.621A2.25 2.25 0 0 0 5.258 13H.75a.75.75 0 0 1-.75-.75Zm7.251 10.324.004-5.073-.002-2.253A2.25 2.25 0 0 0 5.003 2.5H1.5v9h3.757a3.75 3.75 0 0 1 1.994.574ZM8.755 4.75l-.004 7.322a3.752 3.752 0 0 1 1.992-.572H14.5v-9h-3.495a2.25 2.25 0 0 0-2.25 2.25Z">
                   </path>
                  </svg>
                  Readme
                 </a>
                </div>
                <include-fragment src="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/hovercards/citation/sidebar_partial?tree_name=master">
                </include-fragment>
                <div class="mt-2">
                 <a class="Link Link--muted" data-view-component="true" href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/activity">
                  <svg aria-hidden="true" class="octicon octicon-pulse mr-2" data-view-component="true" height="16" text="gray" version="1.1" viewbox="0 0 16 16" width="16">
                   <path d="M6 2c.306 0 .582.187.696.471L10 10.731l1.304-3.26A.751.751 0 0 1 12 7h3.25a.75.75 0 0 1 0 1.5h-2.742l-1.812 4.528a.751.751 0 0 1-1.392 0L6 4.77 4.696 8.03A.75.75 0 0 1 4 8.5H.75a.75.75 0 0 1 0-1.5h2.742l1.812-4.529A.751.751 0 0 1 6 2Z">
                   </path>
                  </svg>
                  <span class="color-fg-muted">
                   Activity
                  </span>
                 </a>
                </div>
                <div class="mt-2">
                 <a class="Link Link--muted" data-view-component="true" href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/custom-properties">
                  <svg aria-hidden="true" class="octicon octicon-note mr-2" data-view-component="true" height="16" text="gray" version="1.1" viewbox="0 0 16 16" width="16">
                   <path d="M0 3.75C0 2.784.784 2 1.75 2h12.5c.966 0 1.75.784 1.75 1.75v8.5A1.75 1.75 0 0 1 14.25 14H1.75A1.75 1.75 0 0 1 0 12.25Zm1.75-.25a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h12.5a.25.25 0 0 0 .25-.25v-8.5a.25.25 0 0 0-.25-.25ZM3.5 6.25a.75.75 0 0 1 .75-.75h7a.75.75 0 0 1 0 1.5h-7a.75.75 0 0 1-.75-.75Zm.75 2.25h4a.75.75 0 0 1 0 1.5h-4a.75.75 0 0 1 0-1.5Z">
                   </path>
                  </svg>
                  <span class="color-fg-muted">
                   Custom properties
                  </span>
                 </a>
                </div>
                <h3 class="sr-only">
                 Stars
                </h3>
                <div class="mt-2">
                 <a class="Link Link--muted" data-view-component="true" href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/stargazers">
                  <svg aria-hidden="true" class="octicon octicon-star mr-2" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                   <path d="M8 .25a.75.75 0 0 1 .673.418l1.882 3.815 4.21.612a.75.75 0 0 1 .416 1.279l-3.046 2.97.719 4.192a.751.751 0 0 1-1.088.791L8 12.347l-3.766 1.98a.75.75 0 0 1-1.088-.79l.72-4.194L.818 6.374a.75.75 0 0 1 .416-1.28l4.21-.611L7.327.668A.75.75 0 0 1 8 .25Zm0 2.445L6.615 5.5a.75.75 0 0 1-.564.41l-3.097.45 2.24 2.184a.75.75 0 0 1 .216.664l-.528 3.084 2.769-1.456a.75.75 0 0 1 .698 0l2.77 1.456-.53-3.084a.75.75 0 0 1 .216-.664l2.24-2.183-3.096-.45a.75.75 0 0 1-.564-.41L8 2.694Z">
                   </path>
                  </svg>
                  <strong>
                   0
                  </strong>
                  stars
                 </a>
                </div>
                <h3 class="sr-only">
                 Watchers
                </h3>
                <div class="mt-2">
                 <a class="Link Link--muted" data-view-component="true" href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/watchers">
                  <svg aria-hidden="true" class="octicon octicon-eye mr-2" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                   <path d="M8 2c1.981 0 3.671.992 4.933 2.078 1.27 1.091 2.187 2.345 2.637 3.023a1.62 1.62 0 0 1 0 1.798c-.45.678-1.367 1.932-2.637 3.023C11.67 13.008 9.981 14 8 14c-1.981 0-3.671-.992-4.933-2.078C1.797 10.83.88 9.576.43 8.898a1.62 1.62 0 0 1 0-1.798c.45-.677 1.367-1.931 2.637-3.022C4.33 2.992 6.019 2 8 2ZM1.679 7.932a.12.12 0 0 0 0 .136c.411.622 1.241 1.75 2.366 2.717C5.176 11.758 6.527 12.5 8 12.5c1.473 0 2.825-.742 3.955-1.715 1.124-.967 1.954-2.096 2.366-2.717a.12.12 0 0 0 0-.136c-.412-.621-1.242-1.75-2.366-2.717C10.824 4.242 9.473 3.5 8 3.5c-1.473 0-2.825.742-3.955 1.715-1.124.967-1.954 2.096-2.366 2.717ZM8 10a2 2 0 1 1-.001-3.999A2 2 0 0 1 8 10Z">
                   </path>
                  </svg>
                  <strong>
                   1
                  </strong>
                  watching
                 </a>
                </div>
                <h3 class="sr-only">
                 Forks
                </h3>
                <div class="mt-2">
                 <a class="Link Link--muted" data-view-component="true" href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/forks">
                  <svg aria-hidden="true" class="octicon octicon-repo-forked mr-2" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
                   <path d="M5 5.372v.878c0 .414.336.75.75.75h4.5a.75.75 0 0 0 .75-.75v-.878a2.25 2.25 0 1 1 1.5 0v.878a2.25 2.25 0 0 1-2.25 2.25h-1.5v2.128a2.251 2.251 0 1 1-1.5 0V8.5h-1.5A2.25 2.25 0 0 1 3.5 6.25v-.878a2.25 2.25 0 1 1 1.5 0ZM5 3.25a.75.75 0 1 0-1.5 0 .75.75 0 0 0 1.5 0Zm6.75.75a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5Zm-3 8.75a.75.75 0 1 0-1.5 0 .75.75 0 0 0 1.5 0Z">
                   </path>
                  </svg>
                  <strong>
                   0
                  </strong>
                  forks
                 </a>
                </div>
                <div class="mt-2">
                 <a class="Link--muted" href="/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2FAIMonk-Labs-Private-Limited%2Fawesome-segmentation-saliency-dataset&amp;report=AIMonk-Labs-Private-Limited+%28user%29">
                  Report repository
                 </a>
                </div>
               </div>
              </div>
             </div>
             <div class="BorderGrid-row">
              <div class="BorderGrid-cell">
               <h2 class="h4 mb-3" data-pjax="#repo-content-pjax-container" data-turbo-frame="repo-content-turbo-frame">
                <a class="Link--primary no-underline Link" data-view-component="true" href="/AIMonk-Labs-Private-Limited/awesome-segmentation-saliency-dataset/releases">
                 Releases
                </a>
               </h2>
               <div class="text-small color-fg-muted">
                No releases published
               </div>
              </div>
             </div>
             <div class="BorderGrid-row">
              <div class="BorderGrid-cell">
               <h2 class="h4 mb-3">
                <a class="Link--primary no-underline Link d-flex flex-items-center" data-view-component="true" href="/orgs/AIMonk-Labs-Private-Limited/packages?repo_name=awesome-segmentation-saliency-dataset">
                 Packages
                 <span class="Counter ml-1" data-view-component="true" hidden="hidden" title="0">
                  0
                 </span>
                </a>
               </h2>
               <div class="text-small color-fg-muted">
                No packages published
                <br/>
               </div>
              </div>
             </div>
             <div class="BorderGrid-row">
              <div class="BorderGrid-cell">
               <h2 class="h4 mb-3">
                Languages
               </h2>
               <div class="mb-2">
                <span class="Progress" data-view-component="true">
                 <span aria-label="HTML 100.0" class="Progress-item color-bg-success-emphasis" data-view-component="true" itemprop="keywords" style="background-color:#e34c26 !important;;width: 100.0%;">
                 </span>
                </span>
               </div>
               <ul class="list-style-none">
                <li class="d-inline">
                 <span class="d-inline-flex flex-items-center flex-nowrap text-small mr-3">
                  <svg aria-hidden="true" class="octicon octicon-dot-fill mr-2" data-view-component="true" height="16" style="color:#e34c26;" version="1.1" viewbox="0 0 16 16" width="16">
                   <path d="M8 4a4 4 0 1 1 0 8 4 4 0 0 1 0-8Z">
                   </path>
                  </svg>
                  <span class="color-fg-default text-bold mr-1">
                   HTML
                  </span>
                  <span>
                   100.0%
                  </span>
                 </span>
                </li>
               </ul>
              </div>
             </div>
            </div>
           </div>
          </div>
         </div>
        </div>
       </div>
      </turbo-frame>
     </main>
    </div>
   </div>
   <footer class="footer pt-8 pb-6 f6 color-fg-muted p-responsive" role="contentinfo">
    <h2 class="sr-only">
     Footer
    </h2>
    <div class="d-flex flex-justify-center flex-items-center flex-column-reverse flex-lg-row flex-wrap flex-lg-nowrap">
     <div class="d-flex flex-items-center flex-shrink-0 mx-2">
      <a aria-label="Homepage" class="footer-octicon mr-2" href="https://github.com" title="GitHub">
       <svg aria-hidden="true" class="octicon octicon-mark-github" data-view-component="true" height="24" version="1.1" viewbox="0 0 16 16" width="24">
        <path d="M8 0c4.42 0 8 3.58 8 8a8.013 8.013 0 0 1-5.45 7.59c-.4.08-.55-.17-.55-.38 0-.27.01-1.13.01-2.2 0-.75-.25-1.23-.54-1.48 1.78-.2 3.65-.88 3.65-3.95 0-.88-.31-1.59-.82-2.15.08-.2.36-1.02-.08-2.12 0 0-.67-.22-2.2.82-.64-.18-1.32-.27-2-.27-.68 0-1.36.09-2 .27-1.53-1.03-2.2-.82-2.2-.82-.44 1.1-.16 1.92-.08 2.12-.51.56-.82 1.28-.82 2.15 0 3.06 1.86 3.75 3.64 3.95-.23.2-.44.55-.51 1.07-.46.21-1.61.55-2.33-.66-.15-.24-.6-.83-1.23-.82-.67.01-.27.38.01.53.34.19.73.9.82 1.13.16.45.68 1.31 2.69.94 0 .67.01 1.3.01 1.49 0 .21-.15.45-.55.38A7.995 7.995 0 0 1 0 8c0-4.42 3.58-8 8-8Z">
        </path>
       </svg>
      </a>
      <span>
       © 2024 GitHub, Inc.
      </span>
     </div>
     <nav aria-label="Footer">
      <h3 class="sr-only" id="sr-footer-heading">
       Footer navigation
      </h3>
      <ul aria-labelledby="sr-footer-heading" class="list-style-none d-flex flex-justify-center flex-wrap mb-2 mb-lg-0">
       <li class="mx-2">
        <a class="Link--secondary Link" data-analytics-event='{"category":"Footer","action":"go to Terms","label":"text:terms"}' data-view-component="true" href="https://docs.github.com/site-policy/github-terms/github-terms-of-service">
         Terms
        </a>
       </li>
       <li class="mx-2">
        <a class="Link--secondary Link" data-analytics-event='{"category":"Footer","action":"go to privacy","label":"text:privacy"}' data-view-component="true" href="https://docs.github.com/site-policy/privacy-policies/github-privacy-statement">
         Privacy
        </a>
       </li>
       <li class="mx-2">
        <a class="Link--secondary Link" data-analytics-event='{"category":"Footer","action":"go to security","label":"text:security"}' data-view-component="true" href="/security">
         Security
        </a>
       </li>
       <li class="mx-2">
        <a class="Link--secondary Link" data-analytics-event='{"category":"Footer","action":"go to status","label":"text:status"}' data-view-component="true" href="https://www.githubstatus.com/">
         Status
        </a>
       </li>
       <li class="mx-2">
        <a class="Link--secondary Link" data-analytics-event='{"category":"Footer","action":"go to docs","label":"text:docs"}' data-view-component="true" href="https://docs.github.com/">
         Docs
        </a>
       </li>
       <li class="mx-2">
        <a class="Link--secondary Link" data-analytics-event='{"category":"Footer","action":"go to contact","label":"text:contact"}' data-view-component="true" href="https://support.github.com?tags=dotcom-footer">
         Contact
        </a>
       </li>
       <li class="mr-3">
        <cookie-consent-link>
         <button class="Link--secondary underline-on-hover border-0 p-0 color-bg-transparent" data-action="click:cookie-consent-link#showConsentManagement" type="button">
          Manage cookies
         </button>
        </cookie-consent-link>
       </li>
       <li class="mr-3">
        <cookie-consent-link>
         <button class="Link--secondary underline-on-hover border-0 p-0 color-bg-transparent" data-action="click:cookie-consent-link#showConsentManagement" type="button">
          Do not share my personal information
         </button>
        </cookie-consent-link>
       </li>
      </ul>
     </nav>
    </div>
   </footer>
   <ghcc-consent class="position-fixed bottom-0 left-0" data-cookie-consent-required="true" data-initial-cookie-consent-allowed="" id="ghcc" style="z-index: 999999">
   </ghcc-consent>
   <div class="ajax-error-message flash flash-error" hidden="" id="ajax-error-message">
    <svg aria-hidden="true" class="octicon octicon-alert" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path d="M6.457 1.047c.659-1.234 2.427-1.234 3.086 0l6.082 11.378A1.75 1.75 0 0 1 14.082 15H1.918a1.75 1.75 0 0 1-1.543-2.575Zm1.763.707a.25.25 0 0 0-.44 0L1.698 13.132a.25.25 0 0 0 .22.368h12.164a.25.25 0 0 0 .22-.368Zm.53 3.996v2.5a.75.75 0 0 1-1.5 0v-2.5a.75.75 0 0 1 1.5 0ZM9 11a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z">
     </path>
    </svg>
    <button aria-label="Dismiss error" class="flash-close js-ajax-error-dismiss" type="button">
     <svg aria-hidden="true" class="octicon octicon-x" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
      <path d="M3.72 3.72a.75.75 0 0 1 1.06 0L8 6.94l3.22-3.22a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l3.22 3.22a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-3.22 3.22a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L6.94 8 3.72 4.78a.75.75 0 0 1 0-1.06Z">
      </path>
     </svg>
    </button>
    You can’t perform that action at this time.
   </div>
   <template id="site-details-dialog">
    <details class="details-reset details-overlay details-overlay-dark lh-default color-fg-default hx_rsm" open="">
     <summary aria-label="Close dialog" role="button">
     </summary>
     <details-dialog class="Box Box--overlay d-flex flex-column anim-fade-in fast hx_rsm-dialog hx_rsm-modal">
      <button aria-label="Close dialog" class="Box-btn-octicon m-0 btn-octicon position-absolute right-0 top-0" data-close-dialog="" type="button">
       <svg aria-hidden="true" class="octicon octicon-x" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
        <path d="M3.72 3.72a.75.75 0 0 1 1.06 0L8 6.94l3.22-3.22a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l3.22 3.22a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-3.22 3.22a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L6.94 8 3.72 4.78a.75.75 0 0 1 0-1.06Z">
        </path>
       </svg>
      </button>
      <div class="octocat-spinner my-6 js-details-dialog-spinner">
      </div>
     </details-dialog>
    </details>
   </template>
   <div class="Popover js-hovercard-content position-absolute" style="display: none; outline: none;" tabindex="0">
    <div class="Popover-message Popover-message--bottom-left Popover-message--large Box color-shadow-large" style="width:360px;">
    </div>
   </div>
   <template id="snippet-clipboard-copy-button">
    <div class="zeroclipboard-container position-absolute right-0 top-0">
     <clipboard-copy aria-label="Copy" class="ClipboardButton btn js-clipboard-copy m-2 p-0 tooltipped-no-delay" data-copy-feedback="Copied!" data-tooltip-direction="w">
      <svg aria-hidden="true" class="octicon octicon-copy js-clipboard-copy-icon m-2" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
       <path d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z">
       </path>
       <path d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z">
       </path>
      </svg>
      <svg aria-hidden="true" class="octicon octicon-check js-clipboard-check-icon color-fg-success d-none m-2" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
       <path d="M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z">
       </path>
      </svg>
     </clipboard-copy>
    </div>
   </template>
   <template id="snippet-clipboard-copy-button-unpositioned">
    <div class="zeroclipboard-container">
     <clipboard-copy aria-label="Copy" class="ClipboardButton btn btn-invisible js-clipboard-copy m-2 p-0 tooltipped-no-delay d-flex flex-justify-center flex-items-center" data-copy-feedback="Copied!" data-tooltip-direction="w">
      <svg aria-hidden="true" class="octicon octicon-copy js-clipboard-copy-icon" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
       <path d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z">
       </path>
       <path d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z">
       </path>
      </svg>
      <svg aria-hidden="true" class="octicon octicon-check js-clipboard-check-icon color-fg-success d-none" data-view-component="true" height="16" version="1.1" viewbox="0 0 16 16" width="16">
       <path d="M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z">
       </path>
      </svg>
     </clipboard-copy>
    </div>
   </template>
  </div>
  <div aria-atomic="true" aria-live="polite" class="sr-only" id="js-global-screen-reader-notice">
  </div>
  <div aria-atomic="true" aria-live="assertive" class="sr-only" id="js-global-screen-reader-notice-assertive">
  </div>
 </body>
</html>
